{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head direction attractor network\n",
    "\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [**Synthetic population**](#synthetic)\n",
    "3. [**SNN**](#snn)\n",
    "4. [**RNN**](#rnn)\n",
    "\n",
    "This notebook contains analysis of different UCM hyperparameters and does model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0+cu117\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.special as sps\n",
    "import scipy.stats as scstats\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../lib/\") # access to library\n",
    "sys.path.append(\"../scripts/\") # access to scripts\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./saves'):\n",
    "    os.makedirs('./saves')\n",
    "    \n",
    "\n",
    "import neuroprob as nprb\n",
    "from neuroprob import utils\n",
    "\n",
    "\n",
    "dev = nprb.inference.get_device(gpu=0)\n",
    "\n",
    "import template\n",
    "import fit_model\n",
    "import ucm_stats\n",
    "\n",
    "import pickle\n",
    "\n",
    "plt.style.use(['../paper.mplstyle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model(datatype, cvdata, resamples, rc_t, binsize, \n",
    "                   m, rcov, max_count, neurons):\n",
    "    max_count = int(rc_t.max())\n",
    "    units_used = neurons\n",
    "\n",
    "    \n",
    "    mtype, ll_mode, r_mode, num_induc, inv_link, C, z_dims, delays, folds, cv_switch, basis_mode = m\n",
    "    shared_W = False\n",
    "    enc_layers, basis = models.hyper_params(basis_mode)\n",
    "    kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "\n",
    "    if ll_mode == 'U':\n",
    "        mapping_net = models.net(C, basis, max_count, units_used, shared_W)\n",
    "    else:\n",
    "        mapping_net = None\n",
    "\n",
    "    full_model, _ = models.set_model(max_count, mtype, r_mode, ll_mode, fcov, units_used, \n",
    "                                     tbin, ftrain, num_induc, inv_link=inv_link, mapping_net=mapping_net, \n",
    "                                     batch_size=batch_size, C=C, enc_layers=enc_layers)\n",
    "    full_model.to(dev)\n",
    "\n",
    "\n",
    "    name = 'valS' if shared_W else 'val'\n",
    "    if basis_mode != 'ew':\n",
    "        name += basis_mode\n",
    "    model_name = '{}{}_{}_{}_{}_C={}_{}'.format(name, datatype, mtype, ll_mode, r_mode, C, kcv)\n",
    "    checkpoint = torch.load('../scripts/checkpoint/' + model_name, map_location='cuda:0')\n",
    "    full_model.load_state_dict(checkpoint['full_model'])\n",
    "    return full_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = 1\n",
    "rcov, neurons, tbin, resamples, rc_t = validation.get_dataset(datatype, '../scripts/data')\n",
    "max_count = int(rc_t.max())\n",
    "trials = 1\n",
    "\n",
    "use_neuron = np.arange(50)\n",
    "\n",
    "\n",
    "rhd_t = rcov[0]\n",
    "ra_t = rcov[1]\n",
    "covariates = [rhd_t[None, :, None].repeat(trials, axis=0), \n",
    "              ra_t[None, :, None].repeat(trials, axis=0)]\n",
    "glm = validation.IP_bumps(tbin, resamples, covariates, neurons, trials=trials)\n",
    "glm.to(dev)\n",
    "\n",
    "checkpoint = torch.load('../scripts/data/IP_HDC_model', map_location='cuda:0')\n",
    "glm.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'IP', 'hd', 8, 'exp', 1, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd', 8, 'identity', 3, [], False, 10, False, 'ew'),\n",
    "         ('GP', 'U', 'hdxR1', 16, 'identity', 3, [1], False, 10, False, 'ew')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neuron subgroup likelihood CV\n",
    "beta = 0.0\n",
    "n_group = np.arange(5)\n",
    "val_neuron = [n_group, n_group+10, n_group+20, n_group+30, n_group+40]\n",
    "ncvx = 2\n",
    "kcvs = [2, 5, 8] # validation sets chosen in 10-fold split of data\n",
    "Ms = modes[:3]\n",
    "\n",
    "batch_size = 5000\n",
    "cv_pll = []\n",
    "for em, mode in enumerate(Ms):\n",
    "    for cvdata in model_utils.get_cv_sets(mode, kcvs, batch_size, rc_t, resamples, rcov):\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "        if em > 1:\n",
    "            for v_neuron in val_neuron:\n",
    "\n",
    "                prev_ll = np.inf\n",
    "                for tr in range(ncvx):\n",
    "                    full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                                mode, rcov, max_count, neurons)\n",
    "                    mask = np.ones((neurons,), dtype=bool)\n",
    "                    mask[v_neuron] = False\n",
    "                    f_neuron = np.arange(neurons)[mask]\n",
    "                    ll = model_utils.LVM_pred_ll(full_model, mode[-5], mode[2], models.cov_used, cv_set, f_neuron, v_neuron, \n",
    "                                                 beta=beta, beta_z=0.0, max_iters=3000)[0]\n",
    "                    if ll < prev_ll:\n",
    "                        prev_ll = ll\n",
    "\n",
    "                cv_pll.append(prev_ll)\n",
    "                \n",
    "        else:\n",
    "            for v_neuron in val_neuron:\n",
    "                full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                            mode, rcov, max_count, neurons)\n",
    "                cv_pll.append(model_utils.RG_pred_ll(full_model, mode[2], models.cov_used, cv_set, bound='ELBO', \n",
    "                                                     beta=beta, neuron_group=v_neuron, ll_mode='GH', ll_samples=100))\n",
    "\n",
    "        \n",
    "cv_pll = np.array(cv_pll).reshape(len(Ms), len(kcvs), len(val_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute tuning curves and latent trajectories for XZ joint regression-latent model\n",
    "mode = modes[2]\n",
    "cvdata = model_utils.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov)[0]\n",
    "\n",
    "full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                            mode, rcov, max_count, neurons)\n",
    "\n",
    "# latents\n",
    "X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "\n",
    "T = X_loc[1].shape[0]\n",
    "X_c, shift, sign, scale, _ = utils.latent.signed_scaled_shift(X_loc[1], ra_t[:T], \n",
    "                                                              dev, 'euclid', learn_scale=True)\n",
    "X_s = scale*X_std[1]\n",
    "\n",
    "\n",
    "\n",
    "# tuning\n",
    "steps = 100\n",
    "covariates_z = [0.*np.ones(steps), np.linspace(X_loc[1].min(), X_loc[1].max(), steps)]\n",
    "P_mc = model_utils.compute_P(full_model, covariates_z, use_neuron, MC=1000).cpu()\n",
    "\n",
    "x_counts = torch.arange(max_count+1)\n",
    "avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "xcvar = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "ff = xcvar/avg\n",
    "\n",
    "avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                             smooth_length=5, padding_mode='replicate')\n",
    "avglower, avgmean, avgupper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                            smooth_length=5, padding_mode='replicate')\n",
    "fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "\n",
    "covariates_z[1] = sign*scale*covariates_z[1]+shift\n",
    "grate = glm.mapping.eval_rate(covariates_z, use_neuron)[0, ...]\n",
    "gFF = np.ones_like(grate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS framework\n",
    "Qq = []\n",
    "Zz = []\n",
    "R = []\n",
    "Rp = []\n",
    "\n",
    "batch_size = 5000\n",
    "\n",
    "Ms = modes[:3]\n",
    "CV = [2, 5, 8]\n",
    "for kcv in CV:\n",
    "    for en, mode in enumerate(Ms):\n",
    "        cvdata = model_utils.get_cv_sets(mode, [kcv], batch_size, rc_t, resamples, rcov)[0]\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        time_steps = ftrain.shape[-1]\n",
    "            \n",
    "        full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                    mode, rcov, max_count, neurons)\n",
    "\n",
    "        if en > 0:\n",
    "            # predictive posterior\n",
    "            q_ = []\n",
    "            Z_ = []\n",
    "            for b in range(full_model.inputs.batches):\n",
    "                P_mc = model_utils.compute_pred_P(full_model, b, use_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "                P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "                for n in range(len(use_neuron)):\n",
    "                    spike_binned = full_model.likelihood.spikes[b][0, n, :].numpy()\n",
    "                    q, Z = model_utils.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "\n",
    "                    if b == 0:\n",
    "                        q_.append(q)\n",
    "                        Z_.append(Z)\n",
    "                    else:\n",
    "                        q_[n] = np.concatenate((q_[n], q))\n",
    "                        Z_[n] = np.concatenate((Z_[n], Z))\n",
    "\n",
    "        elif en == 0:\n",
    "            cov_used = models.cov_used(mode[2], fcov)\n",
    "            q_ = model_utils.compute_count_stats(full_model, 'IP', tbin, ftrain, cov_used, use_neuron, \\\n",
    "                                                 traj_len=1, start=0, T=time_steps, bs=5000)\n",
    "            Z_ = [utils.stats.q_to_Z(q) for q in q_]\n",
    "\n",
    "\n",
    "        Pearson_s = []\n",
    "        for n in range(len(use_neuron)):\n",
    "            for m in range(n+1, len(use_neuron)):\n",
    "                r, r_p = scstats.pearsonr(Z_[n], Z_[m]) # Pearson r correlation test\n",
    "                Pearson_s.append((r, r_p))\n",
    "\n",
    "        r = np.array([p[0] for p in Pearson_s])\n",
    "        r_p = np.array([p[1] for p in Pearson_s])\n",
    "\n",
    "        Qq.append(q_)\n",
    "        Zz.append(Z_)\n",
    "        R.append(r)\n",
    "        Rp.append(r_p)\n",
    "\n",
    "    \n",
    "    \n",
    "fisher_z = []\n",
    "fisher_q = []\n",
    "for en, r in enumerate(R):\n",
    "    fz = 0.5*np.log((1+r)/(1-r))*np.sqrt(time_steps-3)\n",
    "    fisher_z.append(fz)\n",
    "    fisher_q.append(utils.stats.Z_to_q(fz))\n",
    "    \n",
    "    \n",
    "    \n",
    "q_DS_ = []\n",
    "T_DS_ = []\n",
    "T_KS_ = []\n",
    "for q in Qq:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_.append(T_DS)\n",
    "        T_KS_.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_.append(utils.stats.Z_to_q(Z_DS))\n",
    "\n",
    "\n",
    "q_DS_ = np.array(q_DS_).reshape(len(CV), len(Ms), -1)\n",
    "T_DS_ = np.array(T_DS_).reshape(len(CV), len(Ms), -1)\n",
    "T_KS_ = np.array(T_KS_).reshape(len(CV), len(Ms), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise correlation structure\n",
    "NN = len(use_neuron)\n",
    "R_mat_Xp = np.zeros((NN, NN))\n",
    "R_mat_X = np.zeros((NN, NN))\n",
    "R_mat_XZ = np.zeros((NN, NN))\n",
    "for a in range(len(R[0])):\n",
    "    n, m = model_utils.ind_to_pair(a, NN)\n",
    "    R_mat_Xp[n, m] = R[0][a]\n",
    "    R_mat_X[n, m] = R[1][a]\n",
    "    R_mat_XZ[n, m] = R[2][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    q_DS_, T_DS_, T_KS_, sign_DS, fisher_z, fisher_q, Qq, Zz, \n",
    "    R, Rp, X_c, X_s, ra_t, R_mat_Xp, R_mat_X, R_mat_XZ, cv_pll, \n",
    "    covariates_z, avglower, avgmean, avgupper, fflower, ffmean, ffupper, \n",
    "    grate, gFF, \n",
    "    use_neuron, max_count, tbin, rcov\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./saves/P2.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hCMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'IP', 'hd', 8, 'exp', 1, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'hNB', 'hd', 8, 'exp', 1, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd', 8, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('ANN', 'U', 'hd', 8, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'IP', 'T1', 8, 'exp', 1, [0], False, 10, False, 'ew'), \n",
    "         ('GP', 'hNB', 'T1', 8, 'exp', 1, [0], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'T1', 8, 'identity', 3, [0], False, 10, False, 'ew'), \n",
    "         ('ANN', 'U', 'T1', 8, 'identity', 3, [0], False, 10, False, 'ew')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatype = 0\n",
    "rcov, neurons, tbin, resamples, rc_t = validation.get_dataset(datatype, '../scripts/data')\n",
    "max_count = int(rc_t.max())\n",
    "\n",
    "use_neuron = list(range(neurons))\n",
    "\n",
    "\n",
    "rhd_t = rcov[0]\n",
    "trials = 1\n",
    "covariates = [rhd_t[None, :, None].repeat(trials, axis=0)]\n",
    "glm = validation.CMP_hdc(tbin, resamples, covariates, neurons, trials=trials)\n",
    "glm.to(dev)\n",
    "\n",
    "checkpoint = torch.load('../scripts/data/hCMP_HDC_model', map_location='cpu')\n",
    "\n",
    "glm.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation of regression models\n",
    "beta = 0.0\n",
    "kcvs = [2, 5, 8] # validation sets chosen in 10-fold split of data\n",
    "batch_size = 5000\n",
    "\n",
    "Ms = modes[:4]\n",
    "RG_cv_ll = []\n",
    "for mode in Ms:\n",
    "    for cvdata in model_utils.get_cv_sets(mode, kcvs, batch_size, rc_t, resamples, rcov):\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "        full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                    mode, rcov, max_count, neurons)\n",
    "        RG_cv_ll.append(model_utils.RG_pred_ll(full_model, mode[2], models.cov_used, cv_set, bound='ELBO', \n",
    "                                               beta=beta, neuron_group=None, ll_mode='GH', ll_samples=100))\n",
    "    \n",
    "RG_cv_ll = np.array(RG_cv_ll).reshape(len(Ms), len(kcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tuning curves of ground truth model\n",
    "batch_size = 5000\n",
    "\n",
    "cvdata = model_utils.get_cv_sets(mode, [2], batch_size, rc_t, resamples, rcov)[0]\n",
    "full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                            modes[2], rcov, max_count, neurons)\n",
    "\n",
    "\n",
    "\n",
    "steps = 100\n",
    "covariates = [np.linspace(0, 2*np.pi, steps)]\n",
    "P_mc = model_utils.compute_P(full_model, covariates, use_neuron, MC=1000)\n",
    "P_rg = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "x_counts = torch.arange(max_count+1)\n",
    "\n",
    "\n",
    "\n",
    "mu = glm.mapping.eval_rate(covariates, use_neuron)[0:1, ...] # mu is the rate, so need to multiply by tbin\n",
    "log_nu = glm.likelihood.dispersion_mapping.eval_rate(covariates, use_neuron)[0:1, ...]\n",
    "\n",
    "log_mudt = torch.tensor(np.log(mu*tbin)).to(dev)\n",
    "nu = torch.tensor(np.exp(log_nu)).to(dev)\n",
    "\n",
    "AD = False # using Autograd for computing the CMP partition function is slow...\n",
    "if AD: # differentiate the partition function\n",
    "    t = torch.tensor(0.).to(dev)\n",
    "    t.requires_grad = True\n",
    "    log_Z = glm.likelihood.log_Z(log_mu+t, nu)\n",
    "\n",
    "    grad_t = torch.empty(log_Z.shape)\n",
    "    ggrad_t = torch.empty(log_Z.shape)\n",
    "    for n in use_neuron:\n",
    "        print(n)\n",
    "        for s in range(steps):\n",
    "            ind = torch.zeros_like(log_Z)\n",
    "            ind[0, n, s] = 1.\n",
    "            grad_t_, = torch.autograd.grad(log_Z, t, ind, retain_graph=True, create_graph=True)\n",
    "            grad_t[0, n, s] = grad_t_\n",
    "\n",
    "            ggrad_t_, = torch.autograd.grad(grad_t_, t, retain_graph=True, create_graph=True)\n",
    "            ggrad_t[0, n, s] = ggrad_t_\n",
    "\n",
    "\n",
    "    gmean = grad_t.data.cpu().numpy()[0, ...]\n",
    "    gvar = ggrad_t.data.cpu().numpy()[0, ...]\n",
    "\n",
    "else: # compute the partition function explicitly\n",
    "    gmean = utils.stats.cmp_moments(1, mu[0, ...], nu.cpu().numpy()[0, ...], tbin, J=10000)\n",
    "    gvar = utils.stats.cmp_moments(2, mu[0, ...], nu.cpu().numpy()[0, ...], tbin, J=10000) - gmean**2\n",
    "    \n",
    "grate = mu[0, ...]\n",
    "gdisp = nu.cpu().numpy()[0, ...]\n",
    "gFF = gvar/gmean\n",
    "\n",
    "\n",
    "\n",
    "# compute tuning curves and SCDs for model fit\n",
    "ref_prob = []\n",
    "hd = [20, 50, 80]\n",
    "for hd_ in hd:\n",
    "    for n in range(len(use_neuron)):\n",
    "        ref_prob.append([utils.stats.cmp_count_prob(xc, grate[n, hd_], gdisp[n, hd_], tbin) for xc in x_counts.numpy()])\n",
    "ref_prob = np.array(ref_prob).reshape(len(hd), len(use_neuron), -1)\n",
    "\n",
    "cs = utils.signal.percentiles_from_samples(P_mc[..., hd, :], percentiles=[0.05, 0.5, 0.95], smooth_length=1)\n",
    "clower, cmean, cupper = [cs_.cpu().numpy() for cs_ in cs]\n",
    "\n",
    "\n",
    "avg = (x_counts[None, None, None, :]*P_mc.cpu()).sum(-1)\n",
    "xcvar = ((x_counts[None, None, None, :]**2*P_mc.cpu()).sum(-1)-avg**2)\n",
    "ff = xcvar/avg\n",
    "\n",
    "avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                             smooth_length=5, padding_mode='circular')\n",
    "avglower, avgmean, avgupper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                            smooth_length=5, padding_mode='circular')\n",
    "fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "xcvars = utils.signal.percentiles_from_samples(xcvar, percentiles=[0.05, 0.5, 0.95], \n",
    "                                               smooth_length=5, padding_mode='circular')\n",
    "varlower, varmean, varupper = [cs_.cpu().numpy() for cs_ in xcvars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KS framework\n",
    "Qq_rg = []\n",
    "Zz_rg = []\n",
    "\n",
    "batch_size = 5000\n",
    "M = modes[:4]\n",
    "CV = [2, 5, 8]\n",
    "for kcv in CV:\n",
    "    for en, mode in enumerate(M):\n",
    "        cvdata = model_utils.get_cv_sets(mode, [kcv], batch_size, rc_t, resamples, rcov)[0]\n",
    "        full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                    mode, rcov, max_count, neurons)\n",
    "\n",
    "        if en > 1:\n",
    "            # predictive posterior\n",
    "            P_mc = model_utils.compute_pred_P(full_model, 0, use_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "            P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "            q_ = []\n",
    "            Z_ = []\n",
    "            for n in range(len(use_neuron)):\n",
    "                spike_binned = full_model.likelihood.spikes[0][0, use_neuron[n], :].numpy()\n",
    "                q, Z = model_utils.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "                q_.append(q)\n",
    "                Z_.append(Z)\n",
    "\n",
    "        elif en < 2:\n",
    "            _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "            time_steps = ftrain.shape[-1]\n",
    "\n",
    "            cov_used = models.cov_used(mode[2], fcov)\n",
    "            q_ = model_utils.compute_count_stats(full_model, mode[1], tbin, ftrain, cov_used, list(range(neurons)), \\\n",
    "                                                 traj_len=1, start=0, T=time_steps, bs=5000)\n",
    "            Z_ = [utils.stats.q_to_Z(q) for q in q_]\n",
    "\n",
    "        Qq_rg.append(q_)\n",
    "        Zz_rg.append(Z_)\n",
    "\n",
    "    \n",
    "q_DS_rg = []\n",
    "T_DS_rg = []\n",
    "T_KS_rg = []\n",
    "for q in Qq_rg:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_rg.append(T_DS)\n",
    "        T_KS_rg.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_rg.append(utils.stats.Z_to_q(Z_DS))\n",
    "\n",
    "q_DS_rg = np.array(q_DS_rg).reshape(len(CV), len(M), -1)\n",
    "T_DS_rg = np.array(T_DS_rg).reshape(len(CV), len(M), -1)\n",
    "T_KS_rg = np.array(T_KS_rg).reshape(len(CV), len(M), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligning trajectory and computing RMS for different models\n",
    "topology = 'torus'\n",
    "cvK = 90\n",
    "CV = [15, 30, 45, 60, 75]\n",
    "Modes = modes[4:8]\n",
    "batch_size = 5000\n",
    "\n",
    "RMS_cv = []\n",
    "for mode in Modes:\n",
    "    cvdata = model_utils.get_cv_sets(mode, [-1], batch_size, rc_t, resamples, rcov)[0]\n",
    "    full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                mode, rcov, max_count, neurons)\n",
    "\n",
    "    X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "    cvT = X_loc[0].shape[0]\n",
    "    tar_t = rhd_t[:cvT]\n",
    "    lat = X_loc[0]\n",
    "    \n",
    "    for rn in CV:\n",
    "        eval_range = np.arange(cvT//cvK) + rn*cvT//cvK\n",
    "\n",
    "        _, shift, sign, _, _ = utils.latent.signed_scaled_shift(lat[eval_range], tar_t[eval_range], \n",
    "                                                                topology=topology, dev=dev, learn_scale=False)\n",
    "        \n",
    "        mask = np.ones((cvT,), dtype=bool)\n",
    "        mask[eval_range] = False\n",
    "        \n",
    "        lat_t = torch.tensor((sign*lat+shift) % (2*np.pi))\n",
    "        D = (utils.latent.metric(torch.tensor(tar_t)[mask], lat_t[mask], topology)**2)\n",
    "        RMS_cv.append(np.sqrt(D.mean().item()))\n",
    "\n",
    "\n",
    "RMS_cv = np.array(RMS_cv).reshape(len(Modes), len(CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neuron subgroup likelihood CV for latent models\n",
    "beta = 0.0\n",
    "n_group = np.arange(5)\n",
    "ncvx = 2\n",
    "kcvs = [2, 5, 8] # validation sets chosen in 10-fold split of data\n",
    "Ms = modes[4:8]\n",
    "val_neuron = [n_group, n_group+10, n_group+20, n_group+30, n_group+40]\n",
    "\n",
    "batch_size = 5000\n",
    "LVM_cv_ll = []\n",
    "for kcv in kcvs:\n",
    "    for mode in Ms:\n",
    "        cvdata = model_utils.get_cv_sets(mode, [kcv], batch_size, rc_t, resamples, rcov)[0]\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "        for v_neuron in val_neuron:\n",
    "\n",
    "            prev_ll = np.inf\n",
    "            for tr in range(ncvx):\n",
    "                full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                            mode, rcov, max_count, neurons)\n",
    "                mask = np.ones((neurons,), dtype=bool)\n",
    "                mask[v_neuron] = False\n",
    "                f_neuron = np.arange(neurons)[mask]\n",
    "                ll = model_utils.LVM_pred_ll(full_model, mode[-5], mode[2], models.cov_used, cv_set, f_neuron, v_neuron, \n",
    "                                             beta=beta, beta_z=0.0, max_iters=3000)[0]\n",
    "                if ll < prev_ll:\n",
    "                    prev_ll = ll\n",
    "\n",
    "            LVM_cv_ll.append(prev_ll)\n",
    "        \n",
    "LVM_cv_ll = np.array(LVM_cv_ll).reshape(len(kcvs), len(Ms), len(val_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tuning curves and latent trajectory of latent Universal model\n",
    "lat_t_ = []\n",
    "lat_std_ = []\n",
    "P_ = []\n",
    "\n",
    "comp_grate = []\n",
    "comp_gdisp = []\n",
    "comp_gFF = []\n",
    "comp_gvar = []\n",
    "\n",
    "comp_avg = []\n",
    "comp_ff = []\n",
    "comp_var = []\n",
    "\n",
    "for mode in modes[-2:]:\n",
    "    cvdata = model_utils.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov)[0]\n",
    "    full_model = get_full_model(datatype, cvdata, resamples, rc_t, 100, \n",
    "                                mode, rcov, max_count, neurons)\n",
    "\n",
    "    # predict latents\n",
    "    X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "    cvT = X_loc[0].shape[0]\n",
    "\n",
    "    lat_t, shift, sign, _, _ = utils.latent.signed_scaled_shift(X_loc[0], rhd_t[:cvT], \n",
    "                                                             dev, learn_scale=False)\n",
    "    lat_t_.append(utils.signal.WrapPi(lat_t, True))\n",
    "    lat_std_.append(X_std[0])\n",
    "\n",
    "    # P\n",
    "    steps = 100\n",
    "    covariates_aligned = [(sign*(np.linspace(0, 2*np.pi, steps)-shift)) % (2*np.pi)]\n",
    "    P_mc = model_utils.compute_P(full_model, covariates_aligned, use_neuron, MC=1000).cpu()\n",
    "\n",
    "    x_counts = torch.arange(max_count+1)\n",
    "    avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "    xcvar = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "    ff = xcvar/avg\n",
    "\n",
    "    avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                 smooth_length=5, padding_mode='circular')\n",
    "    comp_avg.append([cs_.cpu().numpy() for cs_ in avgs])\n",
    "\n",
    "    ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='circular')\n",
    "    comp_ff.append([cs_.cpu().numpy() for cs_ in ffs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    use_neuron, covariates, P_rg, grate, gdisp, gFF, gvar, hd, ref_prob, clower, cmean, cupper, \n",
    "    avglower, avgmean, avgupper, fflower, ffmean, ffupper, varlower, varmean, varupper, \n",
    "    covariates_aligned, lat_t_, lat_std_, comp_avg, comp_ff, \n",
    "    RG_cv_ll, LVM_cv_ll, RMS_cv, \n",
    "    q_DS_rg, T_DS_rg, T_KS_rg, Qq_rg, Zz_rg, sign_DS, sign_KS, \n",
    "    max_count, tbin, rcov\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./saves/P1_hcmp.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
