{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1+cu101\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (fit_model.py, line 40)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/var/home/dl543/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-1163515509e5>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import fit_model\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"../scripts/fit_model.py\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    elif data_type == 'th1':\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.stats as scstats\n",
    "import scipy.special as sps\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import daft\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../scripts/\") # access to scripts\n",
    "\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "    \n",
    "\n",
    "from neuroprob import utils\n",
    "\n",
    "import model_utils\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use(['paper.mplstyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/Mouse28_140313_wake.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_neuron = [11, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "fig.text(-0.08, 1.02, 'A', transform=fig.transFigure, size=15, fontweight='bold')\n",
    "fig.text(0.29, 1.02, 'B', transform=fig.transFigure, size=15, fontweight='bold')\n",
    "fig.text(-0.08, 0.35, 'C', transform=fig.transFigure, size=15, fontweight='bold')\n",
    "fig.text(0.475, 0.35, 'D', transform=fig.transFigure, size=15, fontweight='bold')\n",
    "\n",
    "\n",
    "poscol = 'forestgreen'\n",
    "antcol = 'orange'\n",
    "\n",
    "\n",
    "\n",
    "# scores\n",
    "widths = [1]\n",
    "heights = [1]\n",
    "spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights), width_ratios=widths, wspace=0.4, \n",
    "                        height_ratios=heights, top=.95, bottom=0.75, left=0.0, right=.2)\n",
    "\n",
    "eps = 0.4\n",
    "Ncases = PLL_rg_ll.shape[0]-1\n",
    "fact = 10**3\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "ax.set_xlim(-eps, Ncases+eps)\n",
    "\n",
    "rel_score = (PLL_rg_cov - PLL_rg_cov[0:1, :])/fact\n",
    "ax.errorbar(np.arange(rel_score.shape[0])[1:], rel_score.mean(-1)[1:], linestyle='', marker='+', markersize=4, capsize=3, \n",
    "            yerr=rel_score.std(-1, ddof=1)[1:]/np.sqrt(yerr.shape[-1]), c='k')\n",
    "ax.plot(np.linspace(-eps, Ncases+eps, 2), np.zeros(2), 'gray')\n",
    "ax.plot(np.arange(rel_score.shape[0])[:, None].repeat(rel_score.shape[1], axis=1), rel_score, \n",
    "        color='gray', marker='.', markersize=4, alpha=.5)\n",
    "ax.set_ylabel(r'$\\Delta$cvLL ($10^3$)', fontsize=10, labelpad=5)\n",
    "\n",
    "ax.set_xticks(np.arange(PLL_rg_cov.shape[0]))\n",
    "ax.set_xticklabels(['HD', 'HD\\nAHV\\nspeed\\ntime', 'HD\\nAHV\\nspeed\\npos.\\ntime'])\n",
    "#ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# binning\n",
    "BINS = 20\n",
    "binnings = [20, 40, 100, 200, 500]\n",
    "\n",
    "Xs = [0.0, 0.24, 0.48, 0.0, 0.24]\n",
    "Ys = [0.0, 0.0, 0.0, -0.25, -0.25]\n",
    "skips = [40, 20, 5, 2, 1]\n",
    "b = 0\n",
    "for X, Y in zip(Xs, Ys):\n",
    "    skip = skips[b]\n",
    "    widths = np.ones(len(show_neuron))\n",
    "    heights = [1]\n",
    "    spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights), width_ratios=widths, \n",
    "                            height_ratios=heights, wspace=0.6,  \n",
    "                            left=0.35+X, right=.52+X, bottom=0.82+Y, top=0.95+Y)\n",
    "\n",
    "    if b == 1:\n",
    "        addstr = ' (main results)'\n",
    "    else:\n",
    "        addstr = ''\n",
    "    \n",
    "    fig.text(0.425+X, 0.97+Y, '{} ms'.format(binnings[b])+addstr, ha='center', color='gray')\n",
    "    for k, ne in enumerate(show_neuron):\n",
    "        ax = fig.add_subplot(spec[0, k])\n",
    "        if ne < region_edge:\n",
    "            c = poscol\n",
    "        else:\n",
    "            c = antcol\n",
    "        \n",
    "        if b == 0:\n",
    "            ax.set_title('cell {}'.format(ne+1), fontsize=12, color=c, pad=20)\n",
    "        \n",
    "        if k == 0 and b == 3:\n",
    "            ax.set_ylabel('Fano factor', fontsize=10, labelpad=5)\n",
    "        ax.scatter(avg_models[b][ne, ::skip]/binnings[b]*1000, ff_models[b][ne, ::skip], marker='.', alpha=0.3)\n",
    "        ax.set_xlim(0)\n",
    "        #ax.set_ylim(0, 3)\n",
    "        if b == 0:\n",
    "            if k == 0:\n",
    "                ax.set_yticks([.8, 1.])\n",
    "            else:\n",
    "                ax.set_yticks([.6, 1.])\n",
    "\n",
    "        xlims = ax.get_xlim()\n",
    "        xx = np.linspace(xlims[0], xlims[1])\n",
    "        ax.plot(xx, np.ones_like(xx), 'k', alpha=.5)\n",
    "    b += 1\n",
    "\n",
    "X, Y = Xs[3], Ys[3]\n",
    "fig.text(0.425+X, 0.725+Y, 'firing rate (Hz)', fontsize=10, ha='center')\n",
    "\n",
    "\n",
    "X = 0.0\n",
    "Y= 0.0\n",
    "spec = fig.add_gridspec(ncols=1, nrows=1, width_ratios=[1], \n",
    "                        height_ratios=[1], wspace=0.2, hspace=0.2, \n",
    "                        left=0.83+X, right=1.0+X, bottom=0.55+Y, top=.725+Y)\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "lFF = np.log(np.array([ff_models[b].mean(-1) for b in np.arange(5)]))\n",
    "\n",
    "xx = np.arange(5)[:, None].repeat(len(pick_neuron)-region_edge, axis=-1)\n",
    "xxrnd = np.random.rand(*xx.shape)*0.2-0.1\n",
    "ax.scatter(xx+xxrnd+.2, lFF[:, region_edge:], marker='.', s=4, \n",
    "           c=antcol, label='ANT')\n",
    "xx = np.arange(5)[:, None].repeat(region_edge, axis=-1)\n",
    "xxrnd = np.random.rand(*xx.shape)*0.2-0.1\n",
    "ax.scatter(xx+xxrnd-.2, lFF[:, :region_edge], marker='.', s=4, \n",
    "           c=poscol, label='PoS')\n",
    "\n",
    "ax.set_xticks(np.arange(5))\n",
    "ax.set_xticklabels(binnings)\n",
    "ax.set_xlabel('bin size (ms)', fontsize=10, labelpad=5)\n",
    "ax.set_ylabel('log average FF', fontsize=10, labelpad=1)\n",
    "\n",
    "xlims = ax.get_xlim()\n",
    "xx = np.linspace(xlims[0], xlims[1])\n",
    "ax.plot(xx, np.zeros_like(xx), 'k', alpha=.5)\n",
    "ax.set_xlim(xlims)\n",
    "lgnd = ax.legend(handletextpad=0.0, bbox_to_anchor=(0.5, 0.3))\n",
    "lgnd.legendHandles[0]._sizes = [50]\n",
    "lgnd.legendHandles[1]._sizes = [50]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HD - AHV, ATIs\n",
    "R_min = 0.999 # minimum correlation from circular-linear correlation\n",
    "X = 0.0\n",
    "Y = -0.02\n",
    "widths = np.ones(len(show_neuron))\n",
    "heights = [1]\n",
    "spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights),\n",
    "                         height_ratios=heights, wspace=0.3, \n",
    "                        left=0.0+X, right=.2+X, bottom=0.15+Y, top=0.3+Y)\n",
    "\n",
    "for k, ne in enumerate(show_neuron):\n",
    "    ax = fig.add_subplot(spec[0, k])\n",
    "    if ne < region_edge:\n",
    "        c = poscol\n",
    "    else:\n",
    "        c = antcol\n",
    "\n",
    "    fig.text(0.05+X+.1*k, 0.35+Y, 'cell {}'.format(ne+1), fontsize=12, color=c, ha='center')\n",
    "    \n",
    "    rate = field_hdw[ne]/tbin\n",
    "    ax.set_title('{:.1f} Hz'.format(rate.max()), fontsize=10, pad=-5)\n",
    "    im = utils.plot.visualize_field((fig, ax), rate.T, grid_shape_hdw, cbar=False, aspect='auto')\n",
    "    utils.plot.decorate_ax(ax, spines=[True, True, True, True])\n",
    "    \n",
    "    if k == 0:\n",
    "        ax.set_yticks(grid_shape_hdw[1])\n",
    "        \n",
    "fig.text(-0.06+X, Y+0.225, 'AHV (rad/s)', rotation=90, fontsize=10, va='center')\n",
    "\n",
    "\n",
    "\n",
    "widths = [1]\n",
    "heights = [1, 1, 2]\n",
    "spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights), width_ratios=widths,\n",
    "                        height_ratios=heights, \n",
    "                        left=0.29+X, right=.39+X, bottom=0.2+Y, top=0.3+Y)\n",
    "\n",
    "valid = (res_var[:region_edge] < -R_min)\n",
    "at = ATI[:region_edge][valid]*1000\n",
    "valid = (res_var[region_edge:] < -R_min)\n",
    "at2 = ATI[region_edge:][valid]*1000\n",
    "\n",
    "ax = fig.add_subplot(spec[-1, 0])\n",
    "randnbs = np.random.rand(*at.shape)*0.4-0.2\n",
    "ax.scatter(at, 0*np.ones_like(at)+randnbs, marker='.', s=4, c=poscol)\n",
    "randnbs = np.random.rand(*at2.shape)*0.4-0.2\n",
    "ax.scatter(at2, 1*np.ones_like(at2)+randnbs, marker='.', s=4, c=antcol)\n",
    "ax.set_xlabel('ATI (ms)', labelpad=5, fontsize=10)\n",
    "ax.set_ylim(-1, 2)\n",
    "ax.set_yticks([])\n",
    "\n",
    "bins = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], BINS)\n",
    "ax = fig.add_subplot(spec[1, 0])\n",
    "ax.hist(at, bins=bins, alpha=0.5, density=True, color=poscol)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "ax.hist(at2, bins=bins, alpha=0.5, density=True, color=antcol)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# population drift\n",
    "X = 0.0\n",
    "Y = -0.1\n",
    "widths = np.ones(len(show_neuron))\n",
    "heights = [1]\n",
    "spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights), width_ratios=widths,\n",
    "                        height_ratios=heights, wspace=0.3, \n",
    "                        left=.0+X, right=.2+X, bottom=-0.05+Y, top=0.1+Y)\n",
    "\n",
    "for k, ne in enumerate(show_neuron):\n",
    "    ax = fig.add_subplot(spec[k])\n",
    "    \n",
    "    rate = field_hdt[ne]/tbin\n",
    "    ax.set_title('{:.1f} Hz'.format(rate.max()), fontsize=10, pad=-5)\n",
    "    im = utils.plot.visualize_field((fig, ax), rate.T, grid_shape_hdt, cbar=False, aspect='auto')\n",
    "    utils.plot.decorate_ax(ax, spines=[True, True, True, True])\n",
    "    \n",
    "    if k == 0:\n",
    "        ax.set_xticks([0, 2*np.pi])\n",
    "        ax.set_xticklabels([r'$0$', r'$2\\pi$'])\n",
    "        ax.set_yticks([grid_shape_hdt[1][0], grid_shape_hdt[1][1]//60*60])\n",
    "        ax.set_yticklabels([0, 38])\n",
    "        \n",
    "    rm = rate.max()\n",
    "    \n",
    "fig.text(-.06+X, 0.025+Y, 'time (min)', rotation=90, fontsize=10, va='center')\n",
    "fig.text(.1+X, -0.15+Y, 'head direction', fontsize=10, ha='center')\n",
    "    \n",
    "\n",
    "widths = [1]\n",
    "heights = [1, 1, 2]\n",
    "spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights), width_ratios=widths,\n",
    "                        height_ratios=heights, \n",
    "                        left=.29+X, right=.39+X, bottom=0.0+Y, top=.1+Y)\n",
    "\n",
    "d = drift[:region_edge][res_var_drift[:region_edge] < -R_min]/np.pi*180.*3600\n",
    "d2 = drift[region_edge:][res_var_drift[region_edge:] < -R_min]/np.pi*180.*3600\n",
    "\n",
    "ax = fig.add_subplot(spec[-1, 0])\n",
    "randnbs = np.random.rand(*d.shape)*0.4-0.2\n",
    "ax.scatter(d, 0*np.ones_like(d)+randnbs, marker='.', s=4, c=poscol)\n",
    "randnbs = np.random.rand(*d2.shape)*0.4-0.2\n",
    "ax.scatter(d2, 1*np.ones_like(d2)+randnbs, marker='.', s=4, c=antcol)\n",
    "ax.set_xlabel(r'drift ($^\\circ$/hr)', labelpad=5, fontsize=10)\n",
    "ax.set_ylim(-1, 2)\n",
    "ax.set_yticks([])\n",
    "\n",
    "bins = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], BINS)\n",
    "ax = fig.add_subplot(spec[1, 0])\n",
    "ax.hist(d, bins=bins, alpha=0.5, density=True, color=poscol)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "ax.hist(d2, bins=bins, alpha=0.5, density=True, color=antcol)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "# colorbars\n",
    "X = 0.0\n",
    "Y = -0.01\n",
    "cspec = fig.add_gridspec(ncols=1, nrows=1, width_ratios=[1], height_ratios=[1],\n",
    "                         left=0.215+X, right=0.22+X, bottom=-0.05+Y, top=0.2+Y)\n",
    "ax = fig.add_subplot(cspec[0, 0])\n",
    "#ax.set_title('     max', fontsize=10, pad=1)\n",
    "utils.plot.add_colorbar((fig, ax), im, ticktitle='firing rate', ticks=[0, rm], ticklabels=['0', 'max'], \n",
    "                        cbar_pad=0, cbar_fontsize=10, cbar_format=None, cbar_ori='vertical')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### latent variables ###\n",
    "cvs = RMS_cv.shape[0]\n",
    "fact = 10**3\n",
    "\n",
    "X = 0.0\n",
    "Y = -0.15\n",
    "order = [1, 2, 0]\n",
    "widths = [1]\n",
    "heights = [1, 1]\n",
    "spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights), width_ratios=widths,\n",
    "                        height_ratios=heights, hspace=0.3, \n",
    "                        left=.55+X, right=0.625+X, bottom=0.05+Y, top=.45+Y)\n",
    "\n",
    "c_ = LVM_cv_ll.transpose(1, 0, 2).mean(-1)[order, :]/fact\n",
    "rel_c = c_ - c_[0:1, :]\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0]) \n",
    "ax.errorbar(np.arange(rel_c.shape[0])[1:], rel_c.mean(-1)[1:], linestyle='', marker='+', markersize=4, capsize=3, \n",
    "            yerr=rel_c.std(-1, ddof=1)[1:]/np.sqrt(rel_c.shape[-1]), c='k')\n",
    "ax.plot(np.linspace(-eps, Ncases+eps, 2), np.zeros(2), 'gray')\n",
    "ax.plot(np.arange(rel_c.shape[0])[:, None].repeat(rel_c.shape[1], axis=1), rel_c, \n",
    "        color='gray', marker='.', markersize=4, alpha=.5)\n",
    "\n",
    "ax.set_xticks(np.arange(3))\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "ax.set_ylabel(r'$\\Delta$cvLL ($10^3$)', labelpad=2, fontsize=10)\n",
    "ax.set_xlim(-0.5, 0.5+cvs-1)\n",
    "\n",
    "ax = fig.add_subplot(spec[1, 0])\n",
    "ax.set_xlim(-eps, Ncases+eps)\n",
    "cvtrials = RMS_cv.shape[1]\n",
    "yerr = RMS_cv.std(1, ddof=1)/np.sqrt(cvtrials)\n",
    "ax.bar(np.arange(cvs), RMS_cv.mean(1)[order], yerr=yerr[order], capsize=3, color=[0.5, 0.5, 0.5], width=0.5)\n",
    "    \n",
    "ax.set_ylim(0)\n",
    "ax.set_xticks(np.arange(cvs))\n",
    "ax.set_xlim(-0.5, 0.5+cvs-1)\n",
    "ax.set_xticklabels(['Poisson', 'hNB', 'Universal'], rotation=90)\n",
    "ax.set_ylabel('RMSE', labelpad=5, fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# posterior\n",
    "fig.text(0.865+X, 0.47+Y, 'Universal', fontsize=12, ha='center')\n",
    "widths = [1, 0.5]\n",
    "heights = [1, 1]\n",
    "spec = fig.add_gridspec(ncols=len(widths), nrows=len(heights), width_ratios=widths,\n",
    "                        height_ratios=heights, hspace=0.6, wspace=0.6, \n",
    "                        left=.73+X, right=1.+X, bottom=0.0+Y, top=.45+Y)\n",
    "\n",
    "# lat_t, RMS_cv, LVM_cv_ll, drifts_lv\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "\n",
    "T = 300\n",
    "T_start = 2500\n",
    "\n",
    "ax.set_xlim([0, tbin*T])\n",
    "ax.set_xticks([])\n",
    "ax.set_xlabel('time', fontsize=10, labelpad=5)\n",
    "ax.set_ylabel(r'$z$', fontsize=10, labelpad=0)\n",
    "ax.set_ylim([0, 2*np.pi])\n",
    "ax.set_yticks([0, 2*np.pi])\n",
    "ax.set_yticklabels([r'$0$', r'$2\\pi$'])\n",
    "\n",
    "#ax.set_title(r'posterior $q_{\\varphi}(z)$', fontsize=12, pad=7)\n",
    "d_m = drifts_lv[0].mean(-1)/np.pi*180*3600\n",
    "d_s = drifts_lv[0].std(-1)/np.pi*180*3600/np.sqrt(drifts_lv.shape[-1]-1)\n",
    "ax.text(tbin*T*1.1, -1.0, 'drift:\\n '+r'${:.1f}\\pm{:.1f} ^\\circ$/hr'.format(d_m, d_s), color='gray')\n",
    "utils.plot.plot_circ_posterior(ax, tbin*np.arange(T), rcov_lvm[0][T_start:T_start+T] % (2*np.pi), None, col='k', \n",
    "                               linewidth=1.0, step=1, l='truth')\n",
    "\n",
    "utils.plot.plot_circ_posterior(ax, tbin*np.arange(T), lat_t[T_start:T_start+T], \n",
    "                               lat_t_std[T_start:T_start+T], col='tab:blue', \n",
    "                               linewidth=.7, step=1, alpha=0.3, line_alpha=0.5, l_std='var. post.')\n",
    "\n",
    "leg = ax.legend(bbox_to_anchor=(1.05, 1.2), handlelength=0.8)\n",
    "for l in leg.get_lines()[1:]:\n",
    "    l.set_linewidth(3)\n",
    "\n",
    "\n",
    "# delay\n",
    "ax = fig.add_subplot(spec[1, 0])\n",
    "shift_times = 0.1*(np.arange(delay_RMS.shape[0]) - delay_RMS.shape[0] // 2)\n",
    "_arr = delay_RMS.mean(-1)\n",
    "\n",
    "m = _arr\n",
    "s = _arr.std(-1) / np.sqrt(_arr.shape[-1]-1)\n",
    "line, = ax.plot(shift_times, m, marker='.')\n",
    "ax.fill_between(\n",
    "    shift_times, m-s,\n",
    "    m+s, color=line.get_color(), alpha=0.5\n",
    ")\n",
    "ax.set_xlim([shift_times[0], shift_times[-1]])\n",
    "ax.set_xlabel('behaviour shift (s)', fontsize=10, labelpad=5)\n",
    "ax.set_ylabel('RMSE', fontsize=10, labelpad=5)\n",
    "\n",
    "\n",
    "# scatter comparison\n",
    "ax = fig.add_subplot(spec[1, 1])\n",
    "ax.set_aspect(1)\n",
    "ax.scatter(rcov_lvm[0], lat_t, marker='.', alpha=0.3)\n",
    "ax.set_xlim([0, 2*np.pi])\n",
    "ax.set_xticks([0, 2*np.pi])\n",
    "ax.set_xticklabels([r'$0$', r'$2\\pi$'])\n",
    "ax.set_ylim([0, 2*np.pi])\n",
    "ax.set_yticks([0, 2*np.pi])\n",
    "ax.set_yticklabels([r'$0$', r'$2\\pi$'])\n",
    "ax.set_ylabel(r'$z$', labelpad=-4, fontsize=10)\n",
    "ax.set_xlabel('head direction', labelpad=0, fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('output/plot_hdc_add.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some extra analysis below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean drit in degrees/hr\n",
    "drift.mean()/np.pi*180*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-sample t-test for LVM scores of Universal versus Poisson delta cvLL\n",
    "order = [1, 2, 0]\n",
    "c_ = LVM_cv_ll.transpose(1, 0, 2).mean(-1)[order, :]\n",
    "rel_c = c_ - c_[0:1, :]\n",
    "score_err = rel_c.std(-1, ddof=1)/np.sqrt(rel_c.shape[-1])\n",
    "\n",
    "scipy.stats.ttest_1samp(rel_c[2, :], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting linear joint tuning cells, visualize residuals for all cells and chosen cutoff line\n",
    "plt.scatter(np.arange(33), res_var_drift)\n",
    "plt.plot(np.ones(33)*-0.999)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(np.arange(33), res_var)\n",
    "plt.plot(np.ones(33)*-0.999)\n",
    "plt.ylim(-1, -0.999)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model(session_id, phase, cvdata, resamples, binsize, \n",
    "                   m, rcov, max_count, neurons, gpu):\n",
    "    if session_id == 'Mouse12-120806':\n",
    "        session_id = 0\n",
    "    elif session_id == 'Mouse28-140313':\n",
    "        session_id = 1\n",
    "        \n",
    "    if phase == 'wake':\n",
    "        phase = 1\n",
    "    elif phase == 'sleep':\n",
    "        phase = 0\n",
    "\n",
    "    mtype, ll_mode, r_mode, num_induc, inv_link, C, z_dims, delays, folds, cv_switch, basis_mode = m\n",
    "    shared_W = False\n",
    "    enc_layers, basis = models.hyper_params(basis_mode)\n",
    "    kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "\n",
    "    if ll_mode == 'U':\n",
    "        mapping_net = models.net(C, basis, max_count, neurons, shared_W)\n",
    "    else:\n",
    "        mapping_net = None\n",
    "\n",
    "    full_model, _ = models.set_model(max_count, mtype, r_mode, ll_mode, fcov, neurons, \n",
    "                                     tbin, ftrain, num_induc, batch_size=batch_size, inv_link=inv_link, \n",
    "                                     mapping_net=mapping_net, C=C, enc_layers=enc_layers)\n",
    "    full_model.to(dev)\n",
    "\n",
    "\n",
    "    name = 'HDC{}'.format(binsize)\n",
    "    if shared_W:\n",
    "        name += 'S'\n",
    "    if basis_mode != 'ew':\n",
    "        name += basis_mode\n",
    "        \n",
    "    model_name = '{}{}{}_{}_{}_{}_C={}_{}'.format(name, session_id, phase, mtype, ll_mode, r_mode, C, kcv)\n",
    "    if cv_switch:\n",
    "        model_name += '_'\n",
    "    checkpoint = torch.load('../scripts/checkpoint/' + model_name, map_location='cuda:{}'.format(gpu))\n",
    "    full_model.load_state_dict(checkpoint['full_model'])\n",
    "    return full_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 'Mouse28-140313'\n",
    "phase = 'wake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'U', 'hd', 8, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_t', 48, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t', 64, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'IP', 'hd_w_s_pos_t', 64, 'exp', 1, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'hNB', 'hd_w_s_pos_t', 64, 'exp', 1, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t', 64, 'identity', 3, [], False, 10, False, 'qd')]\n",
    "\n",
    "bn = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn, '../scripts/data')\n",
    "\n",
    "left_x = rcov[3].min()\n",
    "right_x = rcov[3].max()\n",
    "bottom_y = rcov[4].min()\n",
    "top_y = rcov[4].max()\n",
    "\n",
    "pick_neuron = list(range(neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### statistics over the behaviour ###\n",
    "avg_models = []\n",
    "var_models = []\n",
    "ff_models = []\n",
    "\n",
    "binnings = [20, 40, 100, 200, 500]\n",
    "\n",
    "for bn in binnings:\n",
    "\n",
    "    rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn, '../scripts/data')\n",
    "    max_count = int(rc_t.max())\n",
    "    x_counts = torch.arange(max_count+1)\n",
    "    \n",
    "    mode = modes[2]\n",
    "    cvdata = model_utils.get_cv_sets(mode, [2], 5000, rc_t, resamples, rcov)[0]\n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "\n",
    "    avg_model = []\n",
    "    var_model = []\n",
    "    ff_model = []\n",
    "\n",
    "    for b in range(full_model.inputs.batches):\n",
    "        P_mc = model_utils.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0).cpu()\n",
    "\n",
    "        avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "        var = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "        ff = var/(avg+1e-12)\n",
    "        avg_model.append(avg)\n",
    "        var_model.append(var)\n",
    "        ff_model.append(ff)\n",
    "\n",
    "    avg_models.append(torch.cat(avg_model, dim=-1).mean(0).numpy())\n",
    "    var_models.append(torch.cat(var_model, dim=-1).mean(0).numpy())\n",
    "    ff_models.append(torch.cat(ff_model, dim=-1).mean(0).numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS framework for regression models\n",
    "CV = [2, 5, 8]\n",
    "\n",
    "### KS test over binnings ###\n",
    "Qq_bn = []\n",
    "Zz_bn = []\n",
    "R_bn = []\n",
    "Rp_bn = []\n",
    "mode = modes[2]\n",
    "\n",
    "N = len(pick_neuron)\n",
    "for kcv in CV:\n",
    "    for en, bn in enumerate(binnings):\n",
    "        cvdata = model_utils.get_cv_sets(mode, [kcv], 3000, rc_t, resamples, rcov)[0]\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        time_steps = ftrain.shape[-1]\n",
    "\n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "        if en == 0:\n",
    "            q_ = []\n",
    "            Z_ = []\n",
    "            for b in range(full_model.inputs.batches): # predictive posterior\n",
    "                P_mc = model_utils.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "                P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "                for n in range(N):\n",
    "                    spike_binned = full_model.likelihood.spikes[b][0, pick_neuron[n], :].numpy()\n",
    "                    q, Z = model_utils.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "                    q_.append(q)\n",
    "                    Z_.append(Z)\n",
    "\n",
    "            q = []\n",
    "            Z = []\n",
    "            for n in range(N):\n",
    "                q.append(np.concatenate(q_[n::N]))\n",
    "                Z.append(np.concatenate(Z_[n::N]))\n",
    "\n",
    "        elif en > 0:\n",
    "            cov_used = models.cov_used(mode[2], fcov)\n",
    "            q = model_utils.compute_count_stats(full_model, mode[1], tbin, ftrain, cov_used, pick_neuron, \\\n",
    "                                           traj_len=1, start=0, T=time_steps, bs=5000)\n",
    "            Z = [utils.stats.q_to_Z(q_) for q_ in q]    \n",
    "\n",
    "        Pearson_s = []\n",
    "        for n in range(len(pick_neuron)):\n",
    "            for m in range(n+1, len(pick_neuron)):\n",
    "                r, r_p = scstats.pearsonr(Z[n], Z[m]) # Pearson r correlation test\n",
    "                Pearson_s.append((r, r_p))\n",
    "\n",
    "        r = np.array([p[0] for p in Pearson_s])\n",
    "        r_p = np.array([p[1] for p in Pearson_s])\n",
    "\n",
    "        Qq_bn.append(q)\n",
    "        Zz_bn.append(Z)\n",
    "        R_bn.append(r)\n",
    "        Rp_bn.append(r_p)\n",
    "        \n",
    "        \n",
    "q_DS_bn = []\n",
    "T_DS_bn = []\n",
    "T_KS_bn = []\n",
    "for q in Qq_bn:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_ll.append(T_DS)\n",
    "        T_KS_ll.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_ll.append(utils.stats.Z_to_q(Z_DS))\n",
    "\n",
    "Qq_bn = np.array(Qq_bn).reshape(len(CV), len(binnings), -1)\n",
    "Zz_bn = np.array(Zz_bn).reshape(len(CV), len(binnings), -1)\n",
    "R_bn = np.array(R_bn).reshape(len(CV), len(binnings), -1)\n",
    "Rp_bn = np.array(Rp_bn).reshape(len(CV), len(binnings), -1)\n",
    "        \n",
    "q_DS_bn = np.array(q_DS_bn).reshape(len(CV), len(binnings), -1)\n",
    "T_DS_bn = np.array(T_DS_bn).reshape(len(CV), len(binnings), -1)\n",
    "T_KS_bn = np.array(T_KS_bn).reshape(len(CV), len(binnings), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn, '../scripts/data')\n",
    "max_count = int(rc_t.max())\n",
    "x_counts = torch.arange(max_count+1)\n",
    "\n",
    "HD_offset = -1.0 # global shift of head direction coordinates, makes plots better as the preferred head directions are not at axis lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "PLL_rg_ll = []\n",
    "PLL_rg_cov = []\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "\n",
    "beta = 0.0\n",
    "batchsize = 5000\n",
    "\n",
    "PLL_rg_ll = []\n",
    "Ms = modes[2:5]\n",
    "for mode in Ms: # likelihood\n",
    "    \n",
    "    for cvdata in model_utils.get_cv_sets(mode, kcvs, batchsize, rc_t, resamples, rcov):\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "    \n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, mode, \n",
    "                                    rcov, max_count, neurons, gpu=gpu_dev)\n",
    "        PLL_rg_ll.append(model_utils.RG_pred_ll(full_model, mode[2], models.cov_used, cv_set, bound='ELBO', \n",
    "                                           beta=beta, neuron_group=None, ll_mode='GH', ll_samples=100))\n",
    "    \n",
    "PLL_rg_ll = np.array(PLL_rg_ll).reshape(len(Ms), len(kcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CV = [2, 5, 8] # validation segments from splitting data into 10\n",
    "\n",
    "### KS test ###\n",
    "Qq_ll = []\n",
    "Zz_ll = []\n",
    "R_ll = []\n",
    "Rp_ll = []\n",
    "\n",
    "batch_size = 3000\n",
    "N = len(pick_neuron)\n",
    "for kcv in CV:\n",
    "    for en, mode in enumerate(Ms):\n",
    "        cvdata = model_utils.get_cv_sets(mode, [kcv], batch_size, rc_t, resamples, rcov)[0]\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        time_steps = ftrain.shape[-1]\n",
    "\n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "        if en == 0:\n",
    "            q_ = []\n",
    "            Z_ = []\n",
    "            for b in range(full_model.inputs.batches): # predictive posterior\n",
    "                P_mc = model_utils.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "                P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "                for n in range(N):\n",
    "                    spike_binned = full_model.likelihood.spikes[b][0, pick_neuron[n], :].numpy()\n",
    "                    q, Z = model_utils.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "                    q_.append(q)\n",
    "                    Z_.append(Z)\n",
    "\n",
    "            q = []\n",
    "            Z = []\n",
    "            for n in range(N):\n",
    "                q.append(np.concatenate(q_[n::N]))\n",
    "                Z.append(np.concatenate(Z_[n::N]))\n",
    "\n",
    "        elif en > 0:\n",
    "            cov_used = models.cov_used(mode[2], fcov)\n",
    "            q = model_utils.compute_count_stats(full_model, mode[1], tbin, ftrain, cov_used, pick_neuron, \\\n",
    "                                            traj_len=1, start=0, T=time_steps, bs=5000)\n",
    "            Z = [utils.stats.q_to_Z(q_) for q_ in q]    \n",
    "\n",
    "        Pearson_s = []\n",
    "        for n in range(len(pick_neuron)):\n",
    "            for m in range(n+1, len(pick_neuron)):\n",
    "                r, r_p = scstats.pearsonr(Z[n], Z[m]) # Pearson r correlation test\n",
    "                Pearson_s.append((r, r_p))\n",
    "\n",
    "        r = np.array([p[0] for p in Pearson_s])\n",
    "        r_p = np.array([p[1] for p in Pearson_s])\n",
    "\n",
    "        Qq_ll.append(q)\n",
    "        Zz_ll.append(Z)\n",
    "        R_ll.append(r)\n",
    "        Rp_ll.append(r_p)\n",
    "        \n",
    "        \n",
    "q_DS_ll = []\n",
    "T_DS_ll = []\n",
    "T_KS_ll = []\n",
    "for q in Qq_ll:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_ll.append(T_DS)\n",
    "        T_KS_ll.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_ll.append(utils.stats.Z_to_q(Z_DS))\n",
    "\n",
    "Qq_ll = np.array(Qq_ll).reshape(len(CV), len(Ms), -1)\n",
    "Zz_ll = np.array(Zz_ll).reshape(len(CV), len(Ms), -1)\n",
    "R_ll = np.array(R_ll).reshape(len(CV), len(Ms), -1)\n",
    "Rp_ll = np.array(Rp_ll).reshape(len(CV), len(Ms), -1)\n",
    "        \n",
    "q_DS_ll = np.array(q_DS_ll).reshape(len(CV), len(Ms), -1)\n",
    "T_DS_ll = np.array(T_DS_ll).reshape(len(CV), len(Ms), -1)\n",
    "T_KS_ll = np.array(T_KS_ll).reshape(len(CV), len(Ms), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLL_rg_cov = []\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "\n",
    "Ms = modes[:3]\n",
    "for mode in Ms: # input space\n",
    "    \n",
    "    for cvdata in model_utils.get_cv_sets(mode, kcvs, batchsize, rc_t, resamples, rcov):\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "    \n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, mode, \n",
    "                                    rcov, max_count, neurons, gpu=gpu_dev)\n",
    "        PLL_rg_cov.append(model_utils.RG_pred_ll(full_model, mode[2], models.cov_used, cv_set, bound='ELBO', \n",
    "                                                 beta=beta, neuron_group=None, ll_mode='GH', ll_samples=100))\n",
    "    \n",
    "PLL_rg_cov = np.array(PLL_rg_cov).reshape(len(Ms), len(kcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load universal regression model\n",
    "mode = modes[2]\n",
    "kcv = -1 # fit on the full dataset\n",
    "cvdata = model_utils.get_cv_sets(mode, [kcv], 3000, rc_t, resamples, rcov)[0]\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, bn, mode, rcov, \n",
    "                            max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "TT = tbin*resamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# marginalized tuning curves\n",
    "MC = 100\n",
    "skip = 10\n",
    "batch_size = 10000\n",
    "\n",
    "\n",
    "### hd ###\n",
    "steps = 100\n",
    "P_tot = model_utils.marginalized_P(full_model, [np.linspace(0, 2*np.pi, steps)], [0], rcov, batch_size, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                             smooth_length=5, padding_mode='circular')\n",
    "mhd_lower, mhd_mean, mhd_upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                            smooth_length=5, padding_mode='circular')\n",
    "mhd_fflower, mhd_ffmean, mhd_ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "# total variance decomposition\n",
    "hd_mean_EV = avg.var(0).mean(-1)\n",
    "hd_mean_VE = avg.mean(0).var(-1)\n",
    "hd_ff_EV = avg.var(0).mean(-1)\n",
    "hd_ff_VE = avg.mean(0).var(-1)\n",
    "\n",
    "# TI\n",
    "hd_mean_tf = (mhd_mean.max(dim=-1)[0] - mhd_mean.min(dim=-1)[0]) / (mhd_mean.max(dim=-1)[0] + mhd_mean.min(dim=-1)[0])\n",
    "hd_ff_tf = (mhd_ffmean.max(dim=-1)[0] - mhd_ffmean.min(dim=-1)[0]) /(mhd_ffmean.max(dim=-1)[0] + mhd_ffmean.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "### omega ###\n",
    "steps = 100\n",
    "w_edge = (-rcov[1].min()+rcov[1].max())/2.\n",
    "covariates_w = np.linspace(-w_edge, w_edge, steps)\n",
    "P_tot = model_utils.marginalized_P(full_model, [covariates_w], [1], rcov, batch_size, \n",
    "                                   pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mw_mean = avg.mean(0)\n",
    "mw_ff = ff.mean(0)\n",
    "w_mean_tf = (mw_mean.max(dim=-1)[0] - mw_mean.min(dim=-1)[0]) / (mw_mean.max(dim=-1)[0] + mw_mean.min(dim=-1)[0])\n",
    "w_ff_tf = (mw_ff.max(dim=-1)[0] - mw_ff.min(dim=-1)[0]) /(mw_ff.max(dim=-1)[0] + mw_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "### speed ###\n",
    "steps = 100\n",
    "P_tot = model_utils.marginalized_P(full_model, [np.linspace(0, 30., steps)], [2], rcov, batch_size, \n",
    "                                   pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "ms_mean = avg.mean(0)\n",
    "ms_ff = ff.mean(0)\n",
    "s_mean_tf = (ms_mean.max(dim=-1)[0] - ms_mean.min(dim=-1)[0]) / (ms_ff.max(dim=-1)[0] + ms_ff.min(dim=-1)[0])\n",
    "s_ff_tf = (ms_ff.max(dim=-1)[0] - ms_ff.min(dim=-1)[0]) /(ms_ff.max(dim=-1)[0] + ms_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "### time ###\n",
    "steps = 100\n",
    "P_tot = model_utils.marginalized_P(full_model, [np.linspace(0, TT, steps)], [5], rcov, batch_size, \n",
    "                                   pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mt_mean = avg.mean(0)\n",
    "mt_ff = ff.mean(0)\n",
    "t_mean_tf = (mt_mean.max(dim=-1)[0] - mt_mean.min(dim=-1)[0]) / (mt_ff.max(dim=-1)[0] + mt_ff.min(dim=-1)[0])\n",
    "t_ff_tf = (mt_ff.max(dim=-1)[0] - mt_ff.min(dim=-1)[0]) /(mt_ff.max(dim=-1)[0] + mt_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "\n",
    "### position ###\n",
    "grid_size_pos = (12, 10)\n",
    "grid_shape_pos = [[left_x, right_x], [bottom_y, top_y]]\n",
    "\n",
    "steps = np.product(grid_size_pos)\n",
    "A, B = grid_size_pos\n",
    "\n",
    "cov_list = [np.linspace(left_x, right_x, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "            np.linspace(bottom_y, top_y, B)[None, :].repeat(A, axis=0).flatten()]\n",
    "                      \n",
    "P_tot = model_utils.marginalized_P(full_model, cov_list, [3, 4], rcov, 10000, \n",
    "                                   pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mpos_mean = avg.mean(0)\n",
    "mpos_ff = ff.mean(0)\n",
    "pos_mean_tf = (mpos_mean.max(dim=-1)[0] - mpos_mean.min(dim=-1)[0]) / (mpos_mean.max(dim=-1)[0] + mpos_mean.min(dim=-1)[0])\n",
    "pos_ff_tf = (mpos_ff.max(dim=-1)[0] - mpos_ff.min(dim=-1)[0]) / (mpos_ff.max(dim=-1)[0] + mpos_ff.min(dim=-1)[0])\n",
    "mpos_mean = mpos_mean.reshape(-1, A, B)\n",
    "mpos_ff = mpos_ff.reshape(-1, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional tuning curves\n",
    "MC = 300\n",
    "MC_ = 100\n",
    "\n",
    "\n",
    "### head direction tuning ###\n",
    "steps = 100\n",
    "covariates = [np.linspace(0, 2*np.pi, steps)-HD_offset, \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps)]\n",
    "\n",
    "P_mc = model_utils.compute_P(full_model, covariates, pick_neuron, MC=MC).cpu()\n",
    "\n",
    "\n",
    "avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "xcvar = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "ff = xcvar/avg\n",
    "\n",
    "avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                             smooth_length=5, padding_mode='circular')\n",
    "lower_hd, mean_hd, upper_hd = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                            smooth_length=5, padding_mode='circular')\n",
    "fflower_hd, ffmean_hd, ffupper_hd = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "covariates_hd = np.linspace(0, 2*np.pi, steps)\n",
    "\n",
    "\n",
    "\n",
    "### hd_w ###\n",
    "grid_size_hdw = (51, 41)\n",
    "grid_shape_hdw = [[0, 2*np.pi], [-10., 10.]]\n",
    "\n",
    "steps = np.product(grid_size_hdw)\n",
    "A, B = grid_size_hdw\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              np.linspace(-10., 10., B)[None, :].repeat(A, axis=0).flatten(), \n",
    "              0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps)]\n",
    "\n",
    "P_mean = model_utils.compute_P(full_model, covariates, pick_neuron, MC=MC).mean(0).cpu()\n",
    "field_hdw = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# compute preferred HD\n",
    "grid = (101, 21)\n",
    "grid_shape = [[0, 2*np.pi], [-10., 10.]]\n",
    "\n",
    "steps = np.product(grid)\n",
    "A, B = grid\n",
    "\n",
    "w_arr = np.linspace(-10., 10., B)\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              w_arr[None, :].repeat(A, axis=0).flatten(), \n",
    "              0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps)]\n",
    "\n",
    "P_mean = model_utils.compute_P(full_model, covariates, pick_neuron, MC=MC).mean(0).cpu()\n",
    "field = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "Z = np.cos(covariates[0]) + np.sin(covariates[0])*1j # CoM angle\n",
    "Z = Z[None, :].reshape(-1, A, B)\n",
    "pref_hdw = (np.angle((Z*field).mean(1)) % (2*np.pi)) # neurons, w\n",
    "\n",
    "\n",
    "# ATI\n",
    "ATI = []\n",
    "res_var = []\n",
    "for k in range(neurons):\n",
    "    _, a, shift, losses = utils.signal.circ_lin_regression(pref_hdw[k, :], w_arr/(2*np.pi), dev='cpu', iters=1000, lr=1e-2)\n",
    "    ATI.append(-a)\n",
    "    res_var.append(losses[-1])\n",
    "ATI = np.array(ATI)\n",
    "res_var = np.array(res_var)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### omega tuning ###\n",
    "mean_w = []\n",
    "lower_w = []\n",
    "upper_w = []\n",
    "ffmean_w = []\n",
    "fflower_w = []\n",
    "ffupper_w = []\n",
    "\n",
    "steps = 100\n",
    "w_edge = (-rcov[1].min()+rcov[1].max())/2.\n",
    "covariates_w = np.linspace(-w_edge, w_edge, steps)\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  covariates_w, \n",
    "                  0.*np.ones(steps), \n",
    "                  (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                  0.*np.ones(steps)]\n",
    "\n",
    "    P_mc = model_utils.compute_P(full_model, covariates, [n], MC=MC)[:, 0, ...].cpu()\n",
    "\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "    xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "    ff = xcvar/avg\n",
    "\n",
    "    avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                 smooth_length=5, padding_mode='replicate')\n",
    "    lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "    ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "    fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "    \n",
    "    lower_w.append(lower)\n",
    "    mean_w.append(mean)\n",
    "    upper_w.append(upper)\n",
    "    \n",
    "    fflower_w.append(fflower)\n",
    "    ffmean_w.append(ffmean)\n",
    "    ffupper_w.append(ffupper)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### hd_t ###\n",
    "grid_size_hdt = (51, 41)\n",
    "grid_shape_hdt = [[0, 2*np.pi], [0., TT]]\n",
    "\n",
    "steps = np.product(grid_size_hdt)\n",
    "A, B = grid_size_hdt\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              np.linspace(0., TT, B)[None, :].repeat(A, axis=0).flatten()]\n",
    "\n",
    "P_mean = model_utils.compute_P(full_model, covariates, pick_neuron, MC=MC_).mean(0).cpu()\n",
    "field_hdt = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# drift and similarity matrix\n",
    "grid = (201, 16)\n",
    "grid_shape = [[0, 2*np.pi], [0., TT]]\n",
    "\n",
    "steps = np.product(grid)\n",
    "A, B = grid\n",
    "\n",
    "t_arr = np.linspace(0., TT, B)\n",
    "dt_arr = t_arr[1]-t_arr[0]\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              t_arr[None, :].repeat(A, axis=0).flatten()]\n",
    "\n",
    "P_mean = model_utils.compute_P(full_model, covariates, pick_neuron, MC=MC_).mean(0).cpu()\n",
    "field = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "Z = np.cos(covariates[0]) + np.sin(covariates[0])*1j # CoM angle\n",
    "Z = Z[None, :].reshape(-1, A, B)\n",
    "E_exp = (Z*field).sum(-2)/field.sum(-2)\n",
    "pref_hdt = (np.angle(E_exp) % (2*np.pi)) # neurons, t\n",
    "\n",
    "tun_width = 1.-np.abs(E_exp)\n",
    "amp_t = field.mean(-2) # mean amplitude\n",
    "ampm_t = field.max(-2)\n",
    "\n",
    "sim_mat = []\n",
    "act = (field-field.mean(-2, keepdims=True))/field.std(-2, keepdims=True)\n",
    "en = np.argsort(pref_hdt, axis=0)\n",
    "for t in range(B):\n",
    "    a = act[en[:, t], :, t]\n",
    "    sim_mat = ((a[:, None, :]*a[None, ...]).mean(-1))\n",
    "\n",
    "\n",
    "\n",
    "drift = []\n",
    "res_var_drift = []\n",
    "for k in range(len(pick_neuron)):\n",
    "    _, a, shift, losses = utils.signal.circ_lin_regression(pref_hdt[k, :], t_arr/(2*np.pi)/1e2, \n",
    "                                                           dev='cpu', iters=1000, lr=1e-2)\n",
    "    drift.append(a/1e2)\n",
    "    res_var_drift.append(losses[-1])\n",
    "drift = np.array(drift)\n",
    "res_var_drift = np.array(res_var_drift)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### speed ###\n",
    "mean_s = []\n",
    "lower_s = []\n",
    "upper_s = []\n",
    "ffmean_s = []\n",
    "fflower_s = []\n",
    "ffupper_s = []\n",
    "    \n",
    "steps = 100\n",
    "covariates_s = np.linspace(0, 30., steps)\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  0.*np.ones(steps), covariates_s, \n",
    "                  (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                  0.*np.ones(steps)]\n",
    "\n",
    "    P_mc = model_utils.compute_P(full_model, covariates, [n], MC=MC)[:, 0, ...].cpu()\n",
    "\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "    xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "    ff = xcvar/avg\n",
    "\n",
    "    avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                 smooth_length=5, padding_mode='replicate')\n",
    "    lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "    \n",
    "    ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "    fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "    lower_s.append(lower)\n",
    "    mean_s.append(mean)\n",
    "    upper_s.append(upper)\n",
    "    \n",
    "    fflower_s.append(fflower)\n",
    "    ffmean_s.append(ffmean)\n",
    "    ffupper_s.append(ffupper)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### time ###\n",
    "mean_t = []\n",
    "lower_t = []\n",
    "upper_t = []\n",
    "ffmean_t = []\n",
    "fflower_t = []\n",
    "ffupper_t = []\n",
    "    \n",
    "steps = 100\n",
    "covariates_t = np.linspace(0, TT, steps)\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  0.*np.ones(steps), 0.*np.ones(steps), \n",
    "                  (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                  covariates_t]\n",
    "\n",
    "    P_mc = model_utils.compute_P(full_model, covariates, [n], MC=MC)[:, 0, ...].cpu()\n",
    "\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "    xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "    ff = xcvar/avg\n",
    "\n",
    "    avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                 smooth_length=5, padding_mode='replicate')\n",
    "    lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "    \n",
    "    ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "    fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "    lower_t.append(lower)\n",
    "    mean_t.append(mean)\n",
    "    upper_t.append(upper)\n",
    "    \n",
    "    fflower_t.append(fflower)\n",
    "    ffmean_t.append(ffmean)\n",
    "    ffupper_t.append(ffupper)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### pos ###\n",
    "grid_shape_pos = [[left_x, right_x], [bottom_y, top_y]]\n",
    "H = grid_shape_pos[1][1]-grid_shape_pos[1][0]\n",
    "W = grid_shape_pos[0][1]-grid_shape_pos[0][0]\n",
    "grid_size_pos = (int(41*W/H), 41)\n",
    "\n",
    "\n",
    "steps = np.product(grid_size_pos)\n",
    "A, B = grid_size_pos\n",
    "\n",
    "field_pos = []\n",
    "ff_pos = []\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  0.*np.ones(steps), 0.*np.ones(steps), \n",
    "                  np.linspace(left_x, right_x, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "                  np.linspace(bottom_y, top_y, B)[None, :].repeat(A, axis=0).flatten(), \n",
    "                  t*np.ones(steps)]\n",
    "\n",
    "    P_mc = model_utils.compute_P(full_model, covariates, [n], MC=MC_)[:, 0, ...].cpu()\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1).reshape(-1, A, B).numpy()\n",
    "    var = (x_counts[None, None, :]**2*P_mc).sum(-1).reshape(-1, A, B).numpy()\n",
    "    xcvar = (var-avg**2)\n",
    "\n",
    "    field_pos.append(avg.mean(0))\n",
    "    ff_pos.append((xcvar/(avg+1e-12)).mean(0))\n",
    "\n",
    "\n",
    "field_pos = np.stack(field_pos)\n",
    "ff_pos = np.stack(ff_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Pearson correlation between Fano factors and mean firing rates\n",
    "b = 1\n",
    "Pearson_ff = []\n",
    "ratio = []\n",
    "for avg, ff in zip(avg_models[b], ff_models[b]):\n",
    "    r, r_p = scstats.pearsonr(ff, avg) # Pearson r correlation test\n",
    "    Pearson_ff.append((r, r_p))\n",
    "    ratio.append(ff.std()/avg.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    avg_models, var_models, ff_models, \n",
    "    Pearson_ff, ratio, \n",
    "    PLL_rg_ll, PLL_rg_cov, \n",
    "    Qq_ll, Zz_ll, R_ll, Rp_ll, q_DS_ll, T_DS_ll, T_KS_ll, \n",
    "    sign_KS, sign_DS, \n",
    "    mhd_mean, mhd_ff, hd_mean_tf, hd_ff_tf, \n",
    "    mw_mean, mw_ff, w_mean_tf, w_ff_tf, \n",
    "    ms_mean, ms_ff, s_mean_tf, s_ff_tf, \n",
    "    mt_mean, mt_ff, t_mean_tf, t_ff_tf, \n",
    "    mpos_mean, mpos_ff, pos_mean_tf, pos_ff_tf, \n",
    "    covariates_hd, lower_hd, mean_hd, upper_hd, \n",
    "    fflower_hd, ffmean_hd, ffupper_hd, \n",
    "    covariates_s, lower_s, mean_s, upper_s, \n",
    "    fflower_s, ffmean_s, ffupper_s, \n",
    "    covariates_t, lower_t, mean_t, upper_t, \n",
    "    fflower_t, ffmean_t, ffupper_t, \n",
    "    covariates_w, lower_w, mean_w, upper_w, \n",
    "    fflower_w, ffmean_w, ffupper_w, \n",
    "    grid_size_pos, grid_shape_pos, field_pos, ff_pos, \n",
    "    grid_size_hdw, grid_shape_hdw, field_hdw, \n",
    "    grid_size_hdt, grid_shape_hdt, field_hdt, \n",
    "    pref_hdw, ATI, res_var, \n",
    "    pref_hdt, drift, res_var_drift, \n",
    "    tun_width, amp_t, ampm_t, sim_mat, \n",
    "    pick_neuron, max_count, tbin, rcov, region_edge\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./saves/P_HDC_rg40.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn, '../scripts/data')\n",
    "\n",
    "left_x = rcov[3].min()\n",
    "right_x = rcov[3].max()\n",
    "bottom_y = rcov[4].min()\n",
    "top_y = rcov[4].max()\n",
    "\n",
    "pick_neuron = list(range(neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'U', 'hd_w_s_pos_t', 64, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R1', 72, 'identity', 3, [6], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R2', 80, 'identity', 3, [6], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R3', 88, 'identity', 3, [6], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R4', 96, 'identity', 3, [6], False, 10, False, 'ew')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### statistics over the behaviour ###\n",
    "avg_models_z = []\n",
    "var_models_z = []\n",
    "ff_models_z = []\n",
    "\n",
    "batch_size = 5000 # batching data to evaluate over all data\n",
    "kcv = 2\n",
    "\n",
    "bn = 40\n",
    "\n",
    "for mode in modes:\n",
    "\n",
    "    rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn, '../scripts/data')\n",
    "    max_count = int(rc_t.max())\n",
    "    x_counts = torch.arange(max_count+1)\n",
    "    \n",
    "    cvdata = model_utils.get_cv_sets(mode, [kcv], batch_size, rc_t, resamples, rcov)[0]\n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "\n",
    "    avg_model = []\n",
    "    var_model = []\n",
    "    ff_model = []\n",
    "\n",
    "    for b in range(full_model.inputs.batches):\n",
    "        P_mc = model_utils.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0).cpu()\n",
    "\n",
    "        avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "        var = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "        ff = var/(avg+1e-12)\n",
    "        avg_model.append(avg)\n",
    "        var_model.append(var)\n",
    "        ff_model.append(ff)\n",
    "\n",
    "    avg_models_z.append(torch.cat(avg_model, dim=-1).mean(0).numpy())\n",
    "    var_models_z.append(torch.cat(var_model, dim=-1).mean(0).numpy())\n",
    "    ff_models_z.append(torch.cat(ff_model, dim=-1).mean(0).numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1\n",
    "Pearson_ffz = []\n",
    "ratioz = []\n",
    "\n",
    "for d in range(len(avg_models_z)):\n",
    "    Pearson_ffz_ = []\n",
    "    ratioz_ = []\n",
    "    for avg, ff in zip(avg_models_z[d], ff_models_z[d]):\n",
    "        r, r_p = scstats.pearsonr(ff, avg) # Pearson r correlation test\n",
    "        Pearson_ffz_.append((r, r_p))\n",
    "        ratioz_.append(ff.std()/avg.std())\n",
    "        \n",
    "    Pearson_ffz.append(Pearson_ffz_)\n",
    "    ratioz.append(ratioz_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, binning, '../scripts/data')\n",
    "max_count = int(rc_t.max())\n",
    "x_counts = torch.arange(max_count+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBO for models of different dimensions\n",
    "kcvs = [2, 5, 8] # get corresponding training sets\n",
    "Ms = modes[:5]\n",
    "batch_size = 3000 # batching data to evaluate over all data\n",
    "\n",
    "elbo = []\n",
    "for em, mode in enumerate(Ms):\n",
    "    for cvdata in model_utils.get_cv_sets(mode, kcvs, batch_size, rc_t, resamples, rcov):\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, binning, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "        \n",
    "        batches = full_model.likelihood.batches\n",
    "        print(batches)\n",
    "        elbo_ = []\n",
    "        for b in range(batches):\n",
    "            elbo_.append(full_model.objective(b, cov_samples=1, ll_mode='GH', bound='ELBO', neuron=None, \n",
    "                                              beta=1., ll_samples=100).data.cpu().numpy())\n",
    "        elbo.append(np.array(elbo_).mean())\n",
    "        \n",
    "elbo = np.array(elbo).reshape(len(Ms), len(kcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation for dimensionality\n",
    "beta = 0.0\n",
    "n_group = np.arange(5)\n",
    "val_neuron = [n_group, n_group+5, n_group+10, n_group+15, n_group+20, n_group+25, np.arange(3)+30]\n",
    "batch_size = 5000 # batching data to evaluate over all data\n",
    "\n",
    "ncvx = 2\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "Ms = modes[:5]\n",
    "\n",
    "cv_pll = []\n",
    "for em, mode in enumerate(Ms):\n",
    "    for cvdata in model_utils.get_cv_sets(mode, kcvs, batch_size, rc_t, resamples, rcov):\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "        if em > 0:\n",
    "            for v_neuron in val_neuron:\n",
    "                fac = len(n_group)/len(v_neuron)\n",
    "                \n",
    "                prev_ll = np.inf\n",
    "                for tr in range(ncvx):\n",
    "                    full_model = get_full_model(session_id, phase, cvdata, resamples, binning, \n",
    "                                                mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "                    mask = np.ones((neurons,), dtype=bool)\n",
    "                    mask[v_neuron] = False\n",
    "                    f_neuron = np.arange(neurons)[mask]\n",
    "                    ll = model_utils.LVM_pred_ll(full_model, mode[-5], mode[2], models.cov_used, cv_set, f_neuron, v_neuron, \n",
    "                                                 cov_MC=1, ll_MC=10, beta=beta, beta_z=0.0, max_iters=3000)[0]\n",
    "                    if ll < prev_ll:\n",
    "                        prev_ll = ll\n",
    "\n",
    "                cv_pll.append(fac*prev_ll)\n",
    "                \n",
    "        else: # no latent\n",
    "            for v_neuron in val_neuron:\n",
    "                fac = len(n_group)/len(v_neuron)\n",
    "                \n",
    "                full_model = get_full_model(session_id, phase, cvdata, resamples, binning, \n",
    "                                            mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "                cv_pll.append(fac*model_utils.RG_pred_ll(full_model, mode[2], models.cov_used, cv_set, bound='ELBO', \n",
    "                                                         beta=beta, neuron_group=v_neuron, ll_mode='GH', ll_samples=100))\n",
    "\n",
    "        \n",
    "cv_pll = np.array(cv_pll).reshape(len(Ms), len(kcvs), len(val_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latent trajectories and drift timescale of neural tuning for 2D latent model\n",
    "mode = modes[2]\n",
    "batch_size = 5000\n",
    "\n",
    "\n",
    "cvdata = model_utils.get_cv_sets(mode, [-1], batch_size, rc_t, resamples, rcov)[0]\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, binning, mode, rcov, max_count, \n",
    "                            neurons, gpu=gpu_dev)\n",
    "\n",
    "X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "\n",
    "X_c = X_loc[6]\n",
    "X_s = X_std[6]\n",
    "z_tau = tbin/(1-torch.sigmoid(full_model.inputs.p_mu_6).data.cpu().numpy())\n",
    "\n",
    "t_lengths = full_model.mapping.kernel.kern1.lengthscale[:, 0, 0, -3].data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load regression model with most input dimensions\n",
    "mode = modes[4]\n",
    "cvdata = model_utils.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov)[0]\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, 40, mode, rcov, max_count, \n",
    "                            neurons, gpu=gpu_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### head direction tuning ###\n",
    "MC = 100\n",
    "\n",
    "steps = 100\n",
    "covariates = [np.linspace(0, 2*np.pi, steps), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps)]\n",
    "\n",
    "P_mc = model_utils.compute_P(full_model, covariates, pick_neuron, MC=MC).cpu()\n",
    "\n",
    "\n",
    "avg = (x_counts[None, None, None, :]*P_mc).sum(-1).mean(0).numpy()\n",
    "pref_hd = covariates[0][np.argmax(avg, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# marginalized tuning curves\n",
    "rcovz = list(rcov) + [X_c[:, 0], X_c[:, 1]]\n",
    "MC = 10\n",
    "skip = 10\n",
    "\n",
    "\n",
    "\n",
    "### z ###\n",
    "step = 100\n",
    "P_tot = model_utils.marginalized_P(full_model, [np.linspace(-.2, .2, step)], [6], rcovz, 10000, \n",
    "                                   pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mz1_mean = avg.mean(0)\n",
    "mz1_ff = ff.mean(0)\n",
    "z1_mean_tf = (mz1_mean.max(dim=-1)[0] - mz1_mean.min(dim=-1)[0]) / (mz1_mean.max(dim=-1)[0] + mz1_mean.min(dim=-1)[0])\n",
    "z1_ff_tf = (mz1_ff.max(dim=-1)[0] - mz1_ff.min(dim=-1)[0]) /(mz1_ff.max(dim=-1)[0] + mz1_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "\n",
    "step = 100\n",
    "P_tot = model_utils.marginalized_P(full_model, [np.linspace(-.2, .2, step)], [7], rcovz, 10000, \n",
    "                                   pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mz2_mean = avg.mean(0)\n",
    "mz2_ff = ff.mean(0)\n",
    "z2_mean_tf = (mz2_mean.max(dim=-1)[0] - mz2_mean.min(dim=-1)[0]) / (mz2_mean.max(dim=-1)[0] + mz2_mean.min(dim=-1)[0])\n",
    "z2_ff_tf = (mz2_ff.max(dim=-1)[0] - mz2_ff.min(dim=-1)[0]) /(mz2_ff.max(dim=-1)[0] + mz2_ff.min(dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute 2D latent model properties of tuning curves and TI to latent space\n",
    "z_d = 2\n",
    "\n",
    "if z_d == 1: ### latent ###\n",
    "    mean_z = []\n",
    "    lower_z = []\n",
    "    upper_z = []\n",
    "    ffmean_z = []\n",
    "    fflower_z = []\n",
    "    ffupper_z = []\n",
    "\n",
    "    steps = 100\n",
    "    covariates_z = np.linspace(-.2, .2, steps)\n",
    "    for en, n in enumerate(pick_neuron):\n",
    "        # x_t, y_t, s_t, th_t, hd_t, time_t\n",
    "        covariates = [pref_hd[n]*np.ones(steps), 0.*np.ones(steps), np.ones(steps)*0., \n",
    "                      (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                      0.*np.ones(steps), covariates_z]\n",
    "\n",
    "        P_mc = model_utils.compute_P(full_model, covariates, [n], MC=1000).cpu()[:, 0, ...]\n",
    "\n",
    "        avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "        xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "        ff = xcvar/avg\n",
    "\n",
    "        avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                     smooth_length=5, padding_mode='replicate')\n",
    "        lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "        ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "        fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "        lower_z.append(lower)\n",
    "        mean_z.append(mean)\n",
    "        upper_z.append(upper)\n",
    "\n",
    "        fflower_z.append(fflower)\n",
    "        ffmean_z.append(ffmean)\n",
    "        ffupper_z.append(ffupper)\n",
    "    \n",
    "else: ### 2d z ###\n",
    "    grid_size_zz = (41, 41)\n",
    "    grid_shape_zz = [[-.2, .2], [-.2, .2]]\n",
    "\n",
    "    steps = np.product(grid_size_zz)\n",
    "    A, B = grid_size_zz\n",
    "\n",
    "    \n",
    "    field_zz = []\n",
    "    ff_zz = []\n",
    "    t = 0\n",
    "    for en, n in enumerate(pick_neuron):\n",
    "        covariates = [pref_hd[n]*np.ones(steps), \n",
    "                      0.*np.ones(steps), 0.*np.ones(steps), \n",
    "                      (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                      t*np.ones(steps), \n",
    "                      np.linspace(-.2, .2, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "                      np.linspace(-.2, .2, B)[None, :].repeat(A, axis=0).flatten()]\n",
    "\n",
    "        P_mean = model_utils.compute_P(full_model, covariates, [n], MC=100).mean(0).cpu()\n",
    "        avg = (x_counts[None, :]*P_mean[0, ...]).sum(-1).reshape(A, B).numpy()\n",
    "        var = (x_counts[None, :]**2*P_mean[0, ...]).sum(-1).reshape(A, B).numpy()\n",
    "        xcvar = (var-avg**2)\n",
    "\n",
    "        field_zz.append(avg)\n",
    "        ff_zz.append(xcvar/avg)\n",
    "\n",
    "    field_zz = np.stack(field_zz)\n",
    "    ff_zz = np.stack(ff_zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS framework for latent models, including Fisher Z scores\n",
    "CV = [2, 5, 8]\n",
    "bn = 40\n",
    "\n",
    "\n",
    "\n",
    "### KS test ###\n",
    "Qq = []\n",
    "Zz = []\n",
    "R = []\n",
    "Rp = []\n",
    "\n",
    "N = len(pick_neuron)\n",
    "for kcv in CV:\n",
    "    for en, mode in enumerate(modes):\n",
    "        cvdata = model_utils.get_cv_sets(mode, [kcv], 3000, rc_t, resamples, rcov)[0]\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        time_steps = ftrain.shape[-1]\n",
    "\n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "        q_ = []\n",
    "        Z_ = []\n",
    "        for b in range(full_model.inputs.batches): # predictive posterior\n",
    "            P_mc = model_utils.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "            P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "            for n in range(N):\n",
    "                spike_binned = full_model.likelihood.spikes[b][0, pick_neuron[n], :].numpy()\n",
    "                q, Z = model_utils.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "                q_.append(q)\n",
    "                Z_.append(Z)\n",
    "\n",
    "        q = []\n",
    "        Z = []\n",
    "        for n in range(N):\n",
    "            q.append(np.concatenate(q_[n::N]))\n",
    "            Z.append(np.concatenate(Z_[n::N]))\n",
    "\n",
    "\n",
    "        Pearson_s = []\n",
    "        for n in range(len(pick_neuron)):\n",
    "            for m in range(n+1, len(pick_neuron)):\n",
    "                r, r_p = scstats.pearsonr(Z[n], Z[m]) # Pearson r correlation test\n",
    "                Pearson_s.append((r, r_p))\n",
    "\n",
    "        r = np.array([p[0] for p in Pearson_s])\n",
    "        r_p = np.array([p[1] for p in Pearson_s])\n",
    "\n",
    "        Qq.append(q)\n",
    "        Zz.append(Z)\n",
    "        R.append(r)\n",
    "        Rp.append(r_p)\n",
    "\n",
    "\n",
    "fisher_z = []\n",
    "fisher_q = []\n",
    "for en, r in enumerate(R):\n",
    "    fz = 0.5*np.log((1+r)/(1-r))*np.sqrt(time_steps-3)\n",
    "    fisher_z.append(fz)\n",
    "    fisher_q.append(utils.stats.Z_to_q(fz))\n",
    "\n",
    "    \n",
    "q_DS_ = []\n",
    "T_DS_ = []\n",
    "T_KS_ = []\n",
    "for q in Qq:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_.append(T_DS)\n",
    "        T_KS_.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_.append(utils.stats.Z_to_q(Z_DS))\n",
    "        \n",
    "        \n",
    "fisher_z = np.array(fisher_z).reshape(len(CV), len(Ms), -1)\n",
    "fisher_q = np.array(fisher_q).reshape(len(CV), len(Ms), -1)\n",
    "\n",
    "Qq = np.array(Qq).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "Zz = np.array(Zz).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "R = np.array(R).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "Rp = np.array(Rp).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "        \n",
    "q_DS_ = np.array(q_DS_).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "T_DS_ = np.array(T_DS_).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "T_KS_ = np.array(T_KS_).reshape(len(CV), len(Ms), len(pick_neuron), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KS_fishq = []\n",
    "p_KS_fishq = []\n",
    "for q in fisher_q:\n",
    "    for qq in q:\n",
    "        _, T_KS, _, _, _, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_KS_fishq.append(T_KS)\n",
    "        p_KS_fishq.append(p_KS)\n",
    "        \n",
    "T_KS_fishq = np.array(T_KS_fishq).reshape(len(CV), len(Ms))\n",
    "p_KS_fishq = np.array(p_KS_fishq).reshape(len(CV), len(Ms))\n",
    "        \n",
    "        \n",
    "T_KS_ks = []\n",
    "p_KS_ks = []\n",
    "for q in Qq:\n",
    "    for qq in q:\n",
    "        for qqq in qq:\n",
    "            _, T_KS, _, _, _, p_KS = utils.stats.KS_statistics(qqq, alpha=0.05, alpha_s=0.05)\n",
    "            T_KS_ks.append(T_KS)\n",
    "            p_KS_ks.append(p_KS)\n",
    "        \n",
    "T_KS_ks = np.array(T_KS_ks).reshape(len(CV), len(Ms), len(pick_neuron))\n",
    "p_KS_ks = np.array(p_KS_ks).reshape(len(CV), len(Ms), len(pick_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delayed noise or spatiotemporal correlations\n",
    "NN = len(pick_neuron)\n",
    "delays = np.arange(5)\n",
    "R_mat_spt = np.empty((len(Ms), len(delays), NN, NN))\n",
    "R_mat_sptp = np.empty((len(Ms), len(delays), NN, NN))\n",
    "\n",
    "kcv_ind = 1\n",
    "for d, Z_ in enumerate(Zz[kcv_ind]):\n",
    "    steps = len(Z_[0])-len(delays)\n",
    "    \n",
    "    for en, t in enumerate(delays):\n",
    "        Pearson_s = []\n",
    "        for n in range(NN):\n",
    "            for m in range(NN):\n",
    "                r, r_p = scstats.pearsonr(Z_[n][t:t+steps], Z_[m][:-len(delays)]) # Pearson r correlation test\n",
    "                R_mat_spt[d, en, n, m] = r\n",
    "                R_mat_sptp[d, en, n, m] = r_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute timescales for input dimensions from ACG\n",
    "delays = 5000\n",
    "Tsteps = rcov[0].shape[0]\n",
    "L = Tsteps-delays+1\n",
    "acg_rc = []\n",
    "\n",
    "for rc in rcov[:1]: # angular\n",
    "    acg = np.empty(delays)\n",
    "    for d in range(delays):\n",
    "        A = rc[d:d+L]\n",
    "        B = rc[:L]\n",
    "        acg[d] = utils.stats.corr_circ_circ(A, B)\n",
    "    acg_rc.append(acg)\n",
    "\n",
    "for rc in rcov[1:-1]:\n",
    "    acg = np.empty(delays)\n",
    "    for d in range(delays):\n",
    "        A = rc[d:d+L]\n",
    "        B = rc[:L]\n",
    "        acg[d] = ((A-A.mean())*(B-B.mean())).mean()/A.std()/B.std()\n",
    "    acg_rc.append(acg)\n",
    "    \n",
    "\n",
    "acg_z = []\n",
    "for rc in X_c.T:\n",
    "    acg = np.empty(delays)\n",
    "    for d in range(delays):\n",
    "        A = rc[d:d+L]\n",
    "        B = rc[:L]\n",
    "        acg[d] = ((A-A.mean())*(B-B.mean())).mean()/A.std()/B.std()\n",
    "    acg_z.append(acg)\n",
    "    \n",
    "    \n",
    "timescales = []\n",
    "\n",
    "for d in range(len(rcov)-1):\n",
    "    timescales.append(np.where(acg_rc[d] < np.exp(-1))[0][0]*tbin)\n",
    "    \n",
    "for d in range(X_c.shape[-1]):\n",
    "    timescales.append(np.where(acg_z[d] < np.exp(-1))[0][0]*tbin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    avg_models_z, var_models_z, ff_models_z, \n",
    "    Pearson_ffz, ratioz, \n",
    "    X_c, X_s, cv_pll, elbo, z_tau, pref_hd, \n",
    "    grid_size_zz, grid_shape_zz, field_zz, ff_zz, \n",
    "    mz1_mean, mz1_ff, z1_mean_tf, z1_ff_tf, \n",
    "    mz2_mean, mz2_ff, z2_mean_tf, z2_ff_tf, \n",
    "    q_DS_, T_DS_, T_KS_, Qq, Zz, R, Rp, fisher_z, fisher_q, \n",
    "    T_KS_fishq, p_KS_fishq, T_KS_ks, p_KS_ks, \n",
    "    R_mat_spt, R_mat_sptp, \n",
    "    timescales, acg_rc, acg_z, t_lengths\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./saves/P_HDC_nc40.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent variable modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 100\n",
    "rcov_lvm, neurons, tbin, resamples, rc_t, _ = HDC.get_dataset(session_id, phase, binsize, '../scripts/data')\n",
    "max_count = int(rc_t.max())\n",
    "rhd_t = rcov_lvm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'U', 'T1', 8, 'identity', 3, [0], False, 10, False, 'ew'), \n",
    "         ('GP', 'IP', 'T1', 8, 'exp', 1, [0], False, 10, False, 'ew'), \n",
    "         ('GP', 'hNB', 'T1', 8, 'exp', 1, [0], False, 10, False, 'ew')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# likelihood CV over subgroups of neurons as well as validation runs\n",
    "beta = 0.0\n",
    "n_group = np.arange(5)\n",
    "ncvx = 2\n",
    "val_neuron = [n_group, n_group+5, n_group+10, n_group+15, n_group+20, n_group+25, np.arange(3)+30]\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "\n",
    "LVM_cv_ll = []\n",
    "for kcv in kcvs:\n",
    "    for mode in modes:\n",
    "        cvdata = model_utils.get_cv_sets(mode, [kcv], 5000, rc_t, resamples, rcov_lvm)[0]\n",
    "        _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "\n",
    "        for v_neuron in val_neuron:\n",
    "            fac = len(n_group)/len(v_neuron)\n",
    "\n",
    "            prev_ll = np.inf\n",
    "            for tr in range(ncvx):\n",
    "                full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                                            mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "                mask = np.ones((neurons,), dtype=bool)\n",
    "                mask[v_neuron] = False\n",
    "                f_neuron = np.arange(neurons)[mask]\n",
    "                ll = model_utils.LVM_pred_ll(full_model, mode[-5], mode[2], models.cov_used, cv_set, f_neuron, v_neuron, \n",
    "                                             beta=beta, beta_z=0.0, max_iters=3000)[0]\n",
    "                if ll < prev_ll:\n",
    "                    prev_ll = ll\n",
    "\n",
    "            LVM_cv_ll.append(fac*prev_ll)\n",
    "        \n",
    "LVM_cv_ll = np.array(LVM_cv_ll).reshape(len(kcvs), len(modes), len(val_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circ_drift_regression(x, z, t, topology, dev='cpu', iters=1000, lr=1e-2, a_fac=1):\n",
    "    t = torch.tensor(t, device=dev)\n",
    "    X = torch.tensor(x, device=dev)\n",
    "    Z = torch.tensor(z, device=dev)\n",
    "        \n",
    "    lowest_loss = np.inf\n",
    "    for sign in [1, -1]: # select sign automatically\n",
    "        shift = Parameter(torch.zeros(1, device=dev))\n",
    "        a = Parameter(torch.zeros(1, device=dev))\n",
    "\n",
    "        optimizer = optim.Adam([a, shift], lr=lr)\n",
    "        losses = []\n",
    "        for k in range(iters):\n",
    "            optimizer.zero_grad()\n",
    "            Z_ = t*a_fac*a + shift + sign*Z\n",
    "            loss = (utils.latent.metric(Z_, X, topology)**2).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.cpu().item())\n",
    "\n",
    "        l_ = loss.cpu().item()\n",
    "        \n",
    "        if l_ < lowest_loss:\n",
    "            lowest_loss = l_\n",
    "            a_ = a.cpu().item()\n",
    "            shift_ = shift.cpu().item()\n",
    "            sign_ = sign\n",
    "            losses_ = losses\n",
    "\n",
    "    return a_fac*a_, sign_, shift_, losses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory regression to align to data and compute drifts\n",
    "topology = 'torus'\n",
    "cvK = 3\n",
    "CV = [0, 1, 2]\n",
    "\n",
    "RMS_cv = []\n",
    "drifts_lv = []\n",
    "for mode in modes:\n",
    "    cvdata = model_utils.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov_lvm)[0]\n",
    "    _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "    cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                                mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "    X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "    cvT = X_loc[0].shape[0]\n",
    "    tar_t = rhd_t[:cvT]\n",
    "    lat = X_loc[0]\n",
    "    \n",
    "    for rn in CV:\n",
    "        fit_range = np.arange(cvT//cvK) + rn*cvT//cvK\n",
    "\n",
    "        drift, sign, shift, losses = circ_drift_regression(tar_t[fit_range], lat[fit_range], fit_range*tbin, \n",
    "                                                      topology, dev=dev, a_fac=1e-5)\n",
    "        \n",
    "        #plt.plot(losses)\n",
    "        #plt.show()\n",
    "        mask = np.ones((cvT,), dtype=bool)\n",
    "        mask[fit_range] = False\n",
    "        \n",
    "        lat_t = torch.tensor((np.arange(cvT)*tbin*drift + shift + sign*lat) % (2*np.pi))\n",
    "        D = (utils.latent.metric(torch.tensor(tar_t)[mask], lat_t[mask], topology)**2)\n",
    "        RMS_cv.append(D.mean().item())\n",
    "        drifts_lv.append(drift)\n",
    "\n",
    "\n",
    "RMS_cv = np.array(RMS_cv).reshape(len(modes), len(CV))\n",
    "drifts_lv = np.array(drifts_lv).reshape(len(modes), len(CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute delays in latent trajectory w.r.t. data, see which one fits best in RMS\n",
    "topology = 'torus'\n",
    "cvK = 3\n",
    "CV = [0, 1, 2]\n",
    "\n",
    "D = 5\n",
    "delays = np.arange(-D, D+1)\n",
    "delay_RMS = []\n",
    "mode = modes[0]\n",
    "\n",
    "for delay in delays:\n",
    "    cvdata = model_utils.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov_lvm)[0]\n",
    "    _, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "    cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                                mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "    X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "    cvT = X_loc[0].shape[0]-len(delays)+1\n",
    "    tar_t = rhd_t[D+delay:cvT+D+delay]\n",
    "    lat = X_loc[0][D:cvT+D]\n",
    "    \n",
    "    for rn in CV:\n",
    "        fit_range = np.arange(cvT//cvK) + rn*cvT//cvK\n",
    "\n",
    "        drift, sign, shift, _ = circ_drift_regression(tar_t[fit_range], lat[fit_range], fit_range*tbin, \n",
    "                                                      topology, dev=dev, a_fac=1e-5)\n",
    "        \n",
    "        mask = np.ones((cvT,), dtype=bool)\n",
    "        mask[fit_range] = False\n",
    "        \n",
    "        lat_ = torch.tensor((np.arange(cvT)*tbin*drift + shift + sign*lat) % (2*np.pi))\n",
    "        Dd = (utils.latent.metric(torch.tensor(tar_t)[mask], lat_[mask], topology)**2)\n",
    "        delay_RMS.append(Dd.mean().item())\n",
    "\n",
    "\n",
    "delay_RMS = np.array(delay_RMS).reshape(len(delays), len(CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latent inferred trajectory\n",
    "mode = modes[0]\n",
    "topology = 'torus'\n",
    "\n",
    "\n",
    "cvdata = model_utils.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov_lvm)[0]\n",
    "_, ftrain, fcov, vtrain, vcov, cvbatch_size = cvdata\n",
    "cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                            mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "\n",
    "tar_t = rhd_t\n",
    "lat = X_loc[0]\n",
    "\n",
    "drift, sign, shift, _ = circ_drift_regression(tar_t[fit_range], lat[fit_range], fit_range*tbin, \n",
    "                                              topology, dev=dev, a_fac=1e-5)\n",
    "\n",
    "lat_t = ((np.arange(rhd_t.shape[0])*tbin*drift + shift + sign*lat) % (2*np.pi))\n",
    "lat_t_std = X_std[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    lat_t, lat_t_std, delay_RMS, RMS_cv, LVM_cv_ll, drifts_lv, rcov_lvm\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./saves/P_HDC_lat.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
