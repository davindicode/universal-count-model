{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPLVM\n",
    "Here we run GPLVM on a biological dataset to perform dimensionality reduction. Aims to reproduce the tutorial https://pyro.ai/examples/gplvm.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from neuroprob.utils import tools\n",
    "import neuroprob.models as mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# license: Copyright (c) 2014, the Open Data Science Initiative\n",
    "# license: https://www.elsevier.com/legal/elsevier-website-terms-and-conditions\n",
    "URL = \"https://raw.githubusercontent.com/sods/ods/master/datasets/guo_qpcr.csv\"\n",
    "\n",
    "df = pd.read_csv(URL, index_col=0)\n",
    "print(\"Data shape: {}\\n{}\\n\".format(df.shape, \"-\" * 21))\n",
    "print(\"Data labels: {}\\n{}\\n\".format(df.index.unique().tolist(), \"-\" * 86))\n",
    "print(\"Show a small subset of the data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(df.values)\n",
    "# we need to transpose data to correct its shape\n",
    "y = data.t()\n",
    "\n",
    "capture_time = y.new_tensor([int(cell_name.split(\" \")[0]) for cell_name in df.index.values])\n",
    "# we scale the time into the interval [0, 1]\n",
    "time = capture_time.log2() / 6\n",
    "\n",
    "y_dim = y.shape[0]\n",
    "num_data = y.shape[1]\n",
    "\n",
    "sample_bin = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPLVM\n",
    "l = np.ones(1) # lengthscales\n",
    "v = np.ones(1) # variances\n",
    "kernels_tuples = [v, \n",
    "                  ('RBF', 'euclid', np.array([l, l]))]\n",
    "prior_tuples = [('Normal', # type\n",
    "                 'euclid', # topology\n",
    "                 0.0, # mean\n",
    "                 0.1, # std\n",
    "                 False, # learnable mean\n",
    "                 False), # learnable std\n",
    "               ('Normal', 'euclid', 0.0, 0.1, False, False)]\n",
    "variational_types = [('Normal', 'euclid'), \n",
    "                     ('Normal', 'euclid')]\n",
    "\n",
    "# initial variational parameters\n",
    "ini_X = [np.empty((num_data, 2)), np.empty((num_data, 2))]\n",
    "ini_X[0][:, 0] = np.array(time) # mean\n",
    "ini_X[0][:, 1] = 1.0*np.ones(num_data) # std (before softplus)\n",
    "ini_X[1][:, 0].fill(0.0) # mean\n",
    "ini_X[1][:, 1] = 1.0*np.ones(num_data) # std (before softplus)\n",
    "\n",
    "# inducing_points\n",
    "num_induc = 32\n",
    "ind = np.random.randint(num_data, size=32)\n",
    "inducing_points = ini_X[0][None, ind, :]\n",
    "\n",
    "gp_lvm = mdl.nonparametrics.Gaussian_process(y_dim, inducing_points, kernels_tuples, \n",
    "                                          prior_tuples, variational_types, inv_link='identity', \n",
    "                                          shared_kernel_params=True, full_cov_fit=False)\n",
    "gp_lvm.set_params(sample_bin, jitter=1e-4)\n",
    "\n",
    "log_var = -1.0*np.ones(y_dim)\n",
    "noise_dist = mdl.likelihoods.Gaussian(y_dim, 'identity', log_var)\n",
    "noise_dist.set_params(sample_bin)\n",
    "\n",
    "# NLL model\n",
    "glm = mdl.inference.nll_optimized([gp_lvm], noise_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'cuda:0'\n",
    "\n",
    "# preprocessing\n",
    "glm.preprocess(ini_X, num_data, np.array(y), batch_size=num_data)\n",
    "glm.to(dev) # to device\n",
    "\n",
    "sch = lambda o: optim.lr_scheduler.MultiplicativeLR(o, lambda e: 0.9)\n",
    "opt_tuple = (optim.Adam, 100, sch)\n",
    "opt_lr_dict = {'default': 1e-2}\n",
    "glm.set_optimizers(opt_tuple, opt_lr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting\n",
    "losses = glm.fit(5000, margin=1e0, premature=100, cov_samples=1, ll_samples=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loc, X_std = gp_lvm.eval_X()\n",
    "\n",
    "colors = plt.get_cmap(\"tab10\").colors[::-1]\n",
    "labels = df.index.unique()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    X_i = X_loc[0][df.index == label]\n",
    "    Y_i = X_loc[1][df.index == label]\n",
    "    plt.scatter(X_i[:], Y_i[:], c=[colors[i]], label=label)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head direction Poisson bumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic grid of place cells\n",
    "sample_bin, track_samples, x_t, y_t, s_t, hd_t, theta_t, dir_t, \\\n",
    "            syn_t_spike, spike_samples, units, \\\n",
    "            left_x, right_x, bottom_y, top_y = pickle.load(open('../data/synthetic/grid_IPP.p', 'rb'))\n",
    "\n",
    "arena_width = right_x - left_x\n",
    "arena_height = top_y - bottom_y\n",
    "\n",
    "neurons = np.arange(units)\n",
    "use_samples = 3000000\n",
    "used_t_spike = [syn_t_spike[n][syn_t_spike[n] < use_samples] for n in neurons]\n",
    "units_used = len(neurons)\n",
    "unit_used = np.arange(units_used)\n",
    "show_neurons = [0, 1, 2, 3, 4]\n",
    "behav_data = (x_t, y_t)\n",
    "\n",
    "bin_size = 100\n",
    "tbin, resamples, rc_t, (rx_t, ry_t) = neural_utils.BinTrain(bin_size, sample_bin, used_t_spike, \n",
    "                                                                    use_samples, behav_data, average_behav=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "cv_set = neural_utils.SpikeTrainCV(folds, rc_t, resamples, [rx_t, ry_t])\n",
    "fit_rc_t, fit_behav, dec_rc_t, dec_behav = cv_set[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30000\n",
    "plt.plot(fit_behav[0][:T], fit_behav[1][:T])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPLVM\n",
    "units_ = units_used#1\n",
    "l = 100.*np.ones(units_)\n",
    "v = np.ones(units_)\n",
    "\n",
    "kernels_tuples = [v, \n",
    "                  ('RBF', 'euclid', np.array([l, l]))]\n",
    "prior_tuple = [('Uniform', 'euclid', 0.0, right_x, False, False), \n",
    "                ('Uniform', 'euclid', 0.0, top_y, False, False)]\n",
    "#prior_tuple = [('RW', 'euclid', 0.0, 1.0, True, True), \n",
    "#               ('RW', 'euclid', 0.0, 1.0, True, True)]\n",
    "variational_types = [('Normal', 'euclid'), \n",
    "                     ('Normal', 'euclid')]\n",
    "\n",
    "num_induc = 16\n",
    "inducing_points = np.array([left_x + arena_width*np.random.rand(num_induc), \\\n",
    "                            bottom_y + arena_height*np.random.rand(num_induc)]).T[None, ...].repeat(units_used, axis=0)\n",
    "\n",
    "gp_lvm = mdl.nonparametrics.Gaussian_process(units_used, inducing_points, kernels_tuples, \n",
    "                                          prior_tuple, variational_types, mean=np.zeros(units_used), \n",
    "                                          shared_kernel_params=False, cov_type='factorized', whiten=False)\n",
    "gp_lvm.set_params(tbin, jitter=1e-5)\n",
    "\n",
    "\n",
    "likelihood = mdl.likelihoods.Poisson(units_used, 'exp')\n",
    "likelihood.set_params(tbin)\n",
    "\n",
    "# NLL model\n",
    "glm = mdl.inference.nll_optimized([gp_lvm], likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "ini_X = fit_behav\n",
    "fit_samples = fit_behav[0].shape[0]\n",
    "cov_samples = 1\n",
    "\n",
    "glm.preprocess(ini_X, fit_samples, fit_rc_t, batch_size=10000)\n",
    "glm.to(dev)\n",
    "\n",
    "sch = lambda o: optim.lr_scheduler.MultiplicativeLR(o, lambda e: 0.9)\n",
    "opt_tuple = (optim.Adam, 200, sch)\n",
    "opt_lr_dict = {'default': 5*1e-2}\n",
    "glm.set_optimizers(opt_tuple, opt_lr_dict)\n",
    "\n",
    "# fitting\n",
    "annealing = lambda x: 1.0#min(1.0, 0.005*x)\n",
    "losses = glm.fit(3000, margin=1e1, premature=100, anneal_func=annealing, \n",
    "                 cov_samples=cov_samples, ll_samples=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show fits\n",
    "grid_size = (50, 40)\n",
    "#grid_shape = [[left_x, right_x], [bottom_y, top_y]]\n",
    "grid_shape = [[left_x, right_x], \\\n",
    "              [bottom_y, top_y]]\n",
    "show_neurons = np.arange(5)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 2))\n",
    "nrows = 1\n",
    "ncols = 5\n",
    "axes = [ fig.add_subplot(nrows, ncols, r * ncols + c + 1) for r in range(0, nrows) for c in range(0, ncols) ]\n",
    "\n",
    "for neuron in show_neurons:\n",
    "    \n",
    "    def func(pos):\n",
    "        prevshape = pos.shape[1:]\n",
    "        x = pos[0].flatten()\n",
    "        y = pos[1].flatten()\n",
    "        covariates = np.array([x, y])\n",
    "        return gp_lvm.eval_rate(covariates, [neuron])[0].reshape(*prevshape)\n",
    "\n",
    "    if neuron == 4:\n",
    "        #cbar= True\n",
    "        ticktitle='Firing rate (Hz)'\n",
    "    else:\n",
    "        #cbar = False\n",
    "        ticktitle=''\n",
    "        \n",
    "    _, field = tools.compute_mesh(grid_size, grid_shape, func)\n",
    "    grid_shape = [[left_x, right_x], [bottom_y, top_y]]\n",
    "    _, ax = tools.visualize_field(field, grid_shape, ticktitle=ticktitle, figax=(fig, axes[neuron]))\n",
    "    if neuron == 0:\n",
    "        ylabel=r'$y$'\n",
    "    else:\n",
    "        ylabel=None\n",
    "    tools.decorate_ax(ax, ylabel=ylabel, xlim=[left_x, right_x], ylim=[bottom_y, top_y],\n",
    "                      spines=[False, False, False, False])\n",
    "\n",
    "\n",
    "fig.text(0.51, 0.0, r'$x$', ha='center', va='center', fontsize=12)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "#for n in range(2*ncols, 4*ncols):\n",
    "#    box = axes[n].get_position(transform=axes[n].transAxes)\n",
    "#    box.x0 = box.x0 - 0.1\n",
    "#    box.x1 = box.x1\n",
    "#    axes[n].set_position(box, transform=axes[n].transAxes)\n",
    "\n",
    "#plt.savefig('output/syn_pop.svg', bbox_inches='tight')\n",
    "#plt.savefig('output/syn_pop.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "dec_samples = dec_behav[0].shape[0]\n",
    "\n",
    "ini_X = [(np.random.randn(dec_samples, 2)), \n",
    "         (np.random.randn(dec_samples, 2))]\n",
    "ini_X[0][:, 1].fill(1.0) # std of variational distribution\n",
    "ini_X[1][:, 1].fill(1.0) # std of variational distribution\n",
    "cov_samples = 16\n",
    "\n",
    "glm.preprocess(ini_X, dec_samples, dec_rc_t, batch_size=10000)\n",
    "glm.to(dev)\n",
    "\n",
    "sch = lambda o: optim.lr_scheduler.MultiplicativeLR(o, lambda e: 0.9)\n",
    "opt_tuple = (optim.Adam, 200, sch)\n",
    "opt_lr_dict = {'default': 0., 'lv_mu_0': 1e-1, 'lv_mu_1': 1e-1, \n",
    "               'lv_std_0': 5*1e-3, 'lv_std_1': 5*1e-3}\n",
    "glm.set_optimizers(opt_tuple, opt_lr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting\n",
    "annealing = lambda x: 1.0#min(1.0, 0.005*x)\n",
    "losses = glm.fit(3000, margin=1e1, premature=100, anneal_func=annealing, \n",
    "                 cov_samples=cov_samples, ll_samples=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loc, X_std = gp_lvm.eval_X()\n",
    "\n",
    "plt.plot(X_loc[0], X_loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "start = 0\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# model trajectory\n",
    "ells = [Ellipse(xy=[X_loc[0][t], X_loc[1][t]], \n",
    "                width=2*X_std[0][t], height=2*X_std[1][t], angle=0.)#rnd.rand()*360)\n",
    "        for t in np.arange(start, T, 1)]\n",
    "\n",
    "fig = plt.figure(0)\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "for e in ells:\n",
    "    ax.add_artist(e)\n",
    "    e.set_clip_box(ax.bbox)\n",
    "    e.set_alpha(0.3)\n",
    "    e.set_facecolor('tab:blue')\n",
    "\n",
    "#ax.plot(X_loc[0][start:T], X_loc[1][start:T], 'r-')# marker='.', s=0)\n",
    "ax.plot(rx_t[start:T], ry_t[start:T], 'r-') # true trajectory\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "tt = tbin*np.arange(start, T)\n",
    "plt.plot(tt, X_loc[0][start:T])\n",
    "plt.errorbar(tt, X_loc[0][start:T], yerr=X_std[0][start:T], linestyle='', \n",
    "            elinewidth=1, color='tab:blue', alpha=0.5)\n",
    "plt.plot(tt, (rx_t[start:T] - RX_mu)/RX_std*iX_std + iX_mu, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
