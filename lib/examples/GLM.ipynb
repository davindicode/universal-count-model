{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel regression and splines\n",
    "https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf. The choice of kernel implicitly selects the basis functions selected for kernel expansion. With a polynomial basis, we can fit Lagrange polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2)\n",
    "y = np.random.randn(1, 2) + 4.\n",
    "def phi(x):\n",
    "    x = x[None, :]\n",
    "    return np.concatenate((np.ones_like(x), x, x**2, x**3, x**4, x**5, x**6, x**7, x**8), axis=0)\n",
    "\n",
    "w, f = utils.signal.basis_function_regression(x, y, phi, lambd=0.)\n",
    "\n",
    "x_eval = np.linspace(x.min(), x.max(), 100)\n",
    "ph = phi(x_eval)[None, ...] # o, d, n\n",
    "f = (w[..., None]*ph).sum(1) # o, n\n",
    "\n",
    "plt.plot(x_eval, f[0, :])\n",
    "plt.scatter(x, y[0, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 6, 7)\n",
    "y = x.sin()\n",
    "xs = torch.linspace(0, 6, 101)\n",
    "\n",
    "spline = utils.signal.spline_interpolate(3)\n",
    "\n",
    "ys = spline.interp(x, y, xs)\n",
    "Ys = spline.integ(x, y, xs)\n",
    "\n",
    "plt.scatter(x, y, label='Samples', color='purple')\n",
    "plt.plot(xs, ys, label='Interpolated curve')\n",
    "plt.plot(xs, xs.sin(), '--', label='True Curve')\n",
    "plt.plot(xs, Ys, label='Spline Integral')\n",
    "plt.plot(xs, 1-xs.cos(), '--', label='True Integral')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linn = utils.signal.linear_interpolate(x.numpy(), y.numpy(), xs.numpy())\n",
    "plt.plot(xs.numpy(), linn)\n",
    "plt.scatter(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic data\n",
    "sample_bin, track_samples, x_t, y_t, s_t, hd_t, theta_t, dir_t, \\\n",
    "    syn_t_spike, spike_samples, neurons, \\\n",
    "    left_x, right_x, bottom_y, top_y = pickle.load(open('./data/gauss_spat_IPP.p', 'rb'))\n",
    "\n",
    "fac = 1.\n",
    "\n",
    "max_speed = s_t.max()/fac\n",
    "wrap_theta_t = utils.numpy.WrapPi(theta_t, True)\n",
    "\n",
    "right_x = right_x/fac\n",
    "left_x = left_x/fac\n",
    "top_y = top_y/fac\n",
    "bottom_y = bottom_y/fac\n",
    "\n",
    "arena_width = right_x - left_x\n",
    "arena_height = top_y - bottom_y\n",
    "behav_data = [x_t/fac, y_t/fac]\n",
    "\n",
    "\n",
    "bin_size = 1\n",
    "tbin, resamples, rc_t, rbehav_t = utils.neural.BinTrain(bin_size, sample_bin, syn_t_spike, \n",
    "                                                 track_samples, behaviour_data=behav_data, binned=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_(neurons):\n",
    "    \"\"\"\n",
    "    GP with variable regressors model fit and nonconvexity.\n",
    "    \n",
    "    Here active dimensions are handled by the GP kernels, None entries indicate non-active dimensions.\n",
    "    \"\"\"\n",
    "    num_induc = 16\n",
    "    l = 100.*np.ones(neurons)\n",
    "    v = np.ones(neurons)\n",
    "\n",
    "    ind_list = [np.linspace(left_x, right_x, num_induc), \\\n",
    "                (bottom_y + arena_height*np.random.rand(num_induc))]\n",
    "\n",
    "    kt = [('variance', v), (None,), ('RBF', 'euclid', np.array([l]))]\n",
    "    inducing_points = np.array(ind_list).T[None, ...].repeat(neurons, axis=0)\n",
    "    gp_rate = mdl.nonparametrics.Gaussian_process(\n",
    "        len(ind_list), neurons, kt, inducing_points=inducing_points, jitter=1e-5, \n",
    "        inv_link='exp', mean=np.zeros((neurons)), learn_mean=True\n",
    "    )\n",
    "    \n",
    "    kt = [('variance', v), ('RBF', 'euclid', np.array([l])), (None,)]\n",
    "    inducing_points = np.array(ind_list).T[None, ...].repeat(neurons, axis=0)\n",
    "    gp_rate_ = mdl.nonparametrics.Gaussian_process(\n",
    "        len(ind_list), neurons, kt, inducing_points=inducing_points, jitter=1e-5, \n",
    "        inv_link='exp', mean=np.zeros((neurons)), learn_mean=True\n",
    "    )\n",
    "    \n",
    "    rate_model = mdl.parametrics.product_model(2, [gp_rate, gp_rate_], inv_link='relu')\n",
    "    return rate_model\n",
    "\n",
    "\n",
    "\n",
    "def GP(neurons):\n",
    "    \"\"\"\n",
    "    GP with variable regressors model fit and nonconvexity.\n",
    "    \n",
    "    Here active dimensions are handled by the GP kernels, None entries indicate non-active dimensions.\n",
    "    \"\"\"\n",
    "    num_induc = 16\n",
    "    l = 100.*np.ones(neurons)\n",
    "    v = np.ones(neurons)\n",
    "\n",
    "    ind_list = [np.linspace(left_x, right_x, num_induc)]\n",
    "    kt = [('variance', v), ('RBF', 'euclid', np.array([l]))]\n",
    "    inducing_points = np.array(ind_list).T[None, ...].repeat(neurons, axis=0)\n",
    "    gp_rate = mdl.nonparametrics.Gaussian_process(\n",
    "        len(ind_list), neurons, kt, inducing_points=inducing_points, jitter=1e-5, \n",
    "        inv_link='exp', mean=np.zeros((neurons)), learn_mean=True, active_dims=[0]\n",
    "    )\n",
    "    \n",
    "    ind_list = [(bottom_y + arena_height*np.random.rand(num_induc))]\n",
    "    kt = [('variance', v), ('RBF', 'euclid', np.array([l]))]\n",
    "    inducing_points = np.array(ind_list).T[None, ...].repeat(neurons, axis=0)\n",
    "    gp_rate_ = mdl.nonparametrics.Gaussian_process(\n",
    "        len(ind_list), neurons, kt, inducing_points=inducing_points, jitter=1e-5, \n",
    "        inv_link='exp', mean=np.zeros((neurons)), learn_mean=True, active_dims=[1]\n",
    "    )\n",
    "    \n",
    "    rate_model = mdl.parametrics.product_model(2, [gp_rate, gp_rate_], inv_link='relu')\n",
    "    return rate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972490f97a294bbeb12f07dc3ccd0a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ba9971988365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mannealing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;31m#min(1.0, 0.002*x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannealing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Learning/neuroprob/neuroprob/inference.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, stop_iters, loss_margin, neuron, anneal_func, retain_graph, cov_samples, ll_samples, bound, ll_mode, enum_z, lv_input, callback)\u001b[0m\n\u001b[1;32m    559\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Learning/neuroprob/neuroprob/inference.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    546\u001b[0m                         \u001b[0;31m#with profiler.profile(record_shapes=True, use_cuda=True) as profi:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                         \u001b[0;31m#    with profiler.record_function(\"backwards\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnat_grad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# compute natural gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = mdl.inference.input_group(2, [(None, None, None, 1)]*2)\n",
    "\n",
    "gp_1 = GP_(neurons)\n",
    "\n",
    "# Poisson process output\n",
    "likelihood = mdl.likelihoods.Poisson(tbin, neurons, 'relu')\n",
    "\n",
    "# NLL model\n",
    "hist_len = 1\n",
    "samples = resamples\n",
    "glm = mdl.inference.VI_optimized(inputs, gp_1, likelihood)\n",
    "glm.set_data([rbehav_t[0][:samples], rbehav_t[1][:samples]], samples, rc_t[:, :samples], batch_size=100000)\n",
    "glm.to(dev)\n",
    "\n",
    "# fit\n",
    "sch = lambda o: optim.lr_scheduler.MultiplicativeLR(o, lambda e: 0.9)\n",
    "opt_tuple = (optim.Adam, 100, sch)\n",
    "opt_lr_dict = {'default': 1e-4}\n",
    "glm.set_optimizers(opt_tuple, opt_lr_dict)\n",
    "\n",
    "annealing = lambda x: 1.0#min(1.0, 0.002*x)\n",
    "losses = glm.fit(1000, loss_margin=1e0, stop_iters=100, anneal_func=annealing, cov_samples=1, ll_samples=10)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show fields\n",
    "show_neurons = [4]\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5,4))\n",
    "\n",
    "grid_size = [50, 40]\n",
    "grid_shape = [[left_x, right_x], [bottom_y, top_y]]\n",
    "\n",
    "ticktitle='firing rate (Hz)'\n",
    "\n",
    "def func(pos):\n",
    "    prevshape = pos.shape[1:]\n",
    "    x = pos[0].flatten()\n",
    "    y = pos[1].flatten()\n",
    "    #theta = np.zeros(len(x))\n",
    "    covariates = np.array([x, y])#, theta])\n",
    "    return gp_1.eval_rate(covariates, 0).reshape(*prevshape)\n",
    "\n",
    "_, field = tools.compute_mesh(grid_size, grid_shape, func)\n",
    "\n",
    "_, ax = tools.visualize_field(field, grid_shape, ticktitle=ticktitle, spike_pos=None, figax=(fig, axes))\n",
    "x = np.linspace(left_x, right_x, 10)\n",
    "#tools.decorate_ax(ax, xlim=[left_x, right_x], ylim=[bottom_y, top_y])}\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_x = 0.0\n",
    "right_x = 538.4177999999999\n",
    "bottom_y = 0.0\n",
    "top_y = 432.3225\n",
    "arena_width = right_x - left_x\n",
    "arena_height = top_y - bottom_y\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(track_samples=3000000):\n",
    "    neurons = 1\n",
    "    sample_bin = 1./1250\n",
    "    \n",
    "    # Mixture of Gaussians\n",
    "    t_p = np.array([[0.0, 0.0]])\n",
    "    hd_p = np.array([[0.0, 0.0]])\n",
    "    dir_p = np.array([[0.0, 0.0]])\n",
    "    sp = np.array([0.0])\n",
    "        \n",
    "    mu = np.array([[100., 100.]])\n",
    "    prec = np.array([[0.0002, 0.0002, 0.0]])\n",
    "    rate_0 = np.array([10.0]) # Hz\n",
    "    \n",
    "    gauss_1 = mdl.parametrics.PTP_field(neurons, 'exp')\n",
    "    gauss_1.set_params(sample_bin, mu, prec, rate_0, t_p, hd_p, dir_p, sp)\n",
    "\n",
    "    mu = np.array([[170., 350.]])\n",
    "    prec = np.array([[0.0002, 0.0002, 0.0]])\n",
    "    rate_0 = np.array([10.0]) # Hz\n",
    "\n",
    "    gauss_2 = mdl.parametrics.PTP_field(neurons, 'exp')\n",
    "    gauss_2.set_params(sample_bin, mu, prec, rate_0, t_p, hd_p, dir_p, sp)\n",
    "\n",
    "    mu = np.array([[400., 225.]])\n",
    "    prec = np.array([[0.0003, 0.0001, 0.0]])\n",
    "    rate_0 = np.array([10.0]) # Hz\n",
    "    \n",
    "    gauss_3 = mdl.parametrics.PTP_field(neurons, 'exp')\n",
    "    gauss_3.set_params(sample_bin, mu, prec, rate_0, t_p, hd_p, dir_p, sp)\n",
    "\n",
    "    gauss_rate = mdl.parametrics.mixture_model([gauss_1, gauss_2, gauss_3])\n",
    "    gauss_rate.set_params(sample_bin)\n",
    "    \n",
    "    # sample behaviour\n",
    "    print('Generating animal behaviour...')\n",
    "    arena = ['box', (left_x, right_x, bottom_y, top_y)]\n",
    "    an = mdl.animal.animal_SL(sample_bin, track_samples, arena)\n",
    "\n",
    "    sample_bin, sim_samples, x_t, y_t, \\\n",
    "        s_t, dir_t, hd_t, theta_t = \\\n",
    "        an.sample(0.01, [200.0, 200.0], 200, 0.14, 0.0)\n",
    "\n",
    "    wrap_theta_t = tools.WrapPi(theta_t, True)\n",
    "    maxspeed = s_t.max()\n",
    "    \n",
    "    behav_data = [x_t, y_t, s_t, wrap_theta_t, hd_t, dir_t]\n",
    "    \n",
    "    # add true field\n",
    "    unit = 0\n",
    "    grid_size = [int(arena_width/10), int(arena_height/10)]\n",
    "    grid_shape = [[left_x, right_x], [bottom_y, top_y]]\n",
    "\n",
    "    def func(pos):\n",
    "        prevshape = pos.shape[1:]\n",
    "        x = pos[0].flatten()\n",
    "        y = pos[1].flatten()\n",
    "        s = np.zeros_like(x)\n",
    "        th = np.zeros_like(x)\n",
    "        hd = np.zeros_like(x)\n",
    "        dr = np.zeros_like(x)\n",
    "        covariates = [x, y, s, th, hd, dr]\n",
    "        return gauss_rate.eval_rate(covariates, unit).reshape(*prevshape)\n",
    "\n",
    "    print('Computing true field...')\n",
    "    true_field = tools.compute_mesh(grid_size, grid_shape, func)[1]\n",
    "    #_, ax = tools.visualize_field(true_field, grid_shape, ticktitle='')\n",
    "    #plt.show()\n",
    "    \n",
    "    # sample spikes\n",
    "    #shape = np.ones(neurons)\n",
    "    #renewal_dist = mdl.likelihoods.Gamma(neurons, 'exp', shape)\n",
    "    renewal_dist = mdl.likelihoods.Poisson(neurons, 'exp')\n",
    "    renewal_dist.set_params(sample_bin)\n",
    "    \n",
    "    glm = mdl.inference.nll_optimized([gauss_rate], renewal_dist)\n",
    "    glm.to(dev)\n",
    "    \n",
    "    print('Sampling spike trains...')\n",
    "    ini_train = np.zeros((1, 1, 1)) # for trials\n",
    "    rc_t, _ = glm.sample(behav_data, ini_train)\n",
    "    \n",
    "    return true_field, behav_data, rc_t[0], sample_bin, maxspeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup GP parameters\n",
    "def GP_params(behav_tuple, num_induc, maxspeed):\n",
    "    units_ = 1\n",
    "    l = np.ones(units_)\n",
    "    v = np.ones(units_)\n",
    "\n",
    "    ind_list = [np.linspace(left_x, right_x, num_induc), \\\n",
    "                bottom_y + arena_height*np.random.rand(num_induc)]\n",
    "\n",
    "    kt = [('RBF', 'euclid', np.array([l, l]), v)]\n",
    "    covariates = (behav_tuple[0], behav_tuple[1])\n",
    "        \n",
    "    return covariates, kt, ind_list, units_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic multimodal data\n",
    "true_field, behav_data, rc_t, sample_bin, maxspeed = generate_data()\n",
    "units = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = 1000.\n",
    "behav_list = [b / fac for b in behav_data[:2]]\n",
    "\n",
    "t_spike = []\n",
    "for binned in rc_t:\n",
    "    t_spike.append(neural_utils.BinToTrain(binned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field(glm_rate, grid_size, grid_shape, neuron):\n",
    "\n",
    "    def func(pos):\n",
    "        prevshape = pos.shape[1:]\n",
    "        x = pos[0].flatten()\n",
    "        y = pos[1].flatten()\n",
    "        covariates = [x, y]\n",
    "        return glm_rate.eval_rate(covariates, neuron).reshape(-1, *prevshape)\n",
    "\n",
    "    _, field = tools.compute_mesh(grid_size, grid_shape, func)\n",
    "    return field\n",
    "\n",
    "\n",
    "def compute_rate(glm_rate, neuron):\n",
    "    grid_size = [int(arena_width/10), int(arena_height/10)]\n",
    "    grid_shape = [[left_x/fac, right_x/fac], [bottom_y/fac, top_y/fac]]\n",
    "    field = get_field(glm_rate, grid_size, grid_shape, neuron)\n",
    "    return field\n",
    "\n",
    "\n",
    "def compute_RMS(true_w, model_w): # rate error, with shape (neuron,)\n",
    "    return np.sqrt(((model_w-true_w)**2).mean())\n",
    "    \n",
    "    \n",
    "def compute_loss(model, w_1, w_2, range_w_1, range_w_2):\n",
    "    r\"\"\"\n",
    "    :param tuple w_i: tuple containing (parameter, index)\n",
    "    \"\"\"\n",
    "    lw_1 = w_1[0].data[w_1[1]].item()\n",
    "    lw_2 = w_2[0].data[w_2[1]].item()\n",
    "\n",
    "    def func(pos):\n",
    "        prevshape = pos[0, ...].shape\n",
    "        p_1 = pos[0, ...].flatten()\n",
    "        p_2 = pos[1, ...].flatten()\n",
    "        NLL = np.empty(p_1.shape)\n",
    "        with torch.no_grad():\n",
    "            for k in range(len(p_1)): \n",
    "                w_1[0].data[w_1[1]] = torch.tensor(p_1[k], device=w_1[0].device)\n",
    "                w_2[0].data[w_2[1]] = torch.tensor(p_2[k], device=w_1[0].device)\n",
    "                NLL[k] = model.nll(0, ll_samples=10).item()\n",
    "        return NLL.reshape(*prevshape)\n",
    "    \n",
    "    grid_size = [len(range_w_1), len(range_w_2)]\n",
    "    grid_shape = [[range_w_1.min(), range_w_1.max()], [range_w_2.min(), range_w_2.max()]]\n",
    "    _, landscape = tools.compute_mesh(grid_size, grid_shape, func)\n",
    "    \n",
    "    w_1[0].data[w_1[1]] = lw_1\n",
    "    w_2[0].data[w_2[1]] = lw_2\n",
    "    return landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture():# Mixture of Gaussians\n",
    "    mu = np.array([[100., 100.]])/fac\n",
    "    prec = np.array([[0.0002, 0.0002, 0.0]])*fac**2\n",
    "    rate_0 = np.array([10.0]) # Hz\n",
    "    t_p = np.array([[1.0, 0.2]]) # beta, phi_0 for theta modulation\n",
    "    neurons = rate_0.shape[0]\n",
    "\n",
    "    gauss_1 = mdl.parametrics.Gauss_GLM(neurons, 'exp')\n",
    "    gauss_1.set_params(sample_bin, mdl.parametrics.gaussian_to_w(mu, prec, rate_0, t_p)[:, :6])\n",
    "\n",
    "    mu = np.array([[170., 350.]])/fac\n",
    "    prec = np.array([[0.0002, 0.0002, 0.0]])*fac**2\n",
    "    rate_0 = np.array([10.0]) # Hz\n",
    "    t_p = np.array([[1.0, 0.2]]) # beta, phi_0 for theta modulation\n",
    "    neurons = rate_0.shape[0]\n",
    "\n",
    "    gauss_2 = mdl.parametrics.Gauss_GLM(neurons, 'exp')\n",
    "    gauss_2.set_params(sample_bin, mdl.parametrics.gaussian_to_w(mu, prec, rate_0, t_p)[:, :6])\n",
    "\n",
    "    mu = np.array([[400., 225.]])/fac\n",
    "    prec = np.array([[0.0003, 0.0001, 0.0]])*fac**2\n",
    "    rate_0 = np.array([10.0]) # Hz\n",
    "    t_p = np.array([[1.0, 0.2]]) # beta, phi_0 for theta modulation\n",
    "    neurons = rate_0.shape[0]\n",
    "\n",
    "    gauss_3 = mdl.parametrics.Gauss_GLM(neurons, 'exp')\n",
    "    gauss_3.set_params(sample_bin, mdl.parametrics.gaussian_to_w(mu, prec, rate_0, t_p)[:, :6])\n",
    "\n",
    "    glm_rate = mdl.parametrics.mixture_model([gauss_1, gauss_2, gauss_3])\n",
    "    glm_rate.set_params(sample_bin)\n",
    "    \n",
    "    return glm_rate\n",
    "\n",
    "\n",
    "def GP():# GP with variable regressors model fit and nonconvexity\n",
    "    num_induc = 16\n",
    "    l = 10.*np.ones(units)/fac\n",
    "    v = np.ones(units)\n",
    "\n",
    "    ind_list = [np.linspace(left_x, right_x, num_induc)/fac, \\\n",
    "                (bottom_y + arena_height*np.random.rand(num_induc))/fac]\n",
    "\n",
    "    kt = [('RBF', 'euclid', np.array([l, l]), v)]\n",
    "\n",
    "    inducing_points = np.array(ind_list).T[None, ...].repeat(1, axis=0)\n",
    "    glm_rate = mdl.nonparametrics.GP_covariates(units, inducing_points, kt,\n",
    "                                                ([None],)*2, ([None],)*2, \n",
    "                                                inv_link='exp', shared_kernel_params=True,\n",
    "                                                full_cov_fit=False, \n",
    "                                                mean_ini=np.zeros((1, units, 1)), jitter=1e-5)\n",
    "    glm_rate.set_params(sample_bin)\n",
    "    return glm_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential data increase fitting\n",
    "track_samples = rc_t.shape[1]\n",
    "use_samples = [100000, 300000, 500000, 1000000, 1500000, 2000000, track_samples]\n",
    "\n",
    "RMS = []\n",
    "field_tuples = []\n",
    "\n",
    "for model_type in ['mix', 'GP']:\n",
    "    if model_type == 'mix':\n",
    "        glm_rate = mixture()\n",
    "        lr = 5*1e-3\n",
    "    elif model_type == 'GP':\n",
    "        glm_rate = GP()\n",
    "        lr = 1e-3\n",
    "\n",
    "    for samples in use_samples:\n",
    "        rc_t_ = rc_t[:, :samples]\n",
    "        #if model_type == 'GP' and samples < 1000000:\n",
    "        #    lr = 1e-2\n",
    "    \n",
    "        # Poisson process output\n",
    "        likelihood = mdl.likelihoods.Poisson(units, 'exp')\n",
    "        likelihood.set_params(sample_bin)\n",
    "    \n",
    "        glm = mdl.inference.nll_optimized([glm_rate], likelihood)\n",
    "        glm.preprocess([c[:samples] for c in behav_list], samples, rc_t_, batch_size=100000)\n",
    "        glm.to(dev)\n",
    "\n",
    "        # fit\n",
    "        sch = lambda o: optim.lr_scheduler.MultiplicativeLR(o, lambda e: 0.9)\n",
    "        opt_tuple = (optim.Adam, 100, sch)\n",
    "        opt_lr_dict = {'default': lr}\n",
    "        glm.set_optimizers(opt_tuple, opt_lr_dict)\n",
    "\n",
    "        annealing = lambda x: 1.0#min(1.0, 0.002*x)\n",
    "        losses = glm.fit(3000, margin=1e0, premature=100, anneal_func=annealing, cov_samples=1, ll_samples=10, pred_ll=False)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('NLL')\n",
    "        plt.show()\n",
    "        \n",
    "        field = compute_rate(glm_rate, [0])[0] # field computation\n",
    "        # rate error\n",
    "        RMS.append(compute_RMS(true_field, field))\n",
    "        \n",
    "        if model_type == 'GP':\n",
    "            field_tuples.append(field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss landscape for GP\n",
    "w_1 = (glm.rate_model[0].Xu, (0, 0, 0)) # u_x\n",
    "w_2 = (glm.rate_model[0].Xu, (0, 0, 1)) # u_y\n",
    "#w_1 = (glm.rate_model[0].models[0].w, (0, 1))\n",
    "#w_2 = (glm.rate_model[0].models[0].w, (0, 2))\n",
    "\n",
    "lw_1 = w_1[0].data[w_1[1]].item()\n",
    "lw_2 = w_2[0].data[w_2[1]].item()\n",
    "range_w_1 = np.linspace(lw_1-0.15, lw_1+0.02, 20)\n",
    "range_w_2 = np.linspace(lw_2-0.01, lw_2+0.06, 20) \n",
    "\n",
    "resamples = rc_t.shape[1]\n",
    "glm.preprocess(behav_list, resamples, rc_t, batch_size=resamples) # evaluate NLL for all data\n",
    "landscape = compute_loss(glm, w_1, w_2, range_w_1, range_w_2)\n",
    "\n",
    "lw_x = lw_1\n",
    "lw_y = lw_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,3))\n",
    "fig.text(-0.03, 1.1, 'A', transform=fig.transFigure, size=15)\n",
    "fig.text(-0.03, 0.53, 'B', transform=fig.transFigure, size=15)\n",
    "fig.text(0.73, 0.51, 'C', transform=fig.transFigure, size=15)\n",
    "runs = len(use_samples)\n",
    "\n",
    "\n",
    "### fitting sequence\n",
    "grid_size = [50, 40]\n",
    "grid_shape = [[left_x, right_x], [bottom_y, top_y]]\n",
    "\n",
    "# GT\n",
    "widths = [1]\n",
    "heights = [1]\n",
    "nrows = 1\n",
    "ncols = 1\n",
    "spec = fig.add_gridspec(ncols=ncols, nrows=nrows, width_ratios=widths,\n",
    "                         height_ratios=heights, left=0., right=.2, bottom=0.6, top=1.0)\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "ax.set_title('{:.1f} Hz'.format(true_field.max()), fontsize=12)\n",
    "_, ax = tools.visualize_field(true_field, grid_shape, figax=(fig, ax), ticks=[0, true_field.max()], \n",
    "                              ticklabels=['0', 'max'], ticktitle='firing rate')\n",
    "\n",
    "ax.text(0.5, 1.3, 'true model', transform=ax.transAxes, fontsize=12, ha='center')\n",
    "ax.set_title('{:.1f} Hz'.format(true_field.max()), fontsize=12)\n",
    "ax.annotate('fits with increasing data', xy=(3.9, 1.30), xytext=(3.9, 1.40), xycoords='axes fraction', \n",
    "            fontsize=12, ha='center', va='bottom',\n",
    "            arrowprops=dict(arrowstyle='-[, widthB=17.0, lengthB=0.2', lw=2.0))\n",
    "\n",
    "# plot fields\n",
    "widths = [1, 1, 1]\n",
    "heights = [1]\n",
    "nrows = 1\n",
    "ncols = 3\n",
    "spec = fig.add_gridspec(ncols=ncols, nrows=nrows, width_ratios=widths,\n",
    "                         height_ratios=heights, wspace=0.25, left=0.3, right=1., bottom=0.6, top=1.0)\n",
    "\n",
    "x_t = behav_data[0]\n",
    "y_t = behav_data[1]\n",
    "\n",
    "for k in range(3):\n",
    "    samp = use_samples[k]\n",
    "    t_spike_ = []\n",
    "    for tt in t_spike:\n",
    "        t_spike_.append(tt[tt < samp])\n",
    "    x_s, y_s = neural_utils.CovariatesAtSpikes(t_spike_, (x_t, y_t))\n",
    "    st = (x_s[0][::4], y_s[0][::4])\n",
    "\n",
    "    ax = fig.add_subplot(spec[0, k])\n",
    "    ax.set_title('{:.1f} Hz'.format(field_tuples[k].max()), fontsize=12)\n",
    "    _, ax = tools.visualize_field(field_tuples[k], grid_shape, spike_pos=st, figax=(fig, ax), ticktitle='', cbar=False)\n",
    "\n",
    "\n",
    "\n",
    "### RMS versus amount of data, loss\n",
    "widths = [3, 1]\n",
    "heights = [1]\n",
    "nrows = 1\n",
    "ncols = 2\n",
    "spec = fig.add_gridspec(ncols=ncols, nrows=nrows, width_ratios=widths,\n",
    "                         height_ratios=heights, left=0.05, right=1., bottom=0., top=.52)\n",
    "\n",
    "ax = fig.add_subplot(spec[0, 0])\n",
    "\n",
    "ax.plot(np.log10(use_samples), RMS[:runs], marker='o', label='parametric')\n",
    "ax.plot(np.log10(use_samples), RMS[runs:], marker='o', label='GP')\n",
    "ax.set_ylabel('RMS error (Hz)')\n",
    "ax.set_xlabel('time steps in data')\n",
    "ax.set_ylim(0.)\n",
    "ax.set_xlim(0.)\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(-4, 4))\n",
    "ax.legend()\n",
    "\n",
    "# loss landscape for GP\n",
    "ax = fig.add_subplot(spec[0, 1])\n",
    "grid_shape = [[range_w_1[0], range_w_1[-1]], [range_w_2[0], range_w_2[-1]]]\n",
    "\n",
    "_, ax = tools.visualize_field(landscape, grid_shape, ticktitle='loss', figax=(fig, ax), ticks=[], \n",
    "                              vmin=None, cmap='plasma', cbar_format='%.1e', aspect='auto')\n",
    "\n",
    "ax.scatter(lw_x, lw_y, marker='+', color='w', s=100)\n",
    "#ax.set_xticks(list(range_w_1[::3]))\n",
    "#ax.set_yticks(list(range_w_2[::3]))\n",
    "ax.set_xlabel(r'$u_{1x}$')\n",
    "ax.set_ylabel(r'$u_{1y}$')\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('output/fit_sequence.svg')\n",
    "plt.savefig('output/fit_sequence.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
