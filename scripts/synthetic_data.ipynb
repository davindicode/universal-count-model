{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") # access to library\n",
    "\n",
    "import neuroprob as mdl\n",
    "from neuroprob import utils\n",
    "from neuroprob import GP\n",
    "\n",
    "\n",
    "dev = utils.pytorch.get_device(gpu=0)\n",
    "\n",
    "import validation\n",
    "\n",
    "import pickle\n",
    "\n",
    "plt.style.use(['paper.mplstyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian von Mises bump head direction model\n",
    "sample_bin = 0.1 # 100 ms\n",
    "track_samples = 10000\n",
    "trials = 1\n",
    "\n",
    "hd_t = np.empty(track_samples)\n",
    "\n",
    "hd_t[0] = 0\n",
    "rn = np.random.randn(track_samples)\n",
    "for k in range(1, track_samples):\n",
    "    hd_t[k] = hd_t[k-1] + 0.5*rn[k]\n",
    "    \n",
    "hd_t = hd_t % (2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tl = track_samples\n",
    "\n",
    "l = 200.*sample_bin*np.ones((1, 1))\n",
    "v = np.ones(1)\n",
    "kernel_tuples = [('variance', v), \n",
    "                 ('RBF', 'euclid', l)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    kernel, _, _ = GP.kernels.create_kernel(kernel_tuples, 'softplus', torch.double)\n",
    "\n",
    "    T = torch.arange(Tl)[None, None, :, None]*sample_bin\n",
    "    K = kernel(T, T)[0, 0]\n",
    "    K.view(-1)[::Tl+1] += 1e-6\n",
    "    \n",
    "L = torch.cholesky(K)\n",
    "eps = torch.randn(Tl).double()\n",
    "v = L @ eps\n",
    "a_t = v.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heteroscedastic CMP\n",
    "neurons = 50\n",
    "\n",
    "covariates = [hd_t[None, :, None].repeat(trials, axis=0)]\n",
    "glm = validation.CMP_hdc(sample_bin, track_samples, covariates, neurons, trials=trials)\n",
    "glm.to(dev)\n",
    "\n",
    "XZ, rate, _ = glm.evaluate(0)\n",
    "syn_train = glm.likelihood.sample(rate[0].cpu().numpy(), XZ=XZ)\n",
    "\n",
    "trial = 0\n",
    "bin_size = 1\n",
    "tbin, resamples, rc_t, (rhd_t,) = utils.neural.bin_data(bin_size, sample_bin, syn_train[trial], \n",
    "                                                        track_samples, (np.unwrap(hd_t),), average_behav=True, binned=True)\n",
    "rhd_t = rhd_t % (2*np.pi)\n",
    "\n",
    "\n",
    "np.savez_compressed('./data/CMPh_HDC', spktrain=rc_t, rhd_t=rhd_t, tbin=tbin)\n",
    "torch.save({'model': glm.state_dict()}, './data/CMPh_HDC_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modulated Poisson\n",
    "neurons = 50\n",
    "\n",
    "covariates = [hd_t[None, :, None].repeat(trials, axis=0), \n",
    "              a_t[None, :, None].repeat(trials, axis=0)]\n",
    "glm = validation.IP_bumps(sample_bin, track_samples, covariates, neurons, trials=trials)\n",
    "glm.to(dev)\n",
    "\n",
    "\n",
    "_, rate, _ = glm.evaluate(0)\n",
    "syn_train = glm.likelihood.sample(rate[0].cpu().numpy())\n",
    "\n",
    "trial = 0\n",
    "bin_size = 1\n",
    "tbin, resamples, rc_t, (rhd_t, ra_t) = utils.neural.bin_data(bin_size, sample_bin, syn_train[trial], \n",
    "                                                        track_samples, (np.unwrap(hd_t), a_t), average_behav=True, binned=True)\n",
    "rhd_t = rhd_t % (2*np.pi)\n",
    "\n",
    "\n",
    "np.savez_compressed('./data/IP_HDC', spktrain=rc_t, rhd_t=rhd_t, ra_t=ra_t, tbin=tbin)\n",
    "torch.save({'model': glm.state_dict()}, './data/IP_HDC_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize spike trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "utils.plot.raster_plot((fig, ax), rc_t, track_samples, tbin*1000, neurons, markersize=10)\n",
    "utils.plot.decorate_ax(ax, xlabel='time (ms)', ylabel='neuron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
