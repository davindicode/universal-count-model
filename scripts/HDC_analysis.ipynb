{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as scstats\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") # access to library\n",
    "\n",
    "import neuroprob as mdl\n",
    "from neuroprob import utils\n",
    "from neuroprob import GP\n",
    "\n",
    "import pickle\n",
    "\n",
    "gpu_dev = 0\n",
    "dev = utils.pytorch.get_device(gpu=gpu_dev)\n",
    "\n",
    "import models\n",
    "import HDC\n",
    "\n",
    "plt.style.use(['paper.mplstyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_model(session_id, phase, cvdata, resamples, binsize, \n",
    "                   m, rcov, max_count, neurons, gpu):\n",
    "    if session_id == 'Mouse12-120806':\n",
    "        session_id = 0\n",
    "    elif session_id == 'Mouse28-140313':\n",
    "        session_id = 1\n",
    "        \n",
    "    if phase == 'wake':\n",
    "        phase = 1\n",
    "    elif phase == 'sleep':\n",
    "        phase = 0\n",
    "\n",
    "    mtype, ll_mode, r_mode, num_induc, inv_link, C, z_dims, delays, folds, cv_switch, basis_mode = m\n",
    "    shared_W = False\n",
    "    enc_layers, basis = models.hyper_params(basis_mode)\n",
    "    kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "\n",
    "    if ll_mode == 'U':\n",
    "        mapping_net = models.net(C, basis, max_count, neurons, shared_W)\n",
    "    else:\n",
    "        mapping_net = None\n",
    "\n",
    "    full_model, _ = models.set_model('HDC', max_count, mtype, r_mode, ll_mode, fcov, neurons, \n",
    "                                     tbin, ftrain, num_induc, batch_size=batch_size, inv_link=inv_link, \n",
    "                                     mapping_net=mapping_net, C=C, enc_layers=enc_layers)\n",
    "    full_model.to(dev)\n",
    "\n",
    "\n",
    "    name = 'HDC{}'.format(binsize)\n",
    "    if shared_W:\n",
    "        name += 'S'\n",
    "    if basis_mode != 'ew':\n",
    "        name += basis_mode\n",
    "        \n",
    "    model_name = '{}{}{}_{}_{}_{}_C={}_{}'.format(name, session_id, phase, mtype, ll_mode, r_mode, C, kcv)\n",
    "    if cv_switch:\n",
    "        model_name += '_'\n",
    "    checkpoint = torch.load('./checkpoint/' + model_name, map_location='cuda:{}'.format(gpu))\n",
    "    full_model.load_state_dict(checkpoint['full_model'])\n",
    "    return full_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 'Mouse28-140313'\n",
    "phase = 'wake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'U', 'hd', 8, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_t', 48, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t', 64, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'IP', 'hd_w_s_pos_t', 64, 'exp', 1, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'hNB', 'hd_w_s_pos_t', 64, 'exp', 1, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t', 64, 'identity', 3, [], False, 10, False, 'qd')]\n",
    "\n",
    "bn = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn)\n",
    "\n",
    "left_x = rcov[3].min()\n",
    "right_x = rcov[3].max()\n",
    "bottom_y = rcov[4].min()\n",
    "top_y = rcov[4].max()\n",
    "\n",
    "pick_neuron = list(range(neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### statistics over the behaviour ###\n",
    "avg_models = []\n",
    "var_models = []\n",
    "ff_models = []\n",
    "\n",
    "binnings = [20, 40, 100, 200, 500]\n",
    "\n",
    "for bn in binnings:\n",
    "\n",
    "    rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn)\n",
    "    max_count = int(rc_t.max())\n",
    "    x_counts = torch.arange(max_count+1)\n",
    "    \n",
    "    mode = modes[2]\n",
    "    cvdata = models.get_cv_sets(mode, [2], 5000, rc_t, resamples, rcov)[0]\n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "\n",
    "    avg_model = []\n",
    "    var_model = []\n",
    "    ff_model = []\n",
    "\n",
    "    for b in range(full_model.inputs.batches):\n",
    "        P_mc = models.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0).cpu()\n",
    "\n",
    "        avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "        var = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "        ff = var/(avg+1e-12)\n",
    "        avg_model.append(avg)\n",
    "        var_model.append(var)\n",
    "        ff_model.append(ff)\n",
    "\n",
    "    avg_models.append(torch.cat(avg_model, dim=-1).mean(0).numpy())\n",
    "    var_models.append(torch.cat(var_model, dim=-1).mean(0).numpy())\n",
    "    ff_models.append(torch.cat(ff_model, dim=-1).mean(0).numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS framework for regression models\n",
    "CV = [2, 5, 8]\n",
    "\n",
    "### KS test over binnings ###\n",
    "Qq_bn = []\n",
    "Zz_bn = []\n",
    "R_bn = []\n",
    "Rp_bn = []\n",
    "mode = modes[2]\n",
    "\n",
    "N = len(pick_neuron)\n",
    "for kcv in CV:\n",
    "    for en, bn in enumerate(binnings):\n",
    "        cvdata = models.get_cv_sets(mode, [kcv], 3000, rc_t, resamples, rcov)[0]\n",
    "        kcv_str, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        time_steps = ftrain.shape[-1]\n",
    "\n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "        if en == 0:\n",
    "            q_ = []\n",
    "            Z_ = []\n",
    "            for b in range(full_model.inputs.batches): # predictive posterior\n",
    "                P_mc = models.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "                P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "                for n in range(N):\n",
    "                    spike_binned = full_model.likelihood.spikes[b][0, pick_neuron[n], :].numpy()\n",
    "                    q, Z = models.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "                    q_.append(q)\n",
    "                    Z_.append(Z)\n",
    "\n",
    "            q = []\n",
    "            Z = []\n",
    "            for n in range(N):\n",
    "                q.append(np.concatenate(q_[n::N]))\n",
    "                Z.append(np.concatenate(Z_[n::N]))\n",
    "\n",
    "        elif en > 0:\n",
    "            cov_used = models.cov_used(mode[2], fcov, 'HDC')\n",
    "            q = models.compute_count_stats(full_model, mode[1], tbin, ftrain, cov_used, pick_neuron, \\\n",
    "                                           traj_len=1, start=0, T=time_steps, bs=5000)\n",
    "            Z = [utils.stats.q_to_Z(q_) for q_ in q]    \n",
    "\n",
    "        Pearson_s = []\n",
    "        for n in range(len(pick_neuron)):\n",
    "            for m in range(n+1, len(pick_neuron)):\n",
    "                r, r_p = scstats.pearsonr(Z[n], Z[m]) # Pearson r correlation test\n",
    "                Pearson_s.append((r, r_p))\n",
    "\n",
    "        r = np.array([p[0] for p in Pearson_s])\n",
    "        r_p = np.array([p[1] for p in Pearson_s])\n",
    "\n",
    "        Qq_bn.append(q)\n",
    "        Zz_bn.append(Z)\n",
    "        R_bn.append(r)\n",
    "        Rp_bn.append(r_p)\n",
    "        \n",
    "        \n",
    "q_DS_bn = []\n",
    "T_DS_bn = []\n",
    "T_KS_bn = []\n",
    "for q in Qq_bn:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_ll.append(T_DS)\n",
    "        T_KS_ll.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_ll.append(utils.stats.Z_to_q(Z_DS))\n",
    "\n",
    "Qq_bn = np.array(Qq_bn).reshape(len(CV), len(binnings), -1)\n",
    "Zz_bn = np.array(Zz_bn).reshape(len(CV), len(binnings), -1)\n",
    "R_bn = np.array(R_bn).reshape(len(CV), len(binnings), -1)\n",
    "Rp_bn = np.array(Rp_bn).reshape(len(CV), len(binnings), -1)\n",
    "        \n",
    "q_DS_bn = np.array(q_DS_bn).reshape(len(CV), len(binnings), -1)\n",
    "T_DS_bn = np.array(T_DS_bn).reshape(len(CV), len(binnings), -1)\n",
    "T_KS_bn = np.array(T_KS_bn).reshape(len(CV), len(binnings), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn)\n",
    "max_count = int(rc_t.max())\n",
    "x_counts = torch.arange(max_count+1)\n",
    "\n",
    "HD_offset = -1.0 # global shift of head direction coordinates, makes plots better as the preferred head directions are not at axis lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "PLL_rg_ll = []\n",
    "PLL_rg_cov = []\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "\n",
    "beta = 0.0\n",
    "batchsize = 5000\n",
    "\n",
    "PLL_rg_ll = []\n",
    "Ms = modes[2:5]\n",
    "for mode in Ms: # likelihood\n",
    "    \n",
    "    for cvdata in models.get_cv_sets(mode, kcvs, batchsize, rc_t, resamples, rcov):\n",
    "        kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "    \n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, mode, \n",
    "                                    rcov, max_count, neurons, gpu=gpu_dev)\n",
    "        PLL_rg_ll.append(models.RG_pred_ll(full_model, mode[2], cv_set, bound='ELBO', \n",
    "                                           beta=beta, neuron_group=None, ll_mode='GH', ll_samples=100))\n",
    "    \n",
    "PLL_rg_ll = np.array(PLL_rg_ll).reshape(len(Ms), len(kcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CV = [2, 5, 8] # validation segments from splitting data into 10\n",
    "\n",
    "### KS test ###\n",
    "Qq_ll = []\n",
    "Zz_ll = []\n",
    "R_ll = []\n",
    "Rp_ll = []\n",
    "\n",
    "N = len(pick_neuron)\n",
    "for kcv in CV:\n",
    "    for en, mode in enumerate(Ms):\n",
    "        cvdata = models.get_cv_sets(mode, [kcv], 3000, rc_t, resamples, rcov)[0]\n",
    "        kcv_str, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        time_steps = ftrain.shape[-1]\n",
    "\n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "        if en == 0:\n",
    "            q_ = []\n",
    "            Z_ = []\n",
    "            for b in range(full_model.inputs.batches): # predictive posterior\n",
    "                P_mc = models.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "                P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "                for n in range(N):\n",
    "                    spike_binned = full_model.likelihood.spikes[b][0, pick_neuron[n], :].numpy()\n",
    "                    q, Z = models.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "                    q_.append(q)\n",
    "                    Z_.append(Z)\n",
    "\n",
    "            q = []\n",
    "            Z = []\n",
    "            for n in range(N):\n",
    "                q.append(np.concatenate(q_[n::N]))\n",
    "                Z.append(np.concatenate(Z_[n::N]))\n",
    "\n",
    "        elif en > 0:\n",
    "            cov_used = models.cov_used(mode[2], fcov, 'HDC')\n",
    "            q = models.compute_count_stats(full_model, mode[1], tbin, ftrain, cov_used, pick_neuron, \\\n",
    "                                            traj_len=1, start=0, T=time_steps, bs=5000)\n",
    "            Z = [utils.stats.q_to_Z(q_) for q_ in q]    \n",
    "\n",
    "        Pearson_s = []\n",
    "        for n in range(len(pick_neuron)):\n",
    "            for m in range(n+1, len(pick_neuron)):\n",
    "                r, r_p = scstats.pearsonr(Z[n], Z[m]) # Pearson r correlation test\n",
    "                Pearson_s.append((r, r_p))\n",
    "\n",
    "        r = np.array([p[0] for p in Pearson_s])\n",
    "        r_p = np.array([p[1] for p in Pearson_s])\n",
    "\n",
    "        Qq_ll.append(q)\n",
    "        Zz_ll.append(Z)\n",
    "        R_ll.append(r)\n",
    "        Rp_ll.append(r_p)\n",
    "        \n",
    "        \n",
    "q_DS_ll = []\n",
    "T_DS_ll = []\n",
    "T_KS_ll = []\n",
    "for q in Qq_ll:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_ll.append(T_DS)\n",
    "        T_KS_ll.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_ll.append(utils.stats.Z_to_q(Z_DS))\n",
    "\n",
    "Qq_ll = np.array(Qq_ll).reshape(len(CV), len(Ms), -1)\n",
    "Zz_ll = np.array(Zz_ll).reshape(len(CV), len(Ms), -1)\n",
    "R_ll = np.array(R_ll).reshape(len(CV), len(Ms), -1)\n",
    "Rp_ll = np.array(Rp_ll).reshape(len(CV), len(Ms), -1)\n",
    "        \n",
    "q_DS_ll = np.array(q_DS_ll).reshape(len(CV), len(Ms), -1)\n",
    "T_DS_ll = np.array(T_DS_ll).reshape(len(CV), len(Ms), -1)\n",
    "T_KS_ll = np.array(T_KS_ll).reshape(len(CV), len(Ms), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLL_rg_cov = []\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "\n",
    "Ms = modes[:3]\n",
    "for mode in Ms: # input space\n",
    "    \n",
    "    for cvdata in models.get_cv_sets(mode, kcvs, batchsize, rc_t, resamples, rcov):\n",
    "        kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "    \n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, mode, \n",
    "                                    rcov, max_count, neurons, gpu=gpu_dev)\n",
    "        PLL_rg_cov.append(models.RG_pred_ll(full_model, mode[2], cv_set, bound='ELBO', \n",
    "                                            beta=beta, neuron_group=None, ll_mode='GH', ll_samples=100))\n",
    "    \n",
    "PLL_rg_cov = np.array(PLL_rg_cov).reshape(len(Ms), len(kcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Universal regression model\n",
    "mode = modes[2]\n",
    "kcv = -1 # fit on the full dataset\n",
    "cvdata = models.get_cv_sets(mode, [kcv], 3000, rc_t, resamples, rcov)[0]\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, bn, mode, rcov, \n",
    "                            max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "TT = tbin*resamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# marginalized tuning curves\n",
    "MC = 100\n",
    "skip = 10\n",
    "\n",
    "\n",
    "### hd ###\n",
    "steps = 100\n",
    "P_tot = models.marginalized_P(full_model, [np.linspace(0, 2*np.pi, steps)], [0], rcov, 10000, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                             smooth_length=5, padding_mode='circular')\n",
    "mhd_lower, mhd_mean, mhd_upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                            smooth_length=5, padding_mode='circular')\n",
    "mhd_fflower, mhd_ffmean, mhd_ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "# total variance decomposition\n",
    "hd_mean_EV = avg.var(0).mean(-1)\n",
    "hd_mean_VE = avg.mean(0).var(-1)\n",
    "hd_ff_EV = avg.var(0).mean(-1)\n",
    "hd_ff_VE = avg.mean(0).var(-1)\n",
    "\n",
    "# TI\n",
    "hd_mean_tf = (mhd_mean.max(dim=-1)[0] - mhd_mean.min(dim=-1)[0]) / (mhd_mean.max(dim=-1)[0] + mhd_mean.min(dim=-1)[0])\n",
    "hd_ff_tf = (mhd_ffmean.max(dim=-1)[0] - mhd_ffmean.min(dim=-1)[0]) /(mhd_ffmean.max(dim=-1)[0] + mhd_ffmean.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "### omega ###\n",
    "steps = 100\n",
    "w_edge = (-rcov[1].min()+rcov[1].max())/2.\n",
    "covariates_w = np.linspace(-w_edge, w_edge, steps)\n",
    "P_tot = models.marginalized_P(full_model, [covariates_w], [1], rcov, 10000, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mw_mean = avg.mean(0)\n",
    "mw_ff = ff.mean(0)\n",
    "w_mean_tf = (mw_mean.max(dim=-1)[0] - mw_mean.min(dim=-1)[0]) / (mw_mean.max(dim=-1)[0] + mw_mean.min(dim=-1)[0])\n",
    "w_ff_tf = (mw_ff.max(dim=-1)[0] - mw_ff.min(dim=-1)[0]) /(mw_ff.max(dim=-1)[0] + mw_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "### speed ###\n",
    "steps = 100\n",
    "P_tot = models.marginalized_P(full_model, [np.linspace(0, 30., steps)], [2], rcov, 10000, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "ms_mean = avg.mean(0)\n",
    "ms_ff = ff.mean(0)\n",
    "s_mean_tf = (ms_mean.max(dim=-1)[0] - ms_mean.min(dim=-1)[0]) / (ms_ff.max(dim=-1)[0] + ms_ff.min(dim=-1)[0])\n",
    "s_ff_tf = (ms_ff.max(dim=-1)[0] - ms_ff.min(dim=-1)[0]) /(ms_ff.max(dim=-1)[0] + ms_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "### time ###\n",
    "steps = 100\n",
    "P_tot = models.marginalized_P(full_model, [np.linspace(0, TT, steps)], [5], rcov, 10000, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mt_mean = avg.mean(0)\n",
    "mt_ff = ff.mean(0)\n",
    "t_mean_tf = (mt_mean.max(dim=-1)[0] - mt_mean.min(dim=-1)[0]) / (mt_ff.max(dim=-1)[0] + mt_ff.min(dim=-1)[0])\n",
    "t_ff_tf = (mt_ff.max(dim=-1)[0] - mt_ff.min(dim=-1)[0]) /(mt_ff.max(dim=-1)[0] + mt_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "\n",
    "### position ###\n",
    "grid_size_pos = (12, 10)\n",
    "grid_shape_pos = [[left_x, right_x], [bottom_y, top_y]]\n",
    "\n",
    "steps = np.product(grid_size_pos)\n",
    "A, B = grid_size_pos\n",
    "\n",
    "cov_list = [np.linspace(left_x, right_x, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "            np.linspace(bottom_y, top_y, B)[None, :].repeat(A, axis=0).flatten()]\n",
    "                      \n",
    "P_tot = models.marginalized_P(full_model, cov_list, [3, 4], rcov, 10000, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mpos_mean = avg.mean(0)\n",
    "mpos_ff = ff.mean(0)\n",
    "pos_mean_tf = (mpos_mean.max(dim=-1)[0] - mpos_mean.min(dim=-1)[0]) / (mpos_mean.max(dim=-1)[0] + mpos_mean.min(dim=-1)[0])\n",
    "pos_ff_tf = (mpos_ff.max(dim=-1)[0] - mpos_ff.min(dim=-1)[0]) / (mpos_ff.max(dim=-1)[0] + mpos_ff.min(dim=-1)[0])\n",
    "mpos_mean = mpos_mean.reshape(-1, A, B)\n",
    "mpos_ff = mpos_ff.reshape(-1, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional tuning curves\n",
    "MC = 300\n",
    "MC_ = 100\n",
    "\n",
    "\n",
    "### head direction tuning ###\n",
    "steps = 100\n",
    "covariates = [np.linspace(0, 2*np.pi, steps)-HD_offset, \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps)]\n",
    "\n",
    "P_mc = models.compute_P(full_model, covariates, pick_neuron, MC=MC).cpu()\n",
    "\n",
    "\n",
    "avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "xcvar = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "ff = xcvar/avg\n",
    "\n",
    "avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                             smooth_length=5, padding_mode='circular')\n",
    "lower_hd, mean_hd, upper_hd = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                            smooth_length=5, padding_mode='circular')\n",
    "fflower_hd, ffmean_hd, ffupper_hd = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "covariates_hd = np.linspace(0, 2*np.pi, steps)\n",
    "\n",
    "\n",
    "\n",
    "### hd_w ###\n",
    "grid_size_hdw = (51, 41)\n",
    "grid_shape_hdw = [[0, 2*np.pi], [-10., 10.]]\n",
    "\n",
    "steps = np.product(grid_size_hdw)\n",
    "A, B = grid_size_hdw\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              np.linspace(-10., 10., B)[None, :].repeat(A, axis=0).flatten(), \n",
    "              0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps)]\n",
    "\n",
    "P_mean = models.compute_P(full_model, covariates, pick_neuron, MC=MC).mean(0).cpu()\n",
    "field_hdw = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# compute preferred HD\n",
    "grid = (101, 21)\n",
    "grid_shape = [[0, 2*np.pi], [-10., 10.]]\n",
    "\n",
    "steps = np.product(grid)\n",
    "A, B = grid\n",
    "\n",
    "w_arr = np.linspace(-10., 10., B)\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              w_arr[None, :].repeat(A, axis=0).flatten(), \n",
    "              0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps)]\n",
    "\n",
    "P_mean = models.compute_P(full_model, covariates, pick_neuron, MC=MC).mean(0).cpu()\n",
    "field = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "Z = np.cos(covariates[0]) + np.sin(covariates[0])*1j # CoM angle\n",
    "Z = Z[None, :].reshape(-1, A, B)\n",
    "pref_hdw = (np.angle((Z*field).mean(1)) % (2*np.pi)) # neurons, w\n",
    "\n",
    "\n",
    "# ATI\n",
    "ATI = []\n",
    "res_var = []\n",
    "for k in range(neurons):\n",
    "    _, a, shift, losses = utils.signal.circ_lin_regression(pref_hdw[k, :], w_arr/(2*np.pi), dev='cpu', iters=1000, lr=1e-2)\n",
    "    ATI.append(-a)\n",
    "    res_var.append(losses[-1])\n",
    "ATI = np.array(ATI)\n",
    "res_var = np.array(res_var)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### omega tuning ###\n",
    "mean_w = []\n",
    "lower_w = []\n",
    "upper_w = []\n",
    "ffmean_w = []\n",
    "fflower_w = []\n",
    "ffupper_w = []\n",
    "\n",
    "steps = 100\n",
    "w_edge = (-rcov[1].min()+rcov[1].max())/2.\n",
    "covariates_w = np.linspace(-w_edge, w_edge, steps)\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  covariates_w, \n",
    "                  0.*np.ones(steps), \n",
    "                  (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                  0.*np.ones(steps)]\n",
    "\n",
    "    P_mc = models.compute_P(full_model, covariates, [n], MC=MC)[:, 0, ...].cpu()\n",
    "\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "    xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "    ff = xcvar/avg\n",
    "\n",
    "    avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                 smooth_length=5, padding_mode='replicate')\n",
    "    lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "    ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "    fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "    \n",
    "    lower_w.append(lower)\n",
    "    mean_w.append(mean)\n",
    "    upper_w.append(upper)\n",
    "    \n",
    "    fflower_w.append(fflower)\n",
    "    ffmean_w.append(ffmean)\n",
    "    ffupper_w.append(ffupper)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### hd_t ###\n",
    "grid_size_hdt = (51, 41)\n",
    "grid_shape_hdt = [[0, 2*np.pi], [0., TT]]\n",
    "\n",
    "steps = np.product(grid_size_hdt)\n",
    "A, B = grid_size_hdt\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              np.linspace(0., TT, B)[None, :].repeat(A, axis=0).flatten()]\n",
    "\n",
    "P_mean = models.compute_P(full_model, covariates, pick_neuron, MC=MC_).mean(0).cpu()\n",
    "field_hdt = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "# drift and similarity matrix\n",
    "grid = (201, 16)\n",
    "grid_shape = [[0, 2*np.pi], [0., TT]]\n",
    "\n",
    "steps = np.product(grid)\n",
    "A, B = grid\n",
    "\n",
    "t_arr = np.linspace(0., TT, B)\n",
    "dt_arr = t_arr[1]-t_arr[0]\n",
    "covariates = [np.linspace(0, 2*np.pi, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              t_arr[None, :].repeat(A, axis=0).flatten()]\n",
    "\n",
    "P_mean = models.compute_P(full_model, covariates, pick_neuron, MC=MC_).mean(0).cpu()\n",
    "field = (x_counts[None, None, :]*P_mean).sum(-1).reshape(-1, A, B).numpy()\n",
    "\n",
    "\n",
    "\n",
    "Z = np.cos(covariates[0]) + np.sin(covariates[0])*1j # CoM angle\n",
    "Z = Z[None, :].reshape(-1, A, B)\n",
    "E_exp = (Z*field).sum(-2)/field.sum(-2)\n",
    "pref_hdt = (np.angle(E_exp) % (2*np.pi)) # neurons, t\n",
    "\n",
    "tun_width = 1.-np.abs(E_exp)\n",
    "amp_t = field.mean(-2) # mean amplitude\n",
    "ampm_t = field.max(-2)\n",
    "\n",
    "sim_mat = []\n",
    "act = (field-field.mean(-2, keepdims=True))/field.std(-2, keepdims=True)\n",
    "en = np.argsort(pref_hdt, axis=0)\n",
    "for t in range(B):\n",
    "    a = act[en[:, t], :, t]\n",
    "    sim_mat = ((a[:, None, :]*a[None, ...]).mean(-1))\n",
    "\n",
    "\n",
    "\n",
    "drift = []\n",
    "res_var_drift = []\n",
    "for k in range(len(pick_neuron)):\n",
    "    _, a, shift, losses = utils.signal.circ_lin_regression(pref_hdt[k, :], t_arr/(2*np.pi)/1e2, \n",
    "                                                           dev='cpu', iters=1000, lr=1e-2)\n",
    "    drift.append(a/1e2)\n",
    "    res_var_drift.append(losses[-1])\n",
    "drift = np.array(drift)\n",
    "res_var_drift = np.array(res_var_drift)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### speed ###\n",
    "mean_s = []\n",
    "lower_s = []\n",
    "upper_s = []\n",
    "ffmean_s = []\n",
    "fflower_s = []\n",
    "ffupper_s = []\n",
    "    \n",
    "steps = 100\n",
    "covariates_s = np.linspace(0, 30., steps)\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  0.*np.ones(steps), covariates_s, \n",
    "                  (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                  0.*np.ones(steps)]\n",
    "\n",
    "    P_mc = models.compute_P(full_model, covariates, [n], MC=MC)[:, 0, ...].cpu()\n",
    "\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "    xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "    ff = xcvar/avg\n",
    "\n",
    "    avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                 smooth_length=5, padding_mode='replicate')\n",
    "    lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "    \n",
    "    ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "    fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "    lower_s.append(lower)\n",
    "    mean_s.append(mean)\n",
    "    upper_s.append(upper)\n",
    "    \n",
    "    fflower_s.append(fflower)\n",
    "    ffmean_s.append(ffmean)\n",
    "    ffupper_s.append(ffupper)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### time ###\n",
    "mean_t = []\n",
    "lower_t = []\n",
    "upper_t = []\n",
    "ffmean_t = []\n",
    "fflower_t = []\n",
    "ffupper_t = []\n",
    "    \n",
    "steps = 100\n",
    "covariates_t = np.linspace(0, TT, steps)\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  0.*np.ones(steps), 0.*np.ones(steps), \n",
    "                  (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                  covariates_t]\n",
    "\n",
    "    P_mc = models.compute_P(full_model, covariates, [n], MC=MC)[:, 0, ...].cpu()\n",
    "\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "    xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "    ff = xcvar/avg\n",
    "\n",
    "    avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                 smooth_length=5, padding_mode='replicate')\n",
    "    lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "    \n",
    "    ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "    fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "    lower_t.append(lower)\n",
    "    mean_t.append(mean)\n",
    "    upper_t.append(upper)\n",
    "    \n",
    "    fflower_t.append(fflower)\n",
    "    ffmean_t.append(ffmean)\n",
    "    ffupper_t.append(ffupper)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### pos ###\n",
    "grid_shape_pos = [[left_x, right_x], [bottom_y, top_y]]\n",
    "H = grid_shape_pos[1][1]-grid_shape_pos[1][0]\n",
    "W = grid_shape_pos[0][1]-grid_shape_pos[0][0]\n",
    "grid_size_pos = (int(41*W/H), 41)\n",
    "\n",
    "\n",
    "steps = np.product(grid_size_pos)\n",
    "A, B = grid_size_pos\n",
    "\n",
    "field_pos = []\n",
    "ff_pos = []\n",
    "for en, n in enumerate(pick_neuron):\n",
    "    covariates = [pref_hdw[en, len(w_arr)//2]*np.ones(steps), \n",
    "                  0.*np.ones(steps), 0.*np.ones(steps), \n",
    "                  np.linspace(left_x, right_x, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "                  np.linspace(bottom_y, top_y, B)[None, :].repeat(A, axis=0).flatten(), \n",
    "                  t*np.ones(steps)]\n",
    "\n",
    "    P_mc = models.compute_P(full_model, covariates, [n], MC=MC_)[:, 0, ...].cpu()\n",
    "    avg = (x_counts[None, None, :]*P_mc).sum(-1).reshape(-1, A, B).numpy()\n",
    "    var = (x_counts[None, None, :]**2*P_mc).sum(-1).reshape(-1, A, B).numpy()\n",
    "    xcvar = (var-avg**2)\n",
    "\n",
    "    field_pos.append(avg.mean(0))\n",
    "    ff_pos.append((xcvar/(avg+1e-12)).mean(0))\n",
    "\n",
    "\n",
    "field_pos = np.stack(field_pos)\n",
    "ff_pos = np.stack(ff_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Pearson correlation between Fano factors and mean firing rates\n",
    "b = 1\n",
    "Pearson_ff = []\n",
    "ratio = []\n",
    "for avg, ff in zip(avg_models[b], ff_models[b]):\n",
    "    r, r_p = scstats.pearsonr(ff, avg) # Pearson r correlation test\n",
    "    Pearson_ff.append((r, r_p))\n",
    "    ratio.append(ff.std()/avg.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    avg_models, var_models, ff_models, \n",
    "    Pearson_ff, ratio, \n",
    "    PLL_rg_ll, PLL_rg_cov, \n",
    "    Qq_ll, Zz_ll, R_ll, Rp_ll, q_DS_ll, T_DS_ll, T_KS_ll, \n",
    "    sign_KS, sign_DS, \n",
    "    mhd_mean, mhd_ff, hd_mean_tf, hd_ff_tf, \n",
    "    mw_mean, mw_ff, w_mean_tf, w_ff_tf, \n",
    "    ms_mean, ms_ff, s_mean_tf, s_ff_tf, \n",
    "    mt_mean, mt_ff, t_mean_tf, t_ff_tf, \n",
    "    mpos_mean, mpos_ff, pos_mean_tf, pos_ff_tf, \n",
    "    covariates_hd, lower_hd, mean_hd, upper_hd, \n",
    "    fflower_hd, ffmean_hd, ffupper_hd, \n",
    "    covariates_s, lower_s, mean_s, upper_s, \n",
    "    fflower_s, ffmean_s, ffupper_s, \n",
    "    covariates_t, lower_t, mean_t, upper_t, \n",
    "    fflower_t, ffmean_t, ffupper_t, \n",
    "    covariates_w, lower_w, mean_w, upper_w, \n",
    "    fflower_w, ffmean_w, ffupper_w, \n",
    "    grid_size_pos, grid_shape_pos, field_pos, ff_pos, \n",
    "    grid_size_hdw, grid_shape_hdw, field_hdw, \n",
    "    grid_size_hdt, grid_shape_hdt, field_hdt, \n",
    "    pref_hdw, ATI, res_var, \n",
    "    pref_hdt, drift, res_var_drift, \n",
    "    tun_width, amp_t, ampm_t, sim_mat, \n",
    "    pick_neuron, max_count, tbin, rcov, region_edge\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./checkpoint/P_HDC_rg40.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn)\n",
    "\n",
    "left_x = rcov[3].min()\n",
    "right_x = rcov[3].max()\n",
    "bottom_y = rcov[4].min()\n",
    "top_y = rcov[4].max()\n",
    "\n",
    "pick_neuron = list(range(neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'U', 'hd_w_s_pos_t', 64, 'identity', 3, [], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R1', 72, 'identity', 3, [6], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R2', 80, 'identity', 3, [6], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R3', 88, 'identity', 3, [6], False, 10, False, 'ew'), \n",
    "         ('GP', 'U', 'hd_w_s_pos_t_R4', 96, 'identity', 3, [6], False, 10, False, 'ew')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### statistics over the behaviour ###\n",
    "avg_models_z = []\n",
    "var_models_z = []\n",
    "ff_models_z = []\n",
    "\n",
    "kcv = 2\n",
    "\n",
    "bn = 40\n",
    "\n",
    "for mode in modes:\n",
    "\n",
    "    rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, bn)\n",
    "    max_count = int(rc_t.max())\n",
    "    x_counts = torch.arange(max_count+1)\n",
    "    \n",
    "    cvdata = models.get_cv_sets(mode, [kcv], 5000, rc_t, resamples, rcov)[0]\n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "\n",
    "    avg_model = []\n",
    "    var_model = []\n",
    "    ff_model = []\n",
    "\n",
    "    for b in range(full_model.inputs.batches):\n",
    "        P_mc = models.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0).cpu()\n",
    "\n",
    "        avg = (x_counts[None, None, None, :]*P_mc).sum(-1)\n",
    "        var = ((x_counts[None, None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "        ff = var/(avg+1e-12)\n",
    "        avg_model.append(avg)\n",
    "        var_model.append(var)\n",
    "        ff_model.append(ff)\n",
    "\n",
    "    avg_models_z.append(torch.cat(avg_model, dim=-1).mean(0).numpy())\n",
    "    var_models_z.append(torch.cat(var_model, dim=-1).mean(0).numpy())\n",
    "    ff_models_z.append(torch.cat(ff_model, dim=-1).mean(0).numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1\n",
    "Pearson_ffz = []\n",
    "ratioz = []\n",
    "\n",
    "for d in range(len(avg_models_z)):\n",
    "    Pearson_ffz_ = []\n",
    "    ratioz_ = []\n",
    "    for avg, ff in zip(avg_models_z[d], ff_models_z[d]):\n",
    "        r, r_p = scstats.pearsonr(ff, avg) # Pearson r correlation test\n",
    "        Pearson_ffz_.append((r, r_p))\n",
    "        ratioz_.append(ff.std()/avg.std())\n",
    "        \n",
    "    Pearson_ffz.append(Pearson_ffz_)\n",
    "    ratioz.append(ratioz_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning = 40\n",
    "rcov, neurons, tbin, resamples, rc_t, region_edge = HDC.get_dataset(session_id, phase, binning)\n",
    "max_count = int(rc_t.max())\n",
    "x_counts = torch.arange(max_count+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBO for models of different dimensions\n",
    "kcvs = [2, 5, 8] # get corresponding training sets\n",
    "Ms = modes[:5]\n",
    "\n",
    "elbo = []\n",
    "for em, mode in enumerate(Ms):\n",
    "    for cvdata in models.get_cv_sets(mode, kcvs, 3000, rc_t, resamples, rcov):\n",
    "        kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, binning, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "        \n",
    "        batches = full_model.likelihood.batches\n",
    "        print(batches)\n",
    "        elbo_ = []\n",
    "        for b in range(batches):\n",
    "            elbo_.append(full_model.objective(b, cov_samples=1, ll_mode='GH', bound='ELBO', neuron=None, \n",
    "                                              beta=1., ll_samples=100).data.cpu().numpy())\n",
    "        elbo.append(np.array(elbo_).mean())\n",
    "        \n",
    "elbo = np.array(elbo).reshape(len(Ms), len(kcvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation for dimensionality\n",
    "beta = 0.0\n",
    "n_group = np.arange(5)\n",
    "val_neuron = [n_group, n_group+5, n_group+10, n_group+15, n_group+20, n_group+25, np.arange(3)+30]\n",
    "ncvx = 2\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "Ms = modes[:5]\n",
    "\n",
    "cv_pll = []\n",
    "for em, mode in enumerate(Ms):\n",
    "    for cvdata in models.get_cv_sets(mode, kcvs, 5000, rc_t, resamples, rcov):\n",
    "        kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "        if em > 0:\n",
    "            for v_neuron in val_neuron:\n",
    "                fac = len(n_group)/len(v_neuron)\n",
    "                \n",
    "                prev_ll = np.inf\n",
    "                for tr in range(ncvx):\n",
    "                    full_model = get_full_model(session_id, phase, cvdata, resamples, binning, \n",
    "                                                mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "                    mask = np.ones((neurons,), dtype=bool)\n",
    "                    mask[v_neuron] = False\n",
    "                    f_neuron = np.arange(neurons)[mask]\n",
    "                    ll = models.LVM_pred_ll(full_model, mode[-5], mode[2], cv_set, f_neuron, v_neuron, \n",
    "                                            cov_MC=1, ll_MC=10, beta=beta, beta_z=0.0)[0]\n",
    "                    if ll < prev_ll:\n",
    "                        prev_ll = ll\n",
    "\n",
    "                cv_pll.append(fac*prev_ll)\n",
    "                \n",
    "        else: # no latent\n",
    "            for v_neuron in val_neuron:\n",
    "                fac = len(n_group)/len(v_neuron)\n",
    "                \n",
    "                full_model = get_full_model(session_id, phase, cvdata, resamples, binning, \n",
    "                                            mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "                cv_pll.append(fac*models.RG_pred_ll(full_model, mode[2], cv_set, bound='ELBO', \n",
    "                                                    beta=beta, neuron_group=v_neuron, ll_mode='GH', ll_samples=100))\n",
    "\n",
    "        \n",
    "cv_pll = np.array(cv_pll).reshape(len(Ms), len(kcvs), len(val_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latent trajectories and drift timescale of neural tuning for 2D latent model\n",
    "mode = modes[2]\n",
    "cvdata = models.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov)[0]\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, binning, mode, rcov, max_count, \n",
    "                            neurons, gpu=gpu_dev)\n",
    "\n",
    "X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "\n",
    "X_c = X_loc[6]\n",
    "X_s = X_std[6]\n",
    "z_tau = tbin/(1-torch.sigmoid(full_model.inputs.p_mu_6).data.cpu().numpy())\n",
    "\n",
    "t_lengths = full_model.mapping.kernel.kern1.lengthscale[:, 0, 0, -3].data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load regression model with most input dimensions\n",
    "mode = modes[4]\n",
    "cvdata = models.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov)[0]\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, 40, mode, rcov, max_count, \n",
    "                            neurons, gpu=gpu_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### head direction tuning ###\n",
    "MC = 100\n",
    "\n",
    "steps = 100\n",
    "covariates = [np.linspace(0, 2*np.pi, steps), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps), \n",
    "              (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "              0.*np.ones(steps), \n",
    "              0.*np.ones(steps), 0.*np.ones(steps)]\n",
    "\n",
    "P_mc = models.compute_P(full_model, covariates, pick_neuron, MC=MC).cpu()\n",
    "\n",
    "\n",
    "avg = (x_counts[None, None, None, :]*P_mc).sum(-1).mean(0).numpy()\n",
    "pref_hd = covariates[0][np.argmax(avg, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# marginalized tuning curves\n",
    "rcovz = list(rcov) + [X_c[:, 0], X_c[:, 1]]\n",
    "MC = 10\n",
    "skip = 10\n",
    "\n",
    "\n",
    "\n",
    "### z ###\n",
    "step = 100\n",
    "P_tot = models.marginalized_P(full_model, [np.linspace(-.2, .2, step)], [6], rcovz, 10000, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mz1_mean = avg.mean(0)\n",
    "mz1_ff = ff.mean(0)\n",
    "z1_mean_tf = (mz1_mean.max(dim=-1)[0] - mz1_mean.min(dim=-1)[0]) / (mz1_mean.max(dim=-1)[0] + mz1_mean.min(dim=-1)[0])\n",
    "z1_ff_tf = (mz1_ff.max(dim=-1)[0] - mz1_ff.min(dim=-1)[0]) /(mz1_ff.max(dim=-1)[0] + mz1_ff.min(dim=-1)[0])\n",
    "\n",
    "\n",
    "\n",
    "step = 100\n",
    "P_tot = models.marginalized_P(full_model, [np.linspace(-.2, .2, step)], [7], rcovz, 10000, \n",
    "                              pick_neuron, MC=MC, skip=skip)\n",
    "avg = (x_counts[None, None, None, :]*P_tot).sum(-1)\n",
    "var = (x_counts[None, None, None, :]**2*P_tot).sum(-1)-avg**2\n",
    "ff = var/avg\n",
    "\n",
    "mz2_mean = avg.mean(0)\n",
    "mz2_ff = ff.mean(0)\n",
    "z2_mean_tf = (mz2_mean.max(dim=-1)[0] - mz2_mean.min(dim=-1)[0]) / (mz2_mean.max(dim=-1)[0] + mz2_mean.min(dim=-1)[0])\n",
    "z2_ff_tf = (mz2_ff.max(dim=-1)[0] - mz2_ff.min(dim=-1)[0]) /(mz2_ff.max(dim=-1)[0] + mz2_ff.min(dim=-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute 2D latent model properties of tuning curves and TI to latent space\n",
    "z_d = 2\n",
    "\n",
    "if z_d == 1: ### latent ###\n",
    "    mean_z = []\n",
    "    lower_z = []\n",
    "    upper_z = []\n",
    "    ffmean_z = []\n",
    "    fflower_z = []\n",
    "    ffupper_z = []\n",
    "\n",
    "    steps = 100\n",
    "    covariates_z = np.linspace(-.2, .2, steps)\n",
    "    for en, n in enumerate(pick_neuron):\n",
    "        # x_t, y_t, s_t, th_t, hd_t, time_t\n",
    "        covariates = [pref_hd[n]*np.ones(steps), 0.*np.ones(steps), np.ones(steps)*0., \n",
    "                      (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                      0.*np.ones(steps), covariates_z]\n",
    "\n",
    "        P_mc = models.compute_P(full_model, covariates, [n], MC=1000).cpu()[:, 0, ...]\n",
    "\n",
    "        avg = (x_counts[None, None, :]*P_mc).sum(-1)\n",
    "        xcvar = ((x_counts[None, None, :]**2*P_mc).sum(-1)-avg**2)\n",
    "        ff = xcvar/avg\n",
    "\n",
    "        avgs = utils.signal.percentiles_from_samples(avg, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                     smooth_length=5, padding_mode='replicate')\n",
    "        lower, mean, upper = [cs_.cpu().numpy() for cs_ in avgs]\n",
    "\n",
    "        ffs = utils.signal.percentiles_from_samples(ff, percentiles=[0.05, 0.5, 0.95], \n",
    "                                                smooth_length=5, padding_mode='replicate')\n",
    "        fflower, ffmean, ffupper = [cs_.cpu().numpy() for cs_ in ffs]\n",
    "\n",
    "        lower_z.append(lower)\n",
    "        mean_z.append(mean)\n",
    "        upper_z.append(upper)\n",
    "\n",
    "        fflower_z.append(fflower)\n",
    "        ffmean_z.append(ffmean)\n",
    "        ffupper_z.append(ffupper)\n",
    "    \n",
    "else: ### 2d z ###\n",
    "    grid_size_zz = (41, 41)\n",
    "    grid_shape_zz = [[-.2, .2], [-.2, .2]]\n",
    "\n",
    "    steps = np.product(grid_size_zz)\n",
    "    A, B = grid_size_zz\n",
    "\n",
    "    \n",
    "    field_zz = []\n",
    "    ff_zz = []\n",
    "    t = 0\n",
    "    for en, n in enumerate(pick_neuron):\n",
    "        covariates = [pref_hd[n]*np.ones(steps), \n",
    "                      0.*np.ones(steps), 0.*np.ones(steps), \n",
    "                      (left_x+right_x)/2.*np.ones(steps), (bottom_y+top_y)/2.*np.ones(steps), \n",
    "                      t*np.ones(steps), \n",
    "                      np.linspace(-.2, .2, A)[:, None].repeat(B, axis=1).flatten(), \n",
    "                      np.linspace(-.2, .2, B)[None, :].repeat(A, axis=0).flatten()]\n",
    "\n",
    "        P_mean = models.compute_P(full_model, covariates, [n], MC=100).mean(0).cpu()\n",
    "        avg = (x_counts[None, :]*P_mean[0, ...]).sum(-1).reshape(A, B).numpy()\n",
    "        var = (x_counts[None, :]**2*P_mean[0, ...]).sum(-1).reshape(A, B).numpy()\n",
    "        xcvar = (var-avg**2)\n",
    "\n",
    "        field_zz.append(avg)\n",
    "        ff_zz.append(xcvar/avg)\n",
    "\n",
    "    field_zz = np.stack(field_zz)\n",
    "    ff_zz = np.stack(ff_zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS framework for latent models, including Fisher Z scores\n",
    "CV = [2, 5, 8]\n",
    "bn = 40\n",
    "\n",
    "\n",
    "\n",
    "### KS test ###\n",
    "Qq = []\n",
    "Zz = []\n",
    "R = []\n",
    "Rp = []\n",
    "\n",
    "N = len(pick_neuron)\n",
    "for kcv in CV:\n",
    "    for en, mode in enumerate(modes):\n",
    "        cvdata = models.get_cv_sets(mode, [kcv], 3000, rc_t, resamples, rcov)[0]\n",
    "        kcv_str, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        time_steps = ftrain.shape[-1]\n",
    "\n",
    "        full_model = get_full_model(session_id, phase, cvdata, resamples, bn, \n",
    "                                    mode, rcov, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "        q_ = []\n",
    "        Z_ = []\n",
    "        for b in range(full_model.inputs.batches): # predictive posterior\n",
    "            P_mc = models.compute_pred_P(full_model, b, pick_neuron, None, cov_samples=10, ll_samples=1, tr=0)\n",
    "            P = P_mc.mean(0).cpu().numpy()\n",
    "\n",
    "            for n in range(N):\n",
    "                spike_binned = full_model.likelihood.spikes[b][0, pick_neuron[n], :].numpy()\n",
    "                q, Z = models.get_q_Z(P[n, ...], spike_binned, deq_noise=None)\n",
    "                q_.append(q)\n",
    "                Z_.append(Z)\n",
    "\n",
    "        q = []\n",
    "        Z = []\n",
    "        for n in range(N):\n",
    "            q.append(np.concatenate(q_[n::N]))\n",
    "            Z.append(np.concatenate(Z_[n::N]))\n",
    "\n",
    "\n",
    "        Pearson_s = []\n",
    "        for n in range(len(pick_neuron)):\n",
    "            for m in range(n+1, len(pick_neuron)):\n",
    "                r, r_p = scstats.pearsonr(Z[n], Z[m]) # Pearson r correlation test\n",
    "                Pearson_s.append((r, r_p))\n",
    "\n",
    "        r = np.array([p[0] for p in Pearson_s])\n",
    "        r_p = np.array([p[1] for p in Pearson_s])\n",
    "\n",
    "        Qq.append(q)\n",
    "        Zz.append(Z)\n",
    "        R.append(r)\n",
    "        Rp.append(r_p)\n",
    "\n",
    "\n",
    "fisher_z = []\n",
    "fisher_q = []\n",
    "for en, r in enumerate(R):\n",
    "    fz = 0.5*np.log((1+r)/(1-r))*np.sqrt(time_steps-3)\n",
    "    fisher_z.append(fz)\n",
    "    fisher_q.append(utils.stats.Z_to_q(fz))\n",
    "\n",
    "    \n",
    "q_DS_ = []\n",
    "T_DS_ = []\n",
    "T_KS_ = []\n",
    "for q in Qq:\n",
    "    for qq in q:\n",
    "        T_DS, T_KS, sign_DS, sign_KS, p_DS, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_DS_.append(T_DS)\n",
    "        T_KS_.append(T_KS)\n",
    "        \n",
    "        Z_DS = T_DS/np.sqrt(2/(qq.shape[0]-1))\n",
    "        q_DS_.append(utils.stats.Z_to_q(Z_DS))\n",
    "        \n",
    "        \n",
    "fisher_z = np.array(fisher_z).reshape(len(CV), len(Ms), -1)\n",
    "fisher_q = np.array(fisher_q).reshape(len(CV), len(Ms), -1)\n",
    "\n",
    "Qq = np.array(Qq).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "Zz = np.array(Zz).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "R = np.array(R).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "Rp = np.array(Rp).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "        \n",
    "q_DS_ = np.array(q_DS_).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "T_DS_ = np.array(T_DS_).reshape(len(CV), len(Ms), len(pick_neuron), -1)\n",
    "T_KS_ = np.array(T_KS_).reshape(len(CV), len(Ms), len(pick_neuron), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KS_fishq = []\n",
    "p_KS_fishq = []\n",
    "for q in fisher_q:\n",
    "    for qq in q:\n",
    "        _, T_KS, _, _, _, p_KS = utils.stats.KS_statistics(qq, alpha=0.05, alpha_s=0.05)\n",
    "        T_KS_fishq.append(T_KS)\n",
    "        p_KS_fishq.append(p_KS)\n",
    "        \n",
    "T_KS_fishq = np.array(T_KS_fishq).reshape(len(CV), len(Ms))\n",
    "p_KS_fishq = np.array(p_KS_fishq).reshape(len(CV), len(Ms))\n",
    "        \n",
    "        \n",
    "T_KS_ks = []\n",
    "p_KS_ks = []\n",
    "for q in Qq:\n",
    "    for qq in q:\n",
    "        for qqq in qq:\n",
    "            _, T_KS, _, _, _, p_KS = utils.stats.KS_statistics(qqq, alpha=0.05, alpha_s=0.05)\n",
    "            T_KS_ks.append(T_KS)\n",
    "            p_KS_ks.append(p_KS)\n",
    "        \n",
    "T_KS_ks = np.array(T_KS_ks).reshape(len(CV), len(Ms), len(pick_neuron))\n",
    "p_KS_ks = np.array(p_KS_ks).reshape(len(CV), len(Ms), len(pick_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delayed noise or spatiotemporal correlations\n",
    "NN = len(pick_neuron)\n",
    "delays = np.arange(5)\n",
    "R_mat_spt = np.empty((len(Ms), len(delays), NN, NN))\n",
    "R_mat_sptp = np.empty((len(Ms), len(delays), NN, NN))\n",
    "\n",
    "kcv_ind = 1\n",
    "for d, Z_ in enumerate(Zz[kcv_ind]):\n",
    "    steps = len(Z_[0])-len(delays)\n",
    "    \n",
    "    for en, t in enumerate(delays):\n",
    "        Pearson_s = []\n",
    "        for n in range(NN):\n",
    "            for m in range(NN):\n",
    "                r, r_p = scstats.pearsonr(Z_[n][t:t+steps], Z_[m][:-len(delays)]) # Pearson r correlation test\n",
    "                R_mat_spt[d, en, n, m] = r\n",
    "                R_mat_sptp[d, en, n, m] = r_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute timescales for input dimensions from ACG\n",
    "delays = 5000\n",
    "Tsteps = rcov[0].shape[0]\n",
    "L = Tsteps-delays+1\n",
    "acg_rc = []\n",
    "\n",
    "for rc in rcov[:1]: # angular\n",
    "    acg = np.empty(delays)\n",
    "    for d in range(delays):\n",
    "        A = rc[d:d+L]\n",
    "        B = rc[:L]\n",
    "        acg[d] = utils.stats.corr_circ_circ(A, B)\n",
    "    acg_rc.append(acg)\n",
    "\n",
    "for rc in rcov[1:-1]:\n",
    "    acg = np.empty(delays)\n",
    "    for d in range(delays):\n",
    "        A = rc[d:d+L]\n",
    "        B = rc[:L]\n",
    "        acg[d] = ((A-A.mean())*(B-B.mean())).mean()/A.std()/B.std()\n",
    "    acg_rc.append(acg)\n",
    "    \n",
    "\n",
    "acg_z = []\n",
    "for rc in X_c.T:\n",
    "    acg = np.empty(delays)\n",
    "    for d in range(delays):\n",
    "        A = rc[d:d+L]\n",
    "        B = rc[:L]\n",
    "        acg[d] = ((A-A.mean())*(B-B.mean())).mean()/A.std()/B.std()\n",
    "    acg_z.append(acg)\n",
    "    \n",
    "    \n",
    "timescales = []\n",
    "\n",
    "for d in range(len(rcov)-1):\n",
    "    timescales.append(np.where(acg_rc[d] < np.exp(-1))[0][0]*tbin)\n",
    "    \n",
    "for d in range(X_c.shape[-1]):\n",
    "    timescales.append(np.where(acg_z[d] < np.exp(-1))[0][0]*tbin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    avg_models_z, var_models_z, ff_models_z, \n",
    "    Pearson_ffz, ratioz, \n",
    "    X_c, X_s, cv_pll, elbo, z_tau, pref_hd, \n",
    "    grid_size_zz, grid_shape_zz, field_zz, ff_zz, \n",
    "    mz1_mean, mz1_ff, z1_mean_tf, z1_ff_tf, \n",
    "    mz2_mean, mz2_ff, z2_mean_tf, z2_ff_tf, \n",
    "    q_DS_, T_DS_, T_KS_, Qq, Zz, R, Rp, fisher_z, fisher_q, \n",
    "    T_KS_fishq, p_KS_fishq, T_KS_ks, p_KS_ks, \n",
    "    R_mat_spt, R_mat_sptp, \n",
    "    timescales, acg_rc, acg_z, t_lengths\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./checkpoint/P_HDC_nc40.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent variable modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 100\n",
    "rcov_lvm, neurons, tbin, resamples, rc_t, _ = HDC.get_dataset(session_id, phase, binsize)\n",
    "max_count = int(rc_t.max())\n",
    "rhd_t = rcov_lvm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [('GP', 'U', 'T1', 8, 'identity', 3, [0], False, 10, False, 'ew'), \n",
    "         ('GP', 'IP', 'T1', 8, 'exp', 1, [0], False, 10, False, 'ew'), \n",
    "         ('GP', 'hNB', 'T1', 8, 'exp', 1, [0], False, 10, False, 'ew')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# likelihood CV over subgroups of neurons as well as validation runs\n",
    "beta = 0.0\n",
    "n_group = np.arange(5)\n",
    "ncvx = 2\n",
    "val_neuron = [n_group, n_group+5, n_group+10, n_group+15, n_group+20, n_group+25, np.arange(3)+30]\n",
    "kcvs = [1, 2, 3, 5, 6, 8] # validation segments from splitting data into 10\n",
    "\n",
    "LVM_cv_ll = []\n",
    "for kcv in kcvs:\n",
    "    for mode in modes:\n",
    "        cvdata = models.get_cv_sets(mode, [kcv], 5000, rc_t, resamples, rcov)[0]\n",
    "        _, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "        cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "\n",
    "        for v_neuron in val_neuron:\n",
    "            fac = len(n_group)/len(v_neuron)\n",
    "\n",
    "            prev_ll = np.inf\n",
    "            for tr in range(ncvx):\n",
    "                full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                                            mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "                mask = np.ones((neurons,), dtype=bool)\n",
    "                mask[v_neuron] = False\n",
    "                f_neuron = np.arange(neurons)[mask]\n",
    "                ll = models.LVM_pred_ll(full_model, mode[-5], mode[2], cv_set, f_neuron, v_neuron, \n",
    "                                        beta=beta, beta_z=0.0)[0]\n",
    "                if ll < prev_ll:\n",
    "                    prev_ll = ll\n",
    "\n",
    "            LVM_cv_ll.append(fac*prev_ll)\n",
    "        \n",
    "LVM_cv_ll = np.array(LVM_cv_ll).reshape(len(kcvs), len(modes), len(val_neuron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circ_drift_regression(x, z, t, topology, dev='cpu', iters=1000, lr=1e-2, a_fac=1):\n",
    "    t = torch.tensor(t, device=dev)\n",
    "    X = torch.tensor(x, device=dev)\n",
    "    Z = torch.tensor(z, device=dev)\n",
    "        \n",
    "    lowest_loss = np.inf\n",
    "    for sign in [1, -1]: # select sign automatically\n",
    "        shift = Parameter(torch.zeros(1, device=dev))\n",
    "        a = Parameter(torch.zeros(1, device=dev))\n",
    "\n",
    "        optimizer = optim.Adam([a, shift], lr=lr)\n",
    "        losses = []\n",
    "        for k in range(iters):\n",
    "            optimizer.zero_grad()\n",
    "            Z_ = t*a_fac*a + shift + sign*Z\n",
    "            loss = (utils.latent.metric(Z_, X, topology)**2).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.cpu().item())\n",
    "\n",
    "        l_ = loss.cpu().item()\n",
    "        \n",
    "        if l_ < lowest_loss:\n",
    "            lowest_loss = l_\n",
    "            a_ = a.cpu().item()\n",
    "            shift_ = shift.cpu().item()\n",
    "            sign_ = sign\n",
    "            losses_ = losses\n",
    "\n",
    "    return a_fac*a_, sign_, shift_, losses_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectory regression to align to data and compute drifts\n",
    "topology = 'torus'\n",
    "cvK = 3\n",
    "CV = [0, 1, 2]\n",
    "\n",
    "RMS_cv = []\n",
    "drifts_lv = []\n",
    "for mode in modes:\n",
    "    cvdata = models.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov_lvm)[0]\n",
    "    kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "    cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                                mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "    X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "    cvT = X_loc[0].shape[0]\n",
    "    tar_t = rhd_t[:cvT]\n",
    "    lat = X_loc[0]\n",
    "    \n",
    "    for rn in CV:\n",
    "        fit_range = np.arange(cvT//cvK) + rn*cvT//cvK\n",
    "\n",
    "        drift, sign, shift, losses = circ_drift_regression(tar_t[fit_range], lat[fit_range], fit_range*tbin, \n",
    "                                                      topology, dev=dev, a_fac=1e-5)\n",
    "        \n",
    "        #plt.plot(losses)\n",
    "        #plt.show()\n",
    "        mask = np.ones((cvT,), dtype=bool)\n",
    "        mask[fit_range] = False\n",
    "        \n",
    "        lat_t = torch.tensor((np.arange(cvT)*tbin*drift + shift + sign*lat) % (2*np.pi))\n",
    "        D = (utils.latent.metric(torch.tensor(tar_t)[mask], lat_t[mask], topology)**2)\n",
    "        RMS_cv.append(D.mean().item())\n",
    "        drifts_lv.append(drift)\n",
    "\n",
    "\n",
    "RMS_cv = np.array(RMS_cv).reshape(len(modes), len(CV))\n",
    "drifts_lv = np.array(drifts_lv).reshape(len(modes), len(CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute delays in latent trajectory w.r.t. data, see which one fits best in RMS\n",
    "topology = 'torus'\n",
    "cvK = 3\n",
    "CV = [0, 1, 2]\n",
    "\n",
    "D = 5\n",
    "delays = np.arange(-D, D+1)\n",
    "delay_RMS = []\n",
    "mode = modes[0]\n",
    "\n",
    "for delay in delays:\n",
    "    cvdata = models.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov_lvm)[0]\n",
    "    kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "    cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "        \n",
    "    full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                                mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "    X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "    cvT = X_loc[0].shape[0]-len(delays)+1\n",
    "    tar_t = rhd_t[D+delay:cvT+D+delay]\n",
    "    lat = X_loc[0][D:cvT+D]\n",
    "    \n",
    "    for rn in CV:\n",
    "        fit_range = np.arange(cvT//cvK) + rn*cvT//cvK\n",
    "\n",
    "        drift, sign, shift, _ = circ_drift_regression(tar_t[fit_range], lat[fit_range], fit_range*tbin, \n",
    "                                                      topology, dev=dev, a_fac=1e-5)\n",
    "        \n",
    "        mask = np.ones((cvT,), dtype=bool)\n",
    "        mask[fit_range] = False\n",
    "        \n",
    "        lat_ = torch.tensor((np.arange(cvT)*tbin*drift + shift + sign*lat) % (2*np.pi))\n",
    "        Dd = (utils.latent.metric(torch.tensor(tar_t)[mask], lat_[mask], topology)**2)\n",
    "        delay_RMS.append(Dd.mean().item())\n",
    "\n",
    "\n",
    "delay_RMS = np.array(delay_RMS).reshape(len(delays), len(CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latent inferred trajectory\n",
    "mode = modes[0]\n",
    "topology = 'torus'\n",
    "\n",
    "\n",
    "cvdata = models.get_cv_sets(mode, [-1], 5000, rc_t, resamples, rcov_lvm)[0]\n",
    "kcv, ftrain, fcov, vtrain, vcov, batch_size = cvdata\n",
    "cv_set = (ftrain, fcov, vtrain, vcov)\n",
    "\n",
    "full_model = get_full_model(session_id, phase, cvdata, resamples, 100, \n",
    "                            mode, rcov_lvm, max_count, neurons, gpu=gpu_dev)\n",
    "\n",
    "X_loc, X_std = full_model.inputs.eval_XZ()\n",
    "\n",
    "tar_t = rhd_t\n",
    "lat = X_loc[0]\n",
    "\n",
    "drift, sign, shift, _ = circ_drift_regression(tar_t[fit_range], lat[fit_range], fit_range*tbin, \n",
    "                                              topology, dev=dev, a_fac=1e-5)\n",
    "\n",
    "lat_t = ((np.arange(rhd_t.shape[0])*tbin*drift + shift + sign*lat) % (2*np.pi))\n",
    "lat_t_std = X_std[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_run = (\n",
    "    lat_t, lat_t_std, delay_RMS, RMS_cv, LVM_cv_ll, drifts_lv, rcov_lvm\n",
    ")\n",
    "\n",
    "pickle.dump(data_run, open('./checkpoint/P_HDC_lat.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
